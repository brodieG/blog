---
title: Quosures
author: ~
date: '2020-02-17'
slug: quosures
categories: [r]
tags: [meta-program,rlang]
image: /front-img/default.png
imagerect: ~
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: Front page summary
output:
  blogdown::html_page:
    keep_md: yes
    md_extensions: +raw_attribute
---
```{r echo=FALSE, child='../../static/chunks/init.Rmd'}
```

# Quosures?

Quosures first showed up on the scene as part of `rlang` about three years ago
to a collective &#x1F92F;.  There is good and bad &#x1F92F;, and quosures
delivered both in spades.  The good: what they do is powerful, and how they do
it at a minimum _seems_ magical.  The bad: new terminology and mechanics
confused a lot of people.  Since then the developers have devoted substantial
time and effort to make quosures and `rlang` in general more accessible.

There are two obvious reasons to introduce new terminology: the existing
terminology is bad, or there are new features that cannot be expressed in the
old terminology.  I happen to agree that the existing terminology around
evaluation in R is less clear than it could be, and that `rlang` brings along
many interesting new concepts.  But I also believe it is better to enhance
flawed-but-workable precedents than unilaterally create Standard Sixteen.

<!-- CC BY SA 2.5 -->
<img src='https://imgs.xkcd.com/comics/standards.png' />

There is a Himalaya's worth of hills-to-die-on arguing the rights and wrongs of
`rlang`'s terminology and mechanics, but they are not that interesting.  Much
more fun is what quosures do, and how they do it.

# Preface: Carriages, Horses, and Damn Philosophers...

## Ghosts of Posts Past

If you are unfamiliar with R's evaluation model, and how it enables
"Non-Standard Evaluation" (NSE), now is a good time to check out last week's
[post on the topic][1].  To understand this post it might help to know what
environments, "Enclosures" and "Environment Chains" are, the difference between
"Evaluation", "Calling", "Function", and "function Evaluation" Environments, and
what unevaluated commands are and how they are created.  All these are discussed
at length in that post.

And yes, that post was originally a footnote in this post, but once I hit triple
nested recursive footnotes I had to perform surgery.  Prognosis remains murky.

## Concept: `local`

R provides the `local` function as a mechanism for creating and using ad-hoc
Evaluation Environments:

```{r}
a <- 1
num <- 2
local({
  a <- 100
  a + num
})
a + num
```

The `a` defined at the top-level (`a <- 1`) is masked by the one defined
inside the `local` Evaluation Environment, but only for commands evaluated
in that environment.  This is akin to how commands in function bodies
are evaluated in function Evaluation Environments.

## Quasi-quoi?

As we saw previously it is possible to capture unevaluated R commands with
`quote` and `substitute`[3].  We can also manipulate them:

```{r}
mean_call <- function(cmd) {
  cmd2 <- quote(mean(NULL))     # create unevaled `mean(NULL)`
  cmd2[[2]] <- substitute(cmd)  # sub in user expression
  cmd2
}
mean_call(a + b)
```

The manipulation is awkward though.  Thankfully R provides `bquote` to
partially quote commands (or partially evaluate them, depending on your life
outlook - personally I'm more of a command-half-evaluated guy).  Let's look
first at a trivial example:

```{r}
bquote(mean(.(1 + 2)))
```

Compare to:

```{r}
quote(mean(1 + 2))
```

`quote` captures the entire command unevaluated.  `bquote` allows whatever is
inside the `.(...)` to be evaluated.  We can use `bquote` to recreate
`mean_call`:

```{r}
mean_call2 <- function(cmd) {
  bquote(mean(.(substitute(cmd))))
}
mean_call2(a + b)
```

Wait, but that's an unevaluated call.  Didn't we promise partial evaluation?  We
delivered, but it's not obvious because what was partially evaluated,
`substitute(cmd)`, produced an unevaluated command.  We can see that by
comparing to an implementation that uses `quote` instead of `bquote`:

```{r}
mean_call2a <- function(cmd) {
  quote(mean(substitute(cmd)))
}
mean_call2a(a + b)
```

In this case we can see that what we wanted evaluated and injected into the
command was not.

How does this all relate to quasi-quotation?  Turns out partial evaluation
**is** quasi-quotation.  It's just that [quasi-quotation][4] was coined by the
philosopher [Willard Van Orman Quine][5], and a philosopher's thought is to
arcaneness what Midas's touch is to gold[^resentment].  Lisp adopted the
terminology, and `rlang` brought it explicitly to R.

So what's the etymology of R's `bquote`?  Turns out that in certain Lisp
dialects such as Scheme the equivalent to the R `quote` function is a single
quote `&#x60;`, and quasi-quoting (partial evaluation) is done with the
"backquote" `&#x60;`.  So R's quasi-quoting function is named after the name of
the `&#x60;`.

# About Them Quosures

Just as `quote` and `substitute`, and `bquote`  do, quosures capture unevaluated
R commands.

```{r}
library(rlang)
a <- 1
(qbase <- quote(a + 1))
(qrlang <- quo(a + 1))   # create a quosure with quo
```

Additionally as you can see above, quosures record the environment the command
would have been evaluated it, had it not been captured.  Finally, they do not
self-evaluate even when `eval`ed:

```{r}
eval(qbase)
eval(qrlang)
```

`rlang` provides `eval_tidy` to evaluate quosures:

```{r}
eval_tidy(qrlang)
```

Things get more interesting when environments get more complex:

```{r}
a <- 1
lang <- local({
  a <- sample(100:200, 1)
  list(
    base=quote(a + 1),
    rlang=quo(a + 1)
  )
})
eval(lang[['base']])
eval_tidy(lang[['rlang']])
```

Normal quoted language resolves `a` in the environment in which it is
`eval`ed.  The quosure instead resolves `a` in the Evaluation Environment in
which it was **created**.  Pretty neat.

# Quosures in Functions

Let's build a simple NSE function to compute means in the context of of a
user-supplied data frame.  First with base:

```{r}
mean_with_base <- function(dat, cmd) {
  cmd <- substitute(cmd)           # capture user command
  cmd <- bquote(mean(.(cmd)))      # wrap it in `mean`
  eval(cmd, dat, parent.frame())
}
```

We'll use it to compute mean engine displacement of `mtcars` in liters:

```{r}
l.per.cubic.i <- 100 / 2.54^3
mean_with_base(mtcars, disp * l.per.cubic.i)
```

`rlang::enquo` can be used instead of `substitute`.  Additionally, most of
`rlang`'s capturing functions support quasi-quotation (partial evaluation) out
of the box with the `!!` "operator", so `quo` is actually closer to `bquote`
than to `quote`:

```{r}
mean_with_rlang <- function(dat, cmd) {
  cmd <- enquo(cmd)                # capture user command
  cmd <- quo(mean(!!cmd))          # wrap it in `mean`
  eval_tidy(cmd, dat)
}
mean_with_rlang(mtcars, disp * l.per.cubic.i)
```

It worked!  But, why bother?  As we'll see shortly `rlang` adds some pixie dust
that makes the `rlang` version more powerful.

# The Power Of Quosures

Let's up the difficulty a little bit.  Something that often breaks with bad
NSE implementations is names bound to environments other than the global
environment.  We add a bad version of `l.per.cubic.i` to that environment to
test if the functions find the correct version in the Calling Environment:

```{r}
l.per.cubic.i <- NA
local({
  l.per.cubic.i <- 100 / 2.54^3
  list(
    base=mean_with_base(mtcars, disp * l.per.cubic.i),
    rlang=mean_with_rlang(mtcars, disp * l.per.cubic.i)
  )
})
```

They do.  Neither is fooled.  But let's take it a step further by creating a
decoy mean in the Calling Environment.  Normally functions are shielded from
external function definitions because they look up names in their internal
Evaluation Environments.  But here, what allowed `mean_with_base` to find the
correct `l.per.cubic.i` trips it up:

```{r}
l.per.cubic.i <- NA
local({
  mean <- function(...) 42
  l.per.cubic.i <- 100 / 2.54^3
  list(
    base=mean_with_base(mtcars, disp * l.per.cubic.i),
    rlang=mean_with_rlang(mtcars, disp * l.per.cubic.i)
  )
})
```

`mean_with_base` resolved the wrong `mean`, and would have failed even if it was
defined in a package.  `rlang`'s version on the other hand works fine.
Ultimately the issue is that in the command generated by `mean_with_*`:

<pre><code><span style='background-color: #aaffaa;'>mean</span>(<span
style='background-color: #aaaaff;'>disp * l.per.cubic.i</span>)</code></pre>

We need <code style='background-color: #aaffaa;'>mean</code> to be resolved
according to the Function Environment, but <code style='background-color:
#aaaaff;'>disp * l.per.cubic.i</code> in the Calling Environment.  R, however,
does not allow more than one Environment Chain at a time.

With quosures it works fine right out of the box.  Of course we can fix the R
version by pre-resolving `mean` as we did in [the prior post][2], but that's
extra work.  What sorcery allows `quosures` to evaluate in multiple a single
command in multiple environments, when R itself does not allow it?

# Is It Magic?

Let's peel the curtain back a bit:

```{r}
class(quo(a + b))
unclass(quo(a + b))
```

Quosures are formulas, which we can see both from the class, and also from the
tilde (`~`) once we side-step the quosure print method.  Formulas are an odd
duck in the R world: a form of captured command that cannot be evaluated and
also carries around an environment to use as an Enclosure.  Mostly they are used
to implement Domain Specific languages such as regression model or plot
specifications.  Because they can contain arbitrary R commands as well as
environments they are appealing as a vehicle for quosures.

Formulas do not evaluate:

```{r}
eval(~a + b)
```

So `eval_tidy` must do something special to force evaluation.  It could traverse
the command recursively, finding quosures, extracting their commands and
environments, and evaluating them in turn before finally evaluating the overall
quosure-free command.  But that's not very elegant.

Instead it does something quite clever: it replaces `~` with an internal version
that self-evaluates quosures.  Here is a hack-implementation to demonstrate:

```{r}
eval2 <- function(cmd) {
  env <- new.env(parent=parent.frame())
  env[['~']] <- function(...) {        # replace `~` with our version
    call <- sys.call()
    env <- environment(call)           # interesting this actually works
    eval(call[[2]], env)
  }
  eval(cmd, env)
}
```

`eval2` creates an environment that contains a self-evaluating version of `~`,
and evaluates the command therein.  Let's try it:

```{r}
a <- 666
cmd <- local({
  a <- 42
  ~ a + 1
})
eval2(cmd)
```

With more emotion:

```{r}
cmd2 <- local({
  a <- 1000
  ~ a * 3
})
cmd3 <- bquote(.(cmd) / .(cmd2) + a)
cmd3
eval2(cmd3)
```

Ha, half-assed quosures.  These don't even support adding data to the
Environment Chain, or deity-forbid actually using a formula as a formula, but we
do bottle that multiple-Environment-Chain magic in a handful of lines of code.






The leading tilde is a giveaway that quosures are formulas

Here we see that a quosure is a formula, a type of quoted command that never
evaluates.

```{r}
eval(~a + b)
```





One possible way `eval_tidy` could work is by traversing the captured command,
finding each of the quosures, and "manually" evaluating them in their
environments until only a command without quosures is .

Let's peel the curtain bac a
So how does 


I hate magic.  When something is magic it means I don't understand it.  Sure, I
can't need to understand everything around me, but when there is something I'm
working closely with, particularly related to programming, 



# Interlude - Rant

Feel free to skip this section; it is mostly a rant about names.  There is some
irony that it's coming from the guy who a few paragraphs ago was just
complaining about philosophers...

I wish `rlang` had focused more on harmonizing with precedent in R over a more
_tabula rasa_ approach.  I understand the appeal of the latter, and `rlang`'s
absolute right to pursue it, but believe it more important to build on existing
conventions where possible.  Instead we ended up with things like:

* `rlang::enquo` being completely different to `base::enquote`.
* `rlang::quo` closer to `base::bquote` than `base::quote`.
* `rlang::expr` closest to `base::bquote`, but `base::expression` is something
  else entirely.
* `!!`, a completely new and foreign implementation of an existing concept
  `.()`, that also overloads an existing operator.

There are reasonable arguments for why things ended up where they are.  The
existing names are not perfect, and some are downright bad.  I know the `rlang`
team devoted a lot of time and effort coming up with names, including trying to
work with precedent.  But I can't help but look at the result and think the
balance of priorities is off.  Obviously this is just my opinion, and getting it
is the risk you take when you visit my blog.

A big part of my reaction is that I think **objectively** perfect names are both
impossible and unnecessary.  Impossible because we cannot compress the entire
semantics of functions into names.  Unnecessary because we as humans are
good at keeping semantics anchored to arbitrary names once we learn them.  That
language exists is proof enough of that.  Names just need to be good enough that
they don't actively impede our learning of them, and when possible **hint** at
their purpose without causing confusion[^alternate-names].

A useful thing to do when we add functions with similar semantics to existing
ones is to echo the lexical roots **in the same way they are used in the
language**.  If the concepts don't exist in the language, then sure, let's try
to hang on to an external reference.  If the existing names are really terrible,
then let's make up or own **distinct** lexicon.

Languages are mostly arbitrary.  The only reason they work is everyone agrees
on the meanings and use of the symbols they comprise.  That agreement **is** the
language; fractures in it reduce the value of the language as a whole.

Okay, enough of that.  Now for the good stuff.




## Quasi-quoi?

Our problem, as the authors of of`mean_by_grp`, is to ensure that <code
style='background-color: #aaffaa;'>mean</code> above resolves according to
`mean_by_grp`'s Evaluation Environment, while <code style='background-color:
#aaaaff;'>Sepal.Length / Sepal.Width * M</code> resolves in an Environment
Chain that connects to the Calling Environment.  If we don't do the former then
we risk conflict with user defined functions as in:


the wrong one is

The latter is important if the
user happens to reference variables from their environment in their command.



It is generated and run by `mean_by_grp`, a function that computes the mean of
user-supplied commands within a `data.frame`


provided by the user and is intended to resolve in an Environment Chain that
passes through the Calling Environment.  On the other hand,  was added by our function to the
user-supplied command, and we need to make sure that it resolves


Jkk
Let

This happens because we have created a
function, `mean_by_grp`, that computes the mean of a user supplied command
within groups of a data frame.  So we have

This was the command in question:



to
be resolved according to the Calling Environment.  We can only evaluate a
command in a single environment, so we're stuck.

blue


The problem is that we need <code style='background-color:
#aaffaa;'>mean</code> to be resolved according to our function's Evaluation
Environment, but




In the previous post we ran into an interesting situation

I could bore you to tears with with my opinions about the rights and wrongs
about `rlang`'s terminology, I'

Jkk
but that would not be very useful.  Instead, I'll
bore you to tears


in

many people were

I think
A big part of the bad &#x1f92f; I think stemmed from a desire
to reset the terminology around standard and non-standard evaluation while at
the same time



were many different types of
ranging from "holy &
one experienced I
I think
varied

Part of the challenge with them is that they are

intended for use in
a somewhat arcane part of R
address a somewhat
arcane

<!-- this needs to become a shortcode -->
<img
  id='front-img' src='/front-img/default.png'
  class='post-inset-image'
/>

# Implementing Quosures with `with`?

```{r eval=FALSE}
quo_with <- function(x, env=parent.frame()) {
  force(env)
  exp <- eval(call('bquote', substitute(x)), parent.frame())
  bquote(base::with(.(env), .(exp)))
}
quo_arg_with <- function(x, env=parent.frame(), caller=parent.frame(2)) {
  force(caller)
  exp.s <- eval(bquote(.(substitute)(.(substitute(x)))), env)
  bquote(base::with(.(caller), .(exp.s)))
}

a <- 17
b <- 59
w <- local({
  b <- 1e6
  quo_with(a + b)
})
exp <- local({
  b <- 100
  x <- 3
  quo_with(x + 1 + .(w))
})
exp
eval(exp)

w <- 1
local({
  w <- 2
  f <- function(x) {
    w <- 5
    quo_arg_with(x)
  }
})
eval(f(w))
```

# What does it for self eval formula

* Override `~` in the mask environment
* Need access to the actual formula somehow.  We can get the call with
  `sys.call`.  And if the environment is attached to the unevaluated quosure
  itself then it can be retrieved from `sys.call`, and it seems that's what
  !! as that's how it works does?

```{r eval=FALSE}

`~` <- function(...) {
  call <- sys.call()
  env <- parent.frame()
  is.quo <- function(x)
    is.call(x) && is.environment(attr(x, '.Environment')) &&
    inherits(x, 'bquosure')
  if(!is.quo(call)) {
    tilde <- get('~', mode='function', envir=env)
    eval(bquote(.(tilde))(...), env)
  } else {
    eval(substitute(list(...))[[3]], attr(x, '.Environment'))
  }
}

library(rlang)
`~` <- function(...) stop('boom')
~ 1 + 1
x <- quo(~1 + 1)
eval_tidy(x)
rm(`~`)
x <- quo(~1 + 1)
eval_tidy(x)



```

# Original Garbage

# References

* [TidyEval Bookdown][21]

# Conclusions

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<p id='feedback-cont'></p>

# Appendix

## Acknowledgments

## Session Info

[1]: /2020/05/05/on-nse/
[2]: /2020/05/05/on-nse/#call-assembly
[2]: /2020/05/05/on-nse/#nse-in-functions
[4]: https://en.wikipedia.org/wiki/Quasi-quotation
[5]: https://en.wikipedia.org/wiki/Willard_Van_Orman_Quine
[6]: https://www.cs.unm.edu/~williams/cs491/quasiquote.ps
[22]: https://tidyeval.tidyverse.org/

[^alternatives]: Another solution would be to compute the user
  command by group first, and subsequently `lapply` `mean` over the resulting
  values.  This is a better solution, except for that it side-steps the problem
  we are trying to highlight, perhaps results in higher peak memory usage as we
  must hold the user command evaluated for every group in memory, and it only
  works if the user command contains no aggregating functions.
[^shortcomings]: Likely the most problematic one is that if the user command
  causes an error, the error backtrace will display the function in full rather
  than the name of the function, which may be confusing.
[^resentment]: Oh no, I bear no resentment whatsoever to philosopher's whose
  works were required readings for me back in the day.
[^alternate-names]: This is a set of names I came up with in five minutes for
  the primary `rlang` names:
  * bquo <- expr
  * quo_arg <- enexpr
  * quo_dots <- enexprs
  * bquosure <- quo
  * quosure_arg <- enquo
  * quosure_dots <- enquos (alt: bquosure_args)


