---
title: Quosures
author: ~
date: '2020-02-17'
slug: quosures
categories: [r]
tags: [meta-program,rlang]
image: /front-img/default.png
imagerect: ~
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: Front page summary
output:
  blogdown::html_page:
    keep_md: yes
    md_extensions: +raw_attribute
---
```{r echo=FALSE, child='../../static/chunks/init.Rmd'}
```

# Quosures?

Quosures first showed up on the scene as part of `rlang` about three years ago
to a collective &#x1F92F;.  There is good and bad &#x1F92F;, and quosures
delivered both in spades.  The good: what they do is powerful, and how they do
it at a minimum _seems_ magical.  The bad: new terminology and mechanics
confused a lot of people.  Since then the developers have devoted substantial
time and effort to make quosures and `rlang` in general more accessible.

There are two obvious reasons to introduce new terminology: the existing
terminology is bad, or there are new features that cannot be expressed in the
old terminology.  I happen to agree that the existing terminology around
evaluation in R is less clear than it could be, and that `rlang` brings along
many interesting new concepts.  But I also believe it is better to enhance
flawed-but-workable precedents than unilaterally create Standard Sixteen.

<!-- CC BY SA 2.5 -->
<img src='https://imgs.xkcd.com/comics/standards.png' />

There is a Himalaya's worth of hills-to-die-on arguing the rights and wrongs of
`rlang`'s terminology and mechanics, but they are not that interesting.  Much
more fun is what quosures do, and how they do it.

# Preface: Carriages, Horses, and Damn Philosophers...

## Ghosts of Posts Past

If you are unfamiliar with R's evaluation model, and how it enables
"Non-Standard Evaluation" (NSE), now is a good time to check out last week's
[post on the topic][1].  To understand this post it might help to know what
environments, "Enclosures" and "Environment Chains" are, the difference between
"Evaluation", "Calling", "Function", and "function Evaluation" Environments, and
what unevaluated commands are and how they are created.  All these are discussed
at length in that post.

## Concept: `local`

R provides the `local` function as a mechanism for creating and using ad-hoc
Evaluation Environments:

```{r}
a <- 1
num <- 2
local({
  a <- 100
  a + num
})
a + num
```

The `a` defined at the top-level (`a <- 1`) is masked by the one defined
inside the `local` Evaluation Environment, but only for commands evaluated
in that environment.  This is akin to how commands in function bodies
are evaluated in function Evaluation Environments.

## Quasi-quoi?

As we saw previously it is possible to capture unevaluated R commands with
[`quote` and `substitute`][3].  We can also manipulate them:

```{r}
mean_call <- function(cmd) {
  cmd2 <- quote(mean(NULL))     # create unevaled `mean(NULL)`
  cmd2[[2]] <- substitute(cmd)  # sub in user expression
  cmd2
}
mean_call(a + b)
```

The manipulation is awkward though.  Thankfully R provides `bquote` to
partially quote commands (or partially evaluate them, depending on your life
outlook - personally I'm more of a command-half-evaluated guy).  Let's look
first at a trivial example:

```{r}
bquote(mean(.(1 + 2)))
```

Compare to:

```{r}
quote(mean(1 + 2))
```

`quote` captures the entire command unevaluated.  `bquote` allows whatever is
inside the `.(...)` to be evaluated.  We can use `bquote` to recreate
`mean_call`:

```{r}
mean_call2 <- function(cmd) {
  bquote(mean(.(substitute(cmd))))
}
mean_call2(a + b)
```

Wait, but that's an unevaluated call.  Didn't we promise partial evaluation?  We
delivered, but it's not obvious because what was partially evaluated,
`substitute(cmd)`, produced an unevaluated command.  We can see that by
comparing to an implementation that uses `quote` instead of `bquote`:

```{r}
mean_call2a <- function(cmd) {
  quote(mean(substitute(cmd)))
}
mean_call2a(a + b)
mean_call2(a + b)
```

In this case we can see that `quote` does not evaluate `substitute(cmd)`,
whereas `bquote` does.

How does this all relate to quasi-quotation?  Turns out partial evaluation
**is** quasi-quotation.  It's just that [quasi-quotation][4] was coined by the
philosopher [Willard Van Orman Quine][5], and a philosopher's thought is to
arcaneness what Midas's touch is to gold[^resentment].  Lisp adopted the
terminology, and `rlang` brought it explicitly to R.

So what's the etymology of R's `bquote`?  Turns out that in certain Lisp
dialects such as Scheme the equivalent to the R `quote` function is a single
quote `'`, and quasi-quoting (partial evaluation) is done with
the "backquote" <code>&#x60;</code>.  So R's quasi-quoting function is named
after the name of the <code>&#x60;</code>.

# About Them Quosures

Just as `quote`, `substitute`, and `bquote`  do, quosures capture unevaluated
R commands.

```{r}
library(rlang)
a <- 1
(qrlang <- quo(a + 1))   # create a quosure with quo
```

Additionally as you can see above, quosures record the environment the command
would have been evaluated it, had it not been captured.  Unlike typical
unevaluated commands, quosures do no resolve when the are `eval`ed:

```{r}
eval(qrlang)
```

We'll see why this is in a little bit, but until then we can use `eval_tidy` to
evaluate quosures:

```{r}
eval_tidy(qrlang)
```

Things are more interesting with more complex environments.  Let's compare
commands captured with `quote` vs `quo` in an ad-hoc environment created with
[`local`](# concept-local):

```{r}
a <- 1
lang <- local({
  a <- 100
  list(
    base=quote(a + 1),
    rlang=quo(a + 1)
  )
})
eval(lang[['base']])
eval_tidy(lang[['rlang']])
```

Normal quoted language resolves `a` in the environment in which it is
`eval`ed.  The quosure instead resolves `a` in the Evaluation Environment in
which it was **created**.  Pretty neat.

# Quosures in Functions

Let's build a simple NSE function to compute means in the context of of a
user-supplied data frame.  First with base:

```{r}
mean_with_base <- function(dat, cmd) {
  cmd <- substitute(cmd)           # capture user command
  cmd <- bquote(mean(.(cmd)))      # wrap it in `mean`
  eval(cmd, dat, parent.frame())
}
```

We'll use it to compute mean engine displacement of `mtcars` in liters:

```{r}
l.per.cubic.i <- 100 / 2.54^3
mean_with_base(mtcars, disp * l.per.cubic.i)
```

Next with `rlang`:

```{r}
mean_with_rlang <- function(dat, cmd) {
  cmd <- enquo(cmd)                # capture user command
  cmd <- quo(mean(!!cmd))          # wrap it in `mean`
  eval_tidy(cmd, dat)
}
mean_with_rlang(mtcars, disp * l.per.cubic.i)
```

`enquo` is the equivalent to `substitute`.  `quo` also supports quasi-quotation
(partial evaluation) out of the box using `!!` instead of `.()`.

So far, so good.  But why bother with `rlang`?  As we'll see shortly `rlang`
adds some pixie dust.

# The Power Of Quosures

Let's up the difficulty a little bit.  Bad NSE implementations often break when
we use names mapped in to environments other than the global environment.  Let's 
add a bad version of `l.per.cubic.i` to the global environment to see if our
functions find the correct version in the Calling Environment:

```{r}
l.per.cubic.i <- NA
local({
  l.per.cubic.i <- 2.54^3 / 1000
  list(
    base=mean_with_base(mtcars, disp * l.per.cubic.i),
    rlang=mean_with_rlang(mtcars, disp * l.per.cubic.i)
  )
})
```

Neither is fooled.  But let's take it a step further by creating a decoy `mean`
in the Calling Environment.  Normally commands in functions bodies are shielded
from external function definitions because they look up names in their internal
Evaluation Environments.  But here what allowed `mean_with_base` to find the
correct `l.per.cubic.i` trips it up:

```{r}
l.per.cubic.i <- NA
local({
  mean <- function(...) 42
  l.per.cubic.i <- 2.54^3 / 1000
  list(
    base=mean_with_base(mtcars, disp * l.per.cubic.i),
    rlang=mean_with_rlang(mtcars, disp * l.per.cubic.i)
  )
})
```

`mean_with_base` resolved the wrong `mean`, and would have failed even if it was
defined in a package.  `rlang`'s version on the other hand works fine.
Ultimately the issue is that in the command generated by `mean_with_*`:

<pre><code><span style='background-color: #aaffaa;'>mean</span>(<span
style='background-color: #aaaaff;'>disp * l.per.cubic.i</span>)</code></pre>

We need <code style='background-color: #aaffaa;'>mean</code> to be resolved
according to the Function Environment, but <code style='background-color:
#aaaaff;'>disp * l.per.cubic.i</code> in the Calling Environment.  R, however,
does not allow more than one Environment Chain at a time.

With quosures it works fine right out of the box.  Of course we can fix the R
version by pre-resolving `mean` as we did in [the prior post][2], but that's
extra work.  What sorcery allows `quosures` to evaluate a single command in
multiple environments, when R itself does not allow it?

# Is It Magic?

Let's peel the curtain back a bit:

```{r}
class(quo(a + b))
unclass(quo(a + b))
```

Quosures are formulas, which we can see both from the class, and also from the
tilde (`~`), once we side-step the quosure print method.  Formulas are an odd
duck in the R world: a form of self-quoting command that also captures the
Evaluation Environment in which it is created.  Mostly they are used to
implement Domain Specific languages such as regression model or plot
specifications.  Because they can contain arbitrary R commands as well as
environments they are appealing as a vehicle for quosures.

They do have a big draw-back for implementations that intend to evaluate the
command.  The quoting is triggered by the `~`, but the `~` remains in the
captured command:

```{r}
~a + b
```

This means you cannot evaluate the captured command as evaluating it just leads
to the command quoting itself again!

```{r}
eval(~a + b)
```

`eval_tidy` does something quite clever to force evaluation: it replaces `~`
with an internal version that self-evaluates quosures.  Here is a
hack-implementation to demonstrate:

```{r}
eval_tidyish <- function(cmd) {
  env <- new.env(parent=parent.frame())
  env[['~']] <- function(...) {# replace `~` with our version
    call <- sys.call()
    env <- environment(call)   # recover formula env from call
    eval(call[[2]], env)
  }
  eval(cmd, env)
}
```

`eval_tidyish` creates an environment that contains a self-evaluating version of
`~`, and evaluates the command therein.  We can recover the formula environment
from the call to the formula, which is fortunate as otherwise this trick would
not work.  Let's try it:

```{r}
a <- 666
q1 <- local({
  a <- 42
  ~ a + 1
})
q1
eval_tidyish(q1)
```

With more emotion:

```{r echo=FALSE}
old.opt <- options(digits=4)
```
```{r}
q2 <- local({
  a <- 10
  ~ a * 43
})
bquote(.(q1) / .(q2))
eval_tidyish(bquote(.(q1) / .(q2)))
eval_tidyish(bquote(.(q1) / .(q2) + a))
```
```{r echo=FALSE}
options(old.opt)
```

Ha, half-assed quosures.  These don't even support adding data to the
Environment Chain, deity-forbid using a formula as a _formula_, and they'll give
you a severe case of operator-precedence-anxiety.  But we do bottle that
multiple-Environment-Chain magic in a handful of lines of code.

# Natural Quosures

Formulas seem like a natural fit for quosure-like objects, but they are not.
Their resistance to evaluation is a terrible trait.  It requires a custom
evaluator along with a additional logic to preserve non-quosure `~` behavior,
which even then remains [subtly affected][23].  There is also the recovery of
the formula environment from the call stack, which works but feels a bit
uncomfortable to me[^uncomfortable].

But R doesn't offer any other functions that hold unevaluated command and
Enclosures, so what are we to do?  Let's cheat.

In the previous post we [resolved the two Environment Chain dilemma][2] by
pre-resolving the function along one Environment Chain and embedding the result
in the otherwise unevaluated command.  The key learning is that it is possible
to embed non-language objects in commands.  It's not even really cheating.  R
does this as it evaluates commands, evaluating leaves in the command tree until
the tree is fully evaluated and the result is returned.

Which leads us to:

```{r}
quo_with <- function(x, env=parent.frame()) {
  force(env)  # otherwise parent.frame may be called in a different env
  bquote(with(.(env), .(substitute(x))))
}
quo_with(a + 1)
```

What the hell is that?  It's an unevaluated call to `with` with an
alive-and-kicking environment embedded as the `data` argument.  That means that
when we evaluate the command, the provided sub-command will be evaluated in the
context of that environment.  Let's try it out:

```{r}
a <- 666
q1 <- local({
  a <- 42
  quo_with(a + 1)
})
q1
eval(q1)
```

Boom!  We successfully captured the `a <- 42` from the local environment, even
though we evaluated the command in the top-level environment where the mapping
is `a <- 666`.

Let's up the difficulty:

```{r echo=FALSE}
old.opt <- options(digits=4)
```
```{r}
q2 <- local({
  a <- 10
  quo_with(a * 43)
})
(q3 <- bquote(.(q1) / .(q2)))
eval(q3)
(q4 <- bquote(.(q1) / .(q2) + a))
eval(q4)
```
```{r echo=FALSE}
options(old.opt)
```

Another benefit is that the semantics of the command are obvious to anyone
familiar with `with` even absent any special print methods.  And there is no
operator-precedence-anxiety.  And all that in two lines of code!


Two obvious features are missing: quasi-quotation (a.k.a. backquoting /
partial-evaluation), and data masks.  First backquoting, which we'll do with
`bquo_with`.  Let's look at a usage example before the implementation:

```{r}
a <- 666
q5 <- local({
  q <- bquo_with(a + .(a))
  a <- 42   # assign to `a` AFTER bquo_with!
  q
})
q5
eval(q5)
```

The implementation is trickier (comments include what interim-steps
would contain with the above inputs):

```{r}
bquo_with <- function(x, env=parent.frame()) {
  force(env)
  exp <- substitute(x)          # a + .(a)         # user cmd
  exp <- bquote(bquote(.(exp))) # bquote(a + .(a)) # wrap in bquote
  exp <- eval(exp, env)         # a + 666          # eval in call env
  bquote(with(.(env), .(exp)))
}
```

We need to generate a `bquote` command to evaluate in the Calling Environment,
which leads to the odd-looking `bquote(bquote(.(exp)))`.  But this is an
implementation detail that the user of `bquo_with` need not worry about.

Data masking requires us to add data, often a data frame, to the Evaluation
Chain.  We will do that by replacing the environments embedded in the `with`
statements with a mask environment that has for Enclosure the replaced
environment:

```{r}
mask <- function(cmd, dat) {
  if(is.language(cmd) && length(cmd) > 1L) {
    if(cmd[[1L]] == as.name('with') && is.environment(cmd[[2L]])) {
      cmd[[2L]] <- list2env(dat, parent=cmd[[2L]])
    }
    cmd[2L:length(cmd)] <- lapply(cmd[2L:length(cmd)], mask, dat)
  }
  cmd
}
```

R commands are nested lists of sub-commands, so most of the function is
recursion through those lists.  The key line is:

```{r}
      cmd[[2L]] <- list2env(mask, parent=cmd[[2L]])
```

There we replace the existing environment with one derived from the data
mask that is Enclosed by the replaced environment.  Let's try it:

```{r}
q6 <- local({
  a <- 42
  quo_with(a + b + 1)
})
b <- 1
eval(q6)
q7 <- mask(q6, list(b=1000))
eval(q7)
```



```{r}


quo_arg_with <- function(x, env=parent.frame(), caller=parent.frame(2)) {
  force(caller)
  exp.s <- eval(bquote(.(substitute)(.(substitute(x)))), env)
  bquote(base::with(.(caller), .(exp.s)))
}

a <- 17
b <- 59
w <- local({
  b <- 1e6
  quo_with(a + b)
})
exp <- local({
  b <- 100
  x <- 3
  quo_with(x + 1 + .(w))
})
exp
eval(exp)

w <- 1
local({
  w <- 2
  f <- function(x) {
    w <- 5
    quo_arg_with(x)
  }
})
eval(f(w))
```





One possible way `eval_tidy` could work is by traversing the captured command,
finding each of the quosures, and "manually" evaluating them in their
environments until only a command without quosures is .

Let's peel the curtain bac a
So how does


I hate magic.  When something is magic it means I don't understand it.  Sure, I
can't need to understand everything around me, but when there is something I'm
working closely with, particularly related to programming,



# Interlude - Rant

Feel free to skip this section; it is mostly a rant about names.  There is some
irony that it's coming from the guy who a few paragraphs ago was just
complaining about philosophers...

I wish `rlang` had focused more on harmonizing with precedent in R over a more
_tabula rasa_ approach.  I understand the appeal of the latter, and `rlang`'s
absolute right to pursue it, but believe it more important to build on existing
conventions where possible.  Instead we ended up with things like:

* `rlang::enquo` being completely different to `base::enquote`.
* `rlang::quo` closer to `base::bquote` than `base::quote`.
* `rlang::expr` closest to `base::bquote`, but `base::expression` is something
  else entirely.
* `!!`, a completely new and foreign implementation of an existing concept
  `.()`, that also overloads an existing operator.

There are reasonable arguments for why things ended up where they are.  The
existing names are not perfect, and some are downright bad.  I know the `rlang`
team devoted a lot of time and effort coming up with names, including trying to
work with precedent.  But I can't help but look at the result and think the
balance of priorities is off.  Obviously this is just my opinion, and getting it
is the risk you take when you visit my blog.

A big part of my reaction is that I think **objectively** perfect names are both
impossible and unnecessary.  Impossible because we cannot compress the entire
semantics of functions into names.  Unnecessary because we as humans are
good at keeping semantics anchored to arbitrary names once we learn them.  That
language exists is proof enough of that.  Names just need to be good enough that
they don't actively impede our learning of them, and when possible **hint** at
their purpose without causing confusion[^alternate-names].

A useful thing to do when we add functions with similar semantics to existing
ones is to echo the lexical roots **in the same way they are used in the
language**.  If the concepts don't exist in the language, then sure, let's try
to hang on to an external reference.  If the existing names are really terrible,
then let's make up or own **distinct** lexicon.

Languages are mostly arbitrary.  The only reason they work is everyone agrees
on the meanings and use of the symbols they comprise.  That agreement **is** the
language; fractures in it reduce the value of the language as a whole.

Okay, enough of that.  Now for the good stuff.




## Quasi-quoi?

Our problem, as the authors of of`mean_by_grp`, is to ensure that <code
style='background-color: #aaffaa;'>mean</code> above resolves according to
`mean_by_grp`'s Evaluation Environment, while <code style='background-color:
#aaaaff;'>Sepal.Length / Sepal.Width * M</code> resolves in an Environment
Chain that connects to the Calling Environment.  If we don't do the former then
we risk conflict with user defined functions as in:


the wrong one is

The latter is important if the
user happens to reference variables from their environment in their command.



It is generated and run by `mean_by_grp`, a function that computes the mean of
user-supplied commands within a `data.frame`


provided by the user and is intended to resolve in an Environment Chain that
passes through the Calling Environment.  On the other hand,  was added by our function to the
user-supplied command, and we need to make sure that it resolves


Jkk
Let

This happens because we have created a
function, `mean_by_grp`, that computes the mean of a user supplied command
within groups of a data frame.  So we have

This was the command in question:



to
be resolved according to the Calling Environment.  We can only evaluate a
command in a single environment, so we're stuck.

blue


The problem is that we need <code style='background-color:
#aaffaa;'>mean</code> to be resolved according to our function's Evaluation
Environment, but




In the previous post we ran into an interesting situation

I could bore you to tears with with my opinions about the rights and wrongs
about `rlang`'s terminology, I'

Jkk
but that would not be very useful.  Instead, I'll
bore you to tears


in

many people were

I think
A big part of the bad &#x1f92f; I think stemmed from a desire
to reset the terminology around standard and non-standard evaluation while at
the same time



were many different types of
ranging from "holy &
one experienced I
I think
varied

Part of the challenge with them is that they are

intended for use in
a somewhat arcane part of R
address a somewhat
arcane

<!-- this needs to become a shortcode -->
<img
  id='front-img' src='/front-img/default.png'
  class='post-inset-image'
/>


# What does it for self eval formula

* Override `~` in the mask environment
* Need access to the actual formula somehow.  We can get the call with
  `sys.call`.  And if the environment is attached to the unevaluated quosure
  itself then it can be retrieved from `sys.call`, and it seems that's what
  !! as that's how it works does?

```{r eval=FALSE}

`~` <- function(...) {
  call <- sys.call()
  env <- parent.frame()
  is.quo <- function(x)
    is.call(x) && is.environment(attr(x, '.Environment')) &&
    inherits(x, 'bquosure')
  if(!is.quo(call)) {
    tilde <- get('~', mode='function', envir=env)
    eval(bquote(.(tilde))(...), env)
  } else {
    eval(substitute(list(...))[[3]], attr(x, '.Environment'))
  }
}

library(rlang)
`~` <- function(...) stop('boom')
~ 1 + 1
x <- quo(~1 + 1)
eval_tidy(x)
rm(`~`)
x <- quo(~1 + 1)
eval_tidy(x)



```

# Original Garbage

# References

* [TidyEval Bookdown][21]

# Conclusions

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<p id='feedback-cont'></p>

# Appendix

## Acknowledgments

## Session Info

[1]: /2020/05/05/on-nse/
[2]: /2020/05/05/on-nse/#call-assembly
[3]: /2020/05/05/on-nse/#nse-in-functions
[4]: https://en.wikipedia.org/wiki/Quasi-quotation
[5]: https://en.wikipedia.org/wiki/Willard_Van_Orman_Quine
[6]: https://www.cs.unm.edu/~williams/cs491/quasiquote.ps
[22]: https://tidyeval.tidyverse.org/
[23]: https://github.com/r-lib/rlang/issues/924

[^alternatives]: Another solution would be to compute the user
  command by group first, and subsequently `lapply` `mean` over the resulting
  values.  This is a better solution, except for that it side-steps the problem
  we are trying to highlight, perhaps results in higher peak memory usage as we
  must hold the user command evaluated for every group in memory, and it only
  works if the user command contains no aggregating functions.
[^shortcomings]: Likely the most problematic one is that if the user command
  causes an error, the error backtrace will display the function in full rather
  than the name of the function, which may be confusing.
[^resentment]: Oh no, I bear no resentment whatsoever to philosopher's whose
  works were required readings for me back in the day.
[^alternate-names]: This is a set of names I came up with in five minutes for
  the primary `rlang` names:
  * bquo <- expr
  * quo_arg <- enexpr
  * quo_dots <- enexprs
  * bquosure <- quo
  * quosure_arg <- enquo
  * quosure_dots <- enquos (alt: bquosure_args)
[^uncomfortable]: I don't think it is documented that the attributes of call
  that was evaluated will be put along with the call onto the call stack.
  Maybe there is some implicit guarantee that this will always be the case, or
  perhaps different interpretation 
  it's one of those things
  I wouldn't have been
  surprised if only the actual call was put on the stack.


