---
title: Quosures
author: ~
date: '2020-02-17'
slug: quosures
categories: [r]
tags: [meta-program,rlang]
image: /front-img/default.png
imagerect: ~
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: Front page summary
output:
  blogdown::html_page:
    keep_md: yes
    md_extensions: +raw_attribute
---
```{r echo=FALSE, child='../../static/chunks/init.Rmd'}
```

# Quosures?

Quosures first showed up on the scene as part of `rlang` about three years ago
to a collective &#x1F92F;.  There is good and bad &#x1F92F;, and quosures
delivered both in spades.  The good: what they do is powerful, and how they do
it at a minimum _seems_ magical.  The bad: new terminology and mechanics
confused a lot of people.  Since then the developers have devoted substantial
time and effort to make quosures and `rlang` in general more accessible.

There are two obvious reasons to introduce new terminology: the existing
terminology is bad, or there are new features that cannot be expressed in the
old terminology.  I happen to agree that the existing terminology around
evaluation in R is less clear than it could be, and that `rlang` brings along
many interesting new concepts.  But I also believe it is better to enhance
flawed-but-workable precedents than unilaterally create Standard Sixteen.

<!-- CC BY SA 2.5 -->
<img src='https://imgs.xkcd.com/comics/standards.png' />

There is a Himalaya's worth of hills-to-die-on arguing the rights and wrongs of
`rlang`'s terminology and mechanics, but they are not that interesting.  Much
more fun is what quosures do, and how they do it.

# Preface: Carriages, Horses, and Damn Philosophers...

## Ghosts of Posts Past

If you are unfamiliar with R's evaluation model, and how it enables
"Non-Standard Evaluation" (NSE), now is a good time to check out last week's
[post on the topic][1].  To understand this post it might help to know what
environments, "Enclosures" and "Environment Chains" are, the difference between
"Evaluation", "Calling", "Function", and "function Evaluation" Environments, and
what unevaluated commands are and how they are created.  All these are discussed
at length in that post.

And yes, that post was originally a footnote in this post, but once I hit triple
nested recursive footnotes I had to perform surgery.  Prognosis remains murky.

## Concept: `local`

R provides the `local` function as a mechanism for creating and using ad-hoc
Evaluation Environments:

```{r}
a <- 1
num <- 2
local({
  a <- 100
  a + num
})
a + num
```

The `a` defined at the top-level (`a <- 1`) is masked by the one defined
inside the `local` Evaluation Environment, but only for commands evaluated
in that environment.  This is akin to how commands in function bodies
are evaluated in function Evaluation Environments.

## Quasi-quoi?

As we saw previously it is possible to capture unevaluated R commands with
`quote` and `substitute`[3].  We can also manipulate them:

```{r}
mean_call <- function(cmd) {
  cmd2 <- quote(mean(NULL))     # create unevaled `mean(NULL)`
  cmd2[[2]] <- substitute(cmd)  # sub in user expression
  cmd2
}
mean_call(a + b)
```

The manipulation is awkward though.  Thankfully R provides `bquote` to
partially quote commands (or partially evaluate them, depending on your life
outlook - personally I'm more of a command-half-evaluated guy).  Let's look
first at a trivial example:

```{r}
bquote(mean(.(1 + 2)))
```

Compare to:

```{r}
quote(mean(1 + 2))
```

`quote` captures the entire command unevaluated.  `bquote` allows whatever is
inside the `.(...)` to be evaluated.  We can use `bquote` to recreate
`mean_call`:

```{r}
mean_call2 <- function(cmd) {
  bquote(mean(.(substitute(cmd))))
}
mean_call2(a + b)
```

Wait, but that's an unevaluated call.  Didn't we promise partial evaluation?  We
delivered, but it's not obvious because what was partially evaluated,
`substitute(cmd)`, produced an unevaluated command.  We can see that by
comparing to an implementation that uses `quote` instead of `bquote`:

```{r}
mean_call2a <- function(cmd) {
  quote(mean(substitute(cmd)))
}
mean_call2a(a + b)
```

In this case we can see that what we wanted evaluated and injected into the
command was not.

How does this all relate to quasi-quotation?  Turns out partial evaluation
**is** quasi-quotation.  It's just that [quasi-quotation][4] was coined by the
philosopher [Willard Van Orman Quine][5], and a philosopher's thought is to
arcaneness what Midas's touch is to gold[^resentment].  Lisp adopted the
terminology, and `rlang` brought it explicitly to R.

So what's the etymology of R's `bquote`?  Turns out that in certain Lisp
dialects such as Scheme the equivalent to the R `quote` function is a single
quote `&#x60;`, and quasi-quoting (partial evaluation) is done with the
"backquote" `&#x60;`.  So R's quasi-quoting function is named after the name of
the `&#x60;`.

# About Them Quosures

Just as `quote` and `substitute` do, quosures capture unevaluated R commands.
```{r}
library(rlang)
a <- 1
(qbase <- quote(a + 1))
(qrlang <- quo(a + 1))   # create a quosure with quo
```

Additionally as you can see above, quosures record the environment the command
would have been evaluated it, had it not been captured.  Finally, they do not
self-evaluate even when `eval`ed:

```{r}
eval(qbase)
eval(qrlang)
```

`rlang` provides `eval_tidy` to evaluate quosures:

```{r}
eval_tidy(qrlang)
```

Things get more interesting when environments get more complex:

```{r}
a <- 1
lang <- local({
  a <- sample(100:200, 1)
  list(
    base=quote(a + 1),
    rlang=quo(a + 1)
  )
})
eval(lang[['base']])
eval_tidy(lang[['rlang']])
```

Normal quoted language resolves `a` in the environment in which it is
`eval`ed.  The quosure instead resolves `a` in the Evaluation Environment in
which it was **created**.  Pretty neat.

# Quosures in Functions

Let's build a simple NSE function to compute means in the context of of a
user-supplied data frame.  First with base:

```{r}
mean_with_base <- function(dat, cmd) {
  cmd <- substitute(cmd)           # capture user command
  cmd <- bquote(mean(.(cmd)))      # wrap it in `mean`
  eval(call, cmd, parent.frame())
}
```

We'll use it to compute mean engine displacement of `mtcars` in liters:

```{r}
l.per.cubic.i <- 2.54^3 / 100
mean_with_base(mtcars, disp * l.per.cubic.i)
```

`rlang::enquo` can be used instead of `substitute`.  Additionally, most of
`rlang`'s capturing functions support quasi-quotation (partial evaluation) out
of the box with the `!!` "operator", so `quo` is actually closer to `bquote`
than to `quote`:

```{r}
mean_with_rlang <- function(dat, cmd) {
  cmd <- enquo(cmd)                # capture user command
  cmd <- quo(mean(!!cmd))          # wrap it in `mean`
  cmd
}
mean_with_rlang(mtcars, disp * l.per.cubic.i)
```

It worked!  But, why bother?  As we'll see shortly `rlang` adds some pixie dust
that makes the `rlang` version more powerful.

# Interlude - Rant

Feel free to skip this section, it is mostly a rant about names.  There is some
irony that it's coming from the guy that was just complaining about
philosophers...

I wish `rlang` had focused more on harmonizing with precedent in R over a more
_tabula rasa_ approach.  I understand the appeal of the latter, but personally
believe it more important to build on top of existing conventions where
possible.

Further exacerbating the issues associated with going for Standard Sixteen is
that avoiding conflicts with precedent was low enough in priority to allow
for things like:

* `rlang::enquo` and `base::enquote` are completely different
* `rlang::quo` is closer to `base::bquote` than `base::quote`
* `rlang::expr` is closest to `base::bquote`, but `base::expression` is
  something else again

There are reasonable arguments for why things ended up where they are.  The
existing names are not perfect.  I know the `rlang` team devoted a lot of time
and effort coming up with names, including trying to work with precedent.  But I
can't help but look at the result and think the balance of priorities is off.

A big part of my reaction to all this is that I think **objectively** perfect
names are both impossible and unnecessary.  Impossible because we cannot
compress the entire semantics of functions into single words.  Unnecessary
because we as humans are extremely good at anchoring semantics to arbitrary
names.  That you can look at the collection of symbols that is this blogpost and
understand (hopefully) what I'm ranting about is proof of this.

We are good at holding the name-anchor semantics, but it does take work to first
establish them.  It's a lot easier to remember a language once learnt, than
learn a new one.  This is why I think changing anchors, or worse knocking out an
existing anchor to replace it with a new one, is counterproductive.



What is more difficult is to 

That's what language is.



There
is no question the existing
names leave a bit to be desired, but I'm still left 



Another issue I have with the `rlang` approach is that in implementing Standard
Sixteen 

* `rlang

From this we can see the rough correspondence between tidy and base NSE:

* `enquos(...)` => `substitute(list(....))` and by extension:
* `enquo(x)` => `substitute(x)`
* `quo(fun(!!x))` => `bquote(fun(.(x)))`
* `eval_tidy(call, data=.data)` => `eval(call, .data, parent.frame())`

The problem with this is that the tidy functions re-use the same lexical roots
as base, but mix-up the semantics.  For example base also has `enquote`, but
that does something completely different from `rlang::enquo`.  Similarly base
has `quote` and `bquote`, but `rlang::quo` is closer to `bquote` than `quote`,
and in fact `rlang::expr` is the closest thing to `bquote`.  To further add to
the confusing base also has `expression`, which is something different yet
again.

that we
end up with the following:



was a lower priority than 

What further 

The value of standards is in broad adoption.


A universal imperfect standard is better than 

What I find particularly frustrating jbbb

however flawed.


avoid Standard Sixteen, particularly a Standard Sixteen
that 

with a parallel but partially
overlapping terminology.  

This 

seems 
I also think trying to come up
with perfect names is futile.




Now, let's see what happens if we stress things a bit:

```{r}
local({
  mean <- function(...) stop('nope')
  mean_with_base(mtcars, cyl)
})
```

Oops.  `mean_with_base` resolved the wrong `mean`.  `rlang`'s version on the
other hand works fine:

```{r}
local({
  mean <- function(...) stop('nope')
  mean_with_rlang(mtcars, cyl)
})
```

Ultimately the issue is that in the generated call:

<pre><code><span style='background-color: #aaffaa;'>mean</span>(<span
style='background-color: #aaaaff;'>cyl</span>)</code></pre>



## Quasi-quoi?

Our problem, as the authors of of`mean_by_grp`, is to ensure that <code
style='background-color: #aaffaa;'>mean</code> above resolves according to
`mean_by_grp`'s Evaluation Environment, while <code style='background-color:
#aaaaff;'>Sepal.Length / Sepal.Width * M</code> resolves in an Environment
Chain that connects to the Calling Environment.  If we don't do the former then
we risk conflict with user defined functions as in:


the wrong one is

The latter is important if the
user happens to reference variables from their environment in their command.



It is generated and run by `mean_by_grp`, a function that computes the mean of
user-supplied commands within a `data.frame`


provided by the user and is intended to resolve in an Environment Chain that
passes through the Calling Environment.  On the other hand,  was added by our function to the
user-supplied command, and we need to make sure that it resolves


Jkk
Let

This happens because we have created a
function, `mean_by_grp`, that computes the mean of a user supplied command
within groups of a data frame.  So we have

This was the command in question:



to
be resolved according to the Calling Environment.  We can only evaluate a
command in a single environment, so we're stuck.

blue


The problem is that we need <code style='background-color:
#aaffaa;'>mean</code> to be resolved according to our function's Evaluation
Environment, but




In the previous post we ran into an interesting situation

I could bore you to tears with with my opinions about the rights and wrongs
about `rlang`'s terminology, I'

Jkk
but that would not be very useful.  Instead, I'll
bore you to tears


in

many people were

I think
A big part of the bad &#x1f92f; I think stemmed from a desire
to reset the terminology around standard and non-standard evaluation while at
the same time



were many different types of
ranging from "holy &
one experienced I
I think
varied

Part of the challenge with them is that they are

intended for use in
a somewhat arcane part of R
address a somewhat
arcane

<!-- this needs to become a shortcode -->
<img
  id='front-img' src='/front-img/default.png'
  class='post-inset-image'
/>

# Implementing Quosures with `with`?

```{r eval=FALSE}
quo_with <- function(x, env=parent.frame()) {
  force(env)
  exp <- eval(call('bquote', substitute(x)), parent.frame())
  bquote(base::with(.(env), .(exp)))
}
quo_arg_with <- function(x, env=parent.frame(), caller=parent.frame(2)) {
  force(caller)
  exp.s <- eval(bquote(.(substitute)(.(substitute(x)))), env)
  bquote(base::with(.(caller), .(exp.s)))
}

a <- 17
b <- 59
w <- local({
  b <- 1e6
  quo_with(a + b)
})
exp <- local({
  b <- 100
  x <- 3
  quo_with(x + 1 + .(w))
})
exp
eval(exp)

w <- 1
local({
  w <- 2
  f <- function(x) {
    w <- 5
    quo_arg_with(x)
  }
})
eval(f(w))
```

# What does it for self eval formula

* Override `~` in the mask environment
* Need access to the actual formula somehow.  We can get the call with
  `sys.call`.  And if the environment is attached to the unevaluated quosure
  itself then it can be retrieved from `sys.call`, and it seems that's what
  !! as that's how it works does?

```{r eval=FALSE}

`~` <- function(...) {
  call <- sys.call()
  env <- parent.frame()
  is.quo <- function(x)
    is.call(x) && is.environment(attr(x, '.Environment')) &&
    inherits(x, 'bquosure')
  if(!is.quo(call)) {
    tilde <- get('~', mode='function', envir=env)
    eval(bquote(.(tilde))(...), env)
  } else {
    eval(substitute(list(...))[[3]], attr(x, '.Environment'))
  }
}

library(rlang)
`~` <- function(...) stop('boom')
~ 1 + 1
x <- quo(~1 + 1)
eval_tidy(x)
rm(`~`)
x <- quo(~1 + 1)
eval_tidy(x)



```

# Original Garbage

# References

* [TidyEval Bookdown][21]

# Conclusions

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<p id='feedback-cont'></p>

# Appendix

## Acknowledgments

## Session Info

[1]: /2020/05/05/on-nse/
[2]: /2020/05/05/on-nse/#call-assembly
[2]: /2020/05/05/on-nse/#nse-in-functions
[4]: https://en.wikipedia.org/wiki/Quasi-quotation
[5]: https://en.wikipedia.org/wiki/Willard_Van_Orman_Quine
[6]: https://www.cs.unm.edu/~williams/cs491/quasiquote.ps
[22]: https://tidyeval.tidyverse.org/

[^alternatives]: Another solution would be to compute the user
  command by group first, and subsequently `lapply` `mean` over the resulting
  values.  This is a better solution, except for that it side-steps the problem
  we are trying to highlight, perhaps results in higher peak memory usage as we
  must hold the user command evaluated for every group in memory, and it only
  works if the user command contains no aggregating functions.
[^shortcomings]: Likely the most problematic one is that if the user command
  causes an error, the error backtrace will display the function in full rather
  than the name of the function, which may be confusing.
[^resentment]: Oh no, I bear no resentment whatsoever to philosopher's whose
  works were required readings for me back in the day.


