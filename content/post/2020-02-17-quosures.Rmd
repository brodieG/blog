---
title: Quosures
author: ~
date: '2020-02-17'
slug: quosures
categories: [r]
tags: [meta-program,rlang]
image: /front-img/default.png
imagerect: ~
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: Front page summary
output:
  blogdown::html_page:
    keep_md: yes
    md_extensions: +raw_attribute
---
```{r echo=FALSE, child='../../static/chunks/init.Rmd'}
```

# Quosures?

Quosures first showed up on the scene as part of `rlang` about three years ago
to a collective &#x1F92F;.  There is good and bad &#x1F92F;, and quosures
delivered both in spades.  The good: what they do is powerful, and how they do
it at a minimum _seems_ magical.  The bad: new terminology and mechanics
confused a lot of people.  Since then the developers have devoted substantial
time and effort to make quosures and `rlang` in general more accessible.

There are two obvious reasons to introduce new terminology: the existing
terminology is bad, or there are new features that cannot be expressed in the
old terminology.  I happen to agree that the existing terminology around
evaluation in R is less clear than it could be, and that `rlang` brings along
many interesting new concepts.  But I also believe it is better to enhance
flawed-but-workable precedents than unilaterally create Standard Sixteen.

<!-- CC BY SA 2.5 -->
<img src='https://imgs.xkcd.com/comics/standards.png' />

There is a Himalaya's worth of hills-to-die-on arguing the rights and wrongs of
`rlang`'s terminology and mechanics, but they are not that interesting.  Much
more fun is what quosures do, and how they do it.

# Preface: Carriages, Horses, and Damn Philosophers...

## Ghosts of Posts Past

If you are unfamiliar with R's evaluation model, and how it enables
"Non-Standard Evaluation" (NSE), now is a good time to check out last week's
[post on the topic][1].  To understand this post it might help to know what
environments, "Enclosures" and "Environment Chains" are, the difference between
"Evaluation", "Calling", "Function", and "function Evaluation" Environments, and
what unevaluated commands are and how they are created.  All these are discussed
at length in that post.

And yes, that post was originally a footnote in this post, but once I hit triple
nested recursive footnotes I had to perform surgery.  Prognosis remains murky.

## Concept: `local`

R provides the `local` function as a mechanism for creating and using ad-hoc
Evaluation Environments:

```{r}
a <- 1
num <- 2
local({
  a <- 100
  a + num
})
a + num
```

The `a` defined at the top-level (`a <- 1`) is masked by the one defined
inside the `local` Evaluation Environment, but only for commands evaluated
in that environment.  This is akin to how commands in function bodies
are evaluated in function Evaluation Environments.

## Quasi-quoi?

As we saw previously it is possible to capture unevaluated R commands with
`quote` and `substitute`[3].  We can also manipulate them:

```{r}
mean_call <- function(cmd) {
  cmd2 <- quote(mean(NULL))     # create unevaled `mean(NULL)`
  cmd2[[2]] <- substitute(cmd)  # sub in user expression
  cmd2
}
mean_call(a + b)
```

The manipulation is awkward though.  Thankfully R provides `bquote` to
partially quote commands (or partially evaluate them, depending on your life
outlook - personally I'm more of a command-half-evaluated guy).  Let's look
first at a trivial example:

```{r}
bquote(mean(.(1 + 2)))
```

Compare to:

```{r}
quote(mean(1 + 2))
```

`quote` captures the entire command unevaluated.  `bquote` allows whatever is
inside the `.(...)` to be evaluated.  We can use `bquote` to recreate
`mean_call`:

```{r}
mean_call2 <- function(cmd) {
  bquote(mean(.(substitute(cmd))))
}
mean_call2(a + b)
```

Wait, but that's an unevaluated call.  Didn't we promise partial evaluation?  We
delivered, but it's not obvious because what was partially evaluated,
`substitute(cmd)`, produced an unevaluated command.  We can see that by
comparing to an implementation that uses `quote` instead of `bquote`:

```{r}
mean_call2a <- function(cmd) {
  quote(mean(substitute(cmd)))
}
mean_call2a(a + b)
```

In this case we can see that what we wanted evaluated and injected into the
command was not.

How does this all relate to quasi-quotation?  Turns out partial evaluation
**is** quasi-quotation.  It's just that [quasi-quotation][4] was coined by the
philosopher [Willard Van Orman Quine][5], and a philosopher's thought is to
arcaneness what Midas's touch is to gold[^resentment].  Lisp adopted the
terminology, and `rlang` brought it explicitly to R.

So what's the etymology of R's `bquote`?  Turns out that in certain Lisp
dialects such as Scheme the equivalent to the R `quote` function is a single
quote `&#x60;`, and quasi-quoting (partial evaluation) is done with the
"backquote" `&#x60;`.  So R's quasi-quoting function is named after the name of
the `&#x60;`.

# About Them Quosures

Just as `quote` and `substitute` do, quosures capture unevaluated R commands.
```{r}
library(rlang)
a <- 1
(qbase <- quote(a + 1))
(qrlang <- quo(a + 1))   # create a quosure with quo
```

Additionally as you can see above, quosures record the environment the command
would have been evaluated it, had it not been captured.  Finally, they do not
self-evaluate even when `eval`ed:

```{r}
eval(qbase)
eval(qrlang)
```

`rlang` provides `eval_tidy` to evaluate quosures:

```{r}
eval_tidy(qrlang)
```

Things get more interesting when environments get more complex:

```{r}
a <- 1
lang <- local({
  a <- sample(100:200, 1)
  list(
    base=quote(a + 1),
    rlang=quo(a + 1)
  )
})
eval(lang[['base']])
eval_tidy(lang[['rlang']])
```

Normal quoted language resolves `a` in the environment in which it is
`eval`ed.  The quosure instead resolves `a` in the Evaluation Environment in
which it was **created**.  Pretty neat.

# Quosures in Functions

Let's build a simple NSE function to compute means in the context of of a
user-supplied data frame.  First with base:

```{r}
mean_with_base <- function(dat, cmd) {
  cmd <- substitute(cmd)           # capture user command
  cmd <- bquote(mean(.(cmd)))      # wrap it in `mean`
  eval(call, cmd, parent.frame())
}
```

We'll use it to compute mean engine displacement of `mtcars` in liters:

```{r}
l.per.cubic.i <- 2.54^3 / 100
mean_with_base(mtcars, disp * l.per.cubic.i)
```

`rlang::enquo` can be used instead of `substitute`.  Additionally, most of
`rlang`'s capturing functions support quasi-quotation (partial evaluation) out
of the box with the `!!` "operator", so `quo` is actually closer to `bquote`
than to `quote`:

```{r}
mean_with_rlang <- function(dat, cmd) {
  cmd <- enquo(cmd)                # capture user command
  cmd <- quo(mean(!!cmd))          # wrap it in `mean`
  cmd
}
mean_with_rlang(mtcars, disp * l.per.cubic.i)
```

It worked!  But, why bother?  As we'll see shortly `rlang` adds some pixie dust
that makes the `rlang` version more powerful.

# Interlude - Rant

Feel free to skip this section; it is mostly a rant about names.  There is some
irony that it's coming from the guy who a few paragraphs ago was just
complaining about philosophers...

I wish `rlang` had focused more on harmonizing with precedent in R over a more
_tabula rasa_ approach.  I understand the appeal of the latter, but personally
believe it more important to build on existing conventions where possible.
Maybe it would have bothered me less if Standard Sixteen was orthogonal to the
precedent, but instead we end up with things like:

* `rlang::enquo` being completely different to `base::enquote`.
* `rlang::quo` is closer to `base::bquote` than `base::quote`.
* `rlang::expr` is closest to `base::bquote`, but `base::expression` is
  something else again.
* `!!`, a completely new implementation of an existing concept that overloads an
  existing operator.

There are reasonable arguments for why things ended up where they are.  The
existing names are not perfect, and some are downright bad.  I know the `rlang`
team devoted a lot of time and effort coming up with names, including trying to
work with precedent.  But I can't help but look at the result and think the
balance of priorities is off.  Obviously this is just my opinion, and getting it
is the risk you take when you visit my blog.

A big part of my reaction is that I think **objectively** perfect names are both
impossible and unnecessary.  Impossible because we cannot compress the entire
semantics of functions into names.  Unnecessary because we as humans are
good at keeping semantics anchored to arbitrary names once we learn them.  That
language exists is proof enough of that.

One thing that is useful is when we add functions with similar semantics to
existing ones, to echo the lexical roots **in the same way they are used in the
language**.  If the concepts don't exist in the language, sure, let's try to
hang on to an external reference.  If the existing names are really terrible,
then let's make up or own **distinct** lexicon.

Languages are mostly arbitrary.  The only reason they work is everyone agrees
on the meanings of the symbols it comprises.  That agreement is the single most
important thing about a language.  When extending a language building on that
agreement should be paramount.

This still leaves establishing the initial name-semantics association.  If we're
implementing similar functionality to what already exists in the language we
can give our users semantic hints by appropriate use of existing lexical roots.
In some cases the existing names may be so bad that we wish to move on from
them, and maybe we develop an orthogonal lexicon.  But using the same names or
the same lexical roots in related fields in pursuit of **subjective** name
perfection is counterproductive.  IMO.




We could 

Conversely, we must be careful not to 



Jkk
have a choice: we can build on the existing lexical roots to echo existing
semantics for our users.

Jkk
, we can chose a
distinct lexicon, or we can ignore prece
optimize primarily around our view of what makes
names perfect.


So we
have a choice: we can chose names that 

the initial learning or refreshing 

While we are good at holding the name-semantic anchors, it does take work to
establish them.  There is value in picking names that provide clues about their
semantics.  

And the best way to do this is to reference existing semantics in
the common language, which in this case is R.  The converse is that it is
harmful to change the meaning of existing names, or to re-use existing lexical
roots in different ways within the same subject area.  Again, IMO.

The names need not be perfect, just good enough.  Objective perfection is not
possible, and subjective perfection at the cost of conflict with 

existing names
is a dubious proposition.

Naming things is hard, but the return on effort quickly becomes questionable
once you're 
Jkk

it's also not worth spending a lot of effort on it
once you get past 

it's also very much a diminishing returns 


eases the process of
learning them.


, so it does make sense to try to 
.

It's a lot easier to remember a language once learnt, than
learn a new one.  So is it helpful to riff on existing names when semantics are
related (e.g. `sub` and `gsub`).  

There is no question some name are better than others, but really all we need is
good enough.  **Objective** perfection is impossible, and subjective perfection
is of limited value.

so the best you can do is
subjective perfection, which while still difficult to achieve has limited value.

by virtue of being subjective.


does not have that
much value




.  `865eeb074045` 
or `b240b993d`

Some names are indeed better than others.  After all, it wouldn't do to name
functions 

, although I'll take a hexcode 
over a 
a name that conflicts with existing "base" names.  They should be in
a form we can reasonably remember, and if related to a particular concept, be
evocative of that.  If the concept already exists in R, then working with R's
terminology is best.  After all, if the lingua franca of R programmers is not R,
what on earth is?




On a related vein 

Of course `rlang` is free to do whatever it wants with its names.  Even the
notoriously finicky CRAN allows re-using existing names, whether those from the
base set of packages or otherwise.  And there are probably many other packages
that I'm not subjecting to my opinion that might be doing worse things
(hopefully none of mine).  But `rlang` is authored by influential individuals in
the R community and will affect how many learn about these topics, and we're
ending up with two dialects that are less compatible than they could be.



, and its a
shame that learning that way will not combine as well with learning 


.  


If you want
to claim 



If you
collaborate using R with others and learned the `rlang` way, but then run into
code written the original R way you'll struggle.   And vice-versa.


, 




meta-programming.  It means that
if you want to be fully conversant in meta-programming in R you need to learn
both ways of doing it.






Rstudio is extremely influential in the R
world so what they do affects 


like I am this one.


Imagine you are the first major grower of Asian pears in the U.S.  You need a
name for them.  Maybe call them pear-apples.  Nashi is fine too.  Even Asian
apples could work.  But would you call them figs?  Or pomgrenati, even if it so
happens that the trees that produce them are actually a cousin of pomegrenate
trees?


This is why I think changing anchors, or worse
knocking out an
existing anchor to replace it with a new one, is counterproductive.  




What is more difficult is to 

That's what language is.



There
is no question the existing
names leave a bit to be desired, but I'm still left 



Another issue I have with the `rlang` approach is that in implementing Standard
Sixteen 

* `rlang

From this we can see the rough correspondence between tidy and base NSE:

* `enquos(...)` => `substitute(list(....))` and by extension:
* `enquo(x)` => `substitute(x)`
* `quo(fun(!!x))` => `bquote(fun(.(x)))`
* `eval_tidy(call, data=.data)` => `eval(call, .data, parent.frame())`

The problem with this is that the tidy functions re-use the same lexical roots
as base, but mix-up the semantics.  For example base also has `enquote`, but
that does something completely different from `rlang::enquo`.  Similarly base
has `quote` and `bquote`, but `rlang::quo` is closer to `bquote` than `quote`,
and in fact `rlang::expr` is the closest thing to `bquote`.  To further add to
the confusing base also has `expression`, which is something different yet
again.

that we
end up with the following:



was a lower priority than 

What further 

The value of standards is in broad adoption.


A universal imperfect standard is better than 

What I find particularly frustrating jbbb

however flawed.


avoid Standard Sixteen, particularly a Standard Sixteen
that 

with a parallel but partially
overlapping terminology.  

This 

seems 
I also think trying to come up
with perfect names is futile.




Now, let's see what happens if we stress things a bit:

```{r}
local({
  mean <- function(...) stop('nope')
  mean_with_base(mtcars, cyl)
})
```

Oops.  `mean_with_base` resolved the wrong `mean`.  `rlang`'s version on the
other hand works fine:

```{r}
local({
  mean <- function(...) stop('nope')
  mean_with_rlang(mtcars, cyl)
})
```

Ultimately the issue is that in the generated call:

<pre><code><span style='background-color: #aaffaa;'>mean</span>(<span
style='background-color: #aaaaff;'>cyl</span>)</code></pre>



## Quasi-quoi?

Our problem, as the authors of of`mean_by_grp`, is to ensure that <code
style='background-color: #aaffaa;'>mean</code> above resolves according to
`mean_by_grp`'s Evaluation Environment, while <code style='background-color:
#aaaaff;'>Sepal.Length / Sepal.Width * M</code> resolves in an Environment
Chain that connects to the Calling Environment.  If we don't do the former then
we risk conflict with user defined functions as in:


the wrong one is

The latter is important if the
user happens to reference variables from their environment in their command.



It is generated and run by `mean_by_grp`, a function that computes the mean of
user-supplied commands within a `data.frame`


provided by the user and is intended to resolve in an Environment Chain that
passes through the Calling Environment.  On the other hand,  was added by our function to the
user-supplied command, and we need to make sure that it resolves


Jkk
Let

This happens because we have created a
function, `mean_by_grp`, that computes the mean of a user supplied command
within groups of a data frame.  So we have

This was the command in question:



to
be resolved according to the Calling Environment.  We can only evaluate a
command in a single environment, so we're stuck.

blue


The problem is that we need <code style='background-color:
#aaffaa;'>mean</code> to be resolved according to our function's Evaluation
Environment, but




In the previous post we ran into an interesting situation

I could bore you to tears with with my opinions about the rights and wrongs
about `rlang`'s terminology, I'

Jkk
but that would not be very useful.  Instead, I'll
bore you to tears


in

many people were

I think
A big part of the bad &#x1f92f; I think stemmed from a desire
to reset the terminology around standard and non-standard evaluation while at
the same time



were many different types of
ranging from "holy &
one experienced I
I think
varied

Part of the challenge with them is that they are

intended for use in
a somewhat arcane part of R
address a somewhat
arcane

<!-- this needs to become a shortcode -->
<img
  id='front-img' src='/front-img/default.png'
  class='post-inset-image'
/>

# Implementing Quosures with `with`?

```{r eval=FALSE}
quo_with <- function(x, env=parent.frame()) {
  force(env)
  exp <- eval(call('bquote', substitute(x)), parent.frame())
  bquote(base::with(.(env), .(exp)))
}
quo_arg_with <- function(x, env=parent.frame(), caller=parent.frame(2)) {
  force(caller)
  exp.s <- eval(bquote(.(substitute)(.(substitute(x)))), env)
  bquote(base::with(.(caller), .(exp.s)))
}

a <- 17
b <- 59
w <- local({
  b <- 1e6
  quo_with(a + b)
})
exp <- local({
  b <- 100
  x <- 3
  quo_with(x + 1 + .(w))
})
exp
eval(exp)

w <- 1
local({
  w <- 2
  f <- function(x) {
    w <- 5
    quo_arg_with(x)
  }
})
eval(f(w))
```

# What does it for self eval formula

* Override `~` in the mask environment
* Need access to the actual formula somehow.  We can get the call with
  `sys.call`.  And if the environment is attached to the unevaluated quosure
  itself then it can be retrieved from `sys.call`, and it seems that's what
  !! as that's how it works does?

```{r eval=FALSE}

`~` <- function(...) {
  call <- sys.call()
  env <- parent.frame()
  is.quo <- function(x)
    is.call(x) && is.environment(attr(x, '.Environment')) &&
    inherits(x, 'bquosure')
  if(!is.quo(call)) {
    tilde <- get('~', mode='function', envir=env)
    eval(bquote(.(tilde))(...), env)
  } else {
    eval(substitute(list(...))[[3]], attr(x, '.Environment'))
  }
}

library(rlang)
`~` <- function(...) stop('boom')
~ 1 + 1
x <- quo(~1 + 1)
eval_tidy(x)
rm(`~`)
x <- quo(~1 + 1)
eval_tidy(x)



```

# Original Garbage

# References

* [TidyEval Bookdown][21]

# Conclusions

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<p id='feedback-cont'></p>

# Appendix

## Acknowledgments

## Session Info

[1]: /2020/05/05/on-nse/
[2]: /2020/05/05/on-nse/#call-assembly
[2]: /2020/05/05/on-nse/#nse-in-functions
[4]: https://en.wikipedia.org/wiki/Quasi-quotation
[5]: https://en.wikipedia.org/wiki/Willard_Van_Orman_Quine
[6]: https://www.cs.unm.edu/~williams/cs491/quasiquote.ps
[22]: https://tidyeval.tidyverse.org/

[^alternatives]: Another solution would be to compute the user
  command by group first, and subsequently `lapply` `mean` over the resulting
  values.  This is a better solution, except for that it side-steps the problem
  we are trying to highlight, perhaps results in higher peak memory usage as we
  must hold the user command evaluated for every group in memory, and it only
  works if the user command contains no aggregating functions.
[^shortcomings]: Likely the most problematic one is that if the user command
  causes an error, the error backtrace will display the function in full rather
  than the name of the function, which may be confusing.
[^resentment]: Oh no, I bear no resentment whatsoever to philosopher's whose
  works were required readings for me back in the day.


