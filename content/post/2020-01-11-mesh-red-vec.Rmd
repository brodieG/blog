---
title: "Rtini Part 2: Let's Vectorize the $#!+ Out of It"
author: ~
date: '2020-01-11'
slug: mesh-red-vec
categories: []
tags: []
image: /front-img/default.png
imagerect: ~
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: Front page summary
output:
  blogdown::html_page:
    keep_md: true
    md_extensions: +raw_attribute
---

```{r echo=FALSE, child='../../static/chunks/init.Rmd'}
```
```{r echo=FALSE}
suppressMessages(library(ggplot2))
thm.blnk <- list(
  theme(
    axis.text.x=element_blank(), axis.text.y=element_blank(),
    axis.ticks.x=element_blank(), axis.ticks.y=element_blank(),
    panel.grid=element_blank()
  ),
  ylab(NULL),
  xlab(NULL)
)
```

# Header 1

<!-- this needs to become a shortcode -->
<img
  id='front-img' src='/front-img/default.png'
  class='post-inset-image'
/>

# Vectorize The Absolute $#!+ Out Of This

<div style='background-color: red;'>Update all links to split post</div>

I wish I could tell you that I carefully figured out all the intricacies of the
algorithm before I set off on my initial vectorization attempt, that I didn't
waste hours going cross-eyed at triangles poorly drawn on scraps of paper, and
that I didn't produce algorithms which closer inspection showed to be [laughably
incorrect][10].  Instead I'll have to be content with telling you that I did get
this to work eventually.  A victory that had it wasted lives like it wasted my
time would supplant those of Pyrrhus in the history books[^grandiloquence].

While I was generally aware that there is more to the problem than generating
equal sized triangles, I underestimated the degrees of freedom available and
wasted quite a bit of time going in circles as a result.  There are eight
different triangle orientations, split into two groups of four.  The groups
alternate between approximation levels.  Here we show the orientations for the
2nd and 3rd most granular levels of a 9 x 9 grid[^nine-nine].  The triangles
are colored by orientation.  For reasons that will become clearer later we call
the group with diagonal hypotenuses "Diagonal", and the one with
vertical/horizontal ones "Axis":<span id='sqr-vs-dia'></span>

```{r echo=FALSE}
source('../../static/script/_lib/plot.R')
source('../../static/script/mesh-viz/viz-lib.R')
source('../../static/script/mesh-viz/rtin-vec.R')
source('../../static/script/mesh-viz/rtin-vec2.R')
source('../../static/script/mesh-viz/extract-vec.R')
# source('static/script/mesh-viz/viz-lib.R')
# source('static/script/mesh-viz/rtin-vec.R')
err <- matrix(0, 9, 9)
# note diamond/square is backwards here from what we use in viz-lib
tri.di <- extract_mesh2(err, 1, 4)
tri.sq <- extract_mesh2(err, 1, 3)
tris <- tris_to_df(list(tri.sq, tri.di), err)
tris[['type']] <- factor(
  c(
    rep('Diagonal', length(unlist(tri.sq))),
    rep('Axis', length(unlist(tri.di)))
  ),
  levels=c('Diagonal', 'Axis')
)
points <- tris_to_df(seq_along(err), err)
p <- ggplot(tris) +
  geom_polygon(aes(x=x, y=y, group=id, fill=I(color))) +
  geom_point(data=points, aes(x=x, y=y), size=.5, shape=3, color='white') +
  facet_wrap(~type) +
  coord_fixed() +
  thm.blnk
pdim <- gtable_dim(ggplotGrob(p))
old.opt <- knitr::opts_chunk$get(c('fig.width', 'fig.height'))
do.call(knitr::opts_chunk$set, list(fig.width=pdim[1], fig.height=pdim[2]))
```
```{r square-vs-diamond, echo=FALSE}
p
do.call(knitr::opts_chunk$set, old.opt)
```

The "Diagonal" triangulation, the one with vertical and horizontal hypotenuses,
has just one tiling pattern.  A particular triangle "shape" (a specific color in
the plot above) must always be arranged in the same way relative to the other
shapes that share the vertex opposite their hypotenuses.  Any other arrangement
would prevent us from fully tiling the grid without breaking up triangles.

On the other hand the "Axis" triangulation, the one with the diagonal
hypotenuses, has multiple possible tilings:

```{r echo=FALSE}
# should have used the offsets for this, but we hadn't written that yet.
raw1 <- c(03,19,01, 03,19,21, 03,23,21, 03,23,05)
raw2 <- raw1 + rep(c(2,-2), each=length(raw1)/2)
rawc1 <- rep(raw1, 2) + rep(0:1 * 4, each=length(raw1))
rawc2 <- rep(raw2, 2) + rep(0:1 * 4, each=length(raw2))
rawc3 <- rep(raw1[1:6], 4) + rep(0:3 * 2, each=6)
f1 <- c(rawc1, rawc2 + 18, rawc1 + 36, rawc2 + 54)
f2 <- rep(rawc1, 4) + rep(0:3 * 18, each=length(rawc1))
# f3 <- c(matrix(seq_along(err), nrow(err), byrow=TRUE))[f2]
f3 <- rep(rawc3, 4) + rep(0:3 * 18, each=length(rawc3))
tris.f1 <- tris_to_df(f1, err)
nms <- sprintf( "Faux Diagonal %d", 1:3)
tris.f1[['type']] <- nms[1]
tris.f2 <- tris_to_df(f2, err)
tris.f2[['type']] <- nms[2]
tris.f3 <- tris_to_df(f3, err)
tris.f3[['type']] <- nms[3]
tris.f <- rbind(subset(tris, type=='Diagonal'), tris.f1, tris.f2, tris.f3)
tris.f[['type']] <- factor(tris.f[['type']], levels=unique(tris.f[['type']]))

p <- ggplot(tris.f) +
  geom_polygon(aes(x=x, y=y, group=id, fill=I(color))) +
  geom_point(data=points, aes(x=x, y=y), size=.5, shape=3, color='white') +
  facet_wrap(~type, nrow=1) +
  coord_fixed() +
  thm.blnk
pdim <- gtable_dim(ggplotGrob(p))
old.opt <- knitr::opts_chunk$get(c('fig.width', 'fig.height'))
do.call(knitr::opts_chunk$set, list(fig.width=pdim[1], fig.height=pdim[2]))
```
```{r fake-squares, echo=FALSE}
p
```

We show just three alternate tilings, although there are `$2^{16}$`.  All of
them perfectly cover the grid with equal sized triangles.  "Faux Diagonal 1" is
even a pretty convincing replica.  Unfortunately only one of them fits exactly
into the next level of coarseness, "Axis" [shown previously](#sqr-vs-dia),
seen here in a green outline:

```{r fake-diamonds-outline, echo=FALSE}
bad.1 <- ids_to_df(c(11,13,15,17) + rep(0:3 * 18, each=4), err)
bad.2 <- ids_to_df(c(11,13,15,17) + rep(c(0, 36), each=4), err)
bad.3 <- ids_to_df(c(11,15,31,35) + rep(c(0, 36), each=4), err)
bad.1[['type']] <- factor(nms[1], levels=levels(tris.f[['type']]))
bad.2[['type']] <- factor(nms[2], levels=levels(tris.f[['type']]))
bad.3[['type']] <- factor(nms[3], levels=levels(tris.f[['type']]))

p + geom_polygon(
  data=subset(tris, type=='Axis', -type),
  aes(x, y, group=id), fill=NA, color='green'
) + geom_point(
  data=rbind(bad.1, bad.2, bad.3), aes(x, y),
  fill='blue', color='grey20', shape=21, size=2
)
do.call(knitr::opts_chunk$set, old.opt)
```

It is only with "Diagonal" that every edge of the parent approximation level
completely overlaps with edges of the child.  Every other arrangement has at
least one parent edge crossing a hypotenuse of a child triangle.  These
crossings are what the blue circles highlight.  Ironically, the best looking
fake is the worst offender.

Herein lies the beauty of [&commat;mourner's][6] implementation; by deriving
child triangles from splitting the parent ones, we guarantee that the child
triangles will conform to the parents.  As I learned painfully no such
guarantees apply when we compute the coordinates directly.

There are many ways to solve this vectorization problem, but the simplest I
could come up with was to treat it as a tiling task.  The idea is to define the
smallest tessellating patterns that extend to the correct layout.  These
patterns can then be repeated as needed in internally vectorized
manner[^int-vec].  This is what they look like for our 9 x 9 example:

```{r echo=FALSE}
tris2 <- transform(
  tris,
  alpha=ave(
    seq_along(id), factor(id), FUN=function(i) all(x[i] <= .5 & y[i] <= .5)
  )
)
p <- ggplot(tris2) +
  geom_polygon(
    aes(x=x, y=y, group=id, fill=I(color), alpha=alpha)
  ) +
  geom_point(data=points, aes(x=x, y=y), size=.5, shape=3, color='white') +
  facet_wrap(~type) +
  scale_alpha_continuous(guide=FALSE, range=c(0.2,1)) +
  coord_fixed() +
  thm.blnk
pdim <- gtable_dim(ggplotGrob(p))
old.opt <- knitr::opts_chunk$get(c('fig.width', 'fig.height'))
do.call(knitr::opts_chunk$set, list(fig.width=pdim[1], fig.height=pdim[2]))
```
```{r square-vs-diamond-tile, echo=FALSE}
p
do.call(knitr::opts_chunk$set, old.opt)
```

Unfortunately this is insufficient.  We must also for each triangle hypotenuse
midpoint track their children so that we may [carry over](#carry-over-viz) their
errors.  One nice benefit of the tessellating pattern is that if we define the
parent-child relationship for the simple tile, we can copy that relationship
along with the rest of the tile.  In fact, we don't actually care about the
triangles.  What we actually want are the hypotenuses (black), their midpoints
(<span style='background-color: #8da0cb'>lavender</span>) and endpoints(hollow <span
style='color: #66c2a5'>green</span> ), and the corresponding child
hypotenuse midpoints (<span style='background-color: #fc8d62'>orange</span>).
The parent/child relation is shown with the arrows:

```{r echo=FALSE}
p.dg <- ids_to_df(c(offset.dg[,1,] * 9 + offset.dg[,2,] + 1), matrix(0,9,9))
p.ax <- ids_to_df(c(offset.ax[,1,] * 9 + offset.ax[,2,] + 1), matrix(0,9,9))
p.dg[['type']] <- factor('Diagonal', levels=c('Diagonal', 'Axis'))
p.dg[['ptype']] <- 'child'
p.dg[8:12, 'ptype'] <- 'mid'
p.dg[1:8, 'ptype'] <- 'ends'
p.ax[['type']] <- factor('Axis', levels=c('Diagonal', 'Axis'))
p.ax[['ptype']] <- 'child'
p.ax[9:12, 'ptype'] <- 'mid'
p.ax[1:8, 'ptype'] <- 'ends'
p.all <- rbind(p.dg, p.ax)
p.all[['grp']] <- (seq_len(nrow(p.all)) - 1L) %% 4L

to_arrows <- function(grp) {
  ends <- grp[1,,drop=FALSE]
  ends.b <- grp[2,,drop=FALSE]
  mid <- grp[3L,,drop=FALSE]

  child <- grp[-3L,]
  child[['xend']] <- mid[, 'x']
  child[['yend']] <- mid[, 'y']
  x.diff <- child[['xend']] - child[['x']]
  y.diff <- child[['yend']] - child[['y']]
  off.c <- 5
  child[['xend']] <- child[['xend']] - x.diff / off.c
  child[['x']] <- child[['x']] + x.diff / off.c
  child[['yend']] <- child[['yend']] - y.diff / off.c
  child[['y']] <- child[['y']] + y.diff / off.c

  x.diff <- ends.b[['x']] - ends[['x']]
  y.diff <- ends.b[['y']] - ends[['y']]
  off.e <- 30
  ends[['xend']] <- ends.b[['x']] - x.diff / off.e
  ends[['x']] <- ends[['x']] + x.diff / off.e
  ends[['yend']] <- ends.b[['y']] - y.diff / off.e
  ends[['y']] <- ends[['y']] + y.diff / off.e
  rbind(ends, child)
}
p.arr <-
  do.call(rbind, lapply(split(p.all, p.all[c('type', 'grp')]), to_arrows))

p2 <- ggplot(tris2) +
  geom_polygon(aes(x=x, y=y, group=id, fill=I(color)), alpha=.2) +
  geom_point(data=points, aes(x=x, y=y), size=.5, shape=3, color='white') +
  geom_segment(
    data=subset(p.arr, ptype=='child'),
    arrow=arrow(length=unit(.05, 'inches'), type='closed', angle=25),
    aes(x, y, xend=xend, yend=yend),
    color='grey50'
  ) +
  geom_segment(
    data=subset(p.arr, ptype=='ends'),
    aes(x, y, xend=xend, yend=yend)
  ) +
  geom_point(
    data=subset(p.all, ptype=='mid'),
    aes(x, y, colour=I('#8da0cb')),
    size=3
  ) +
  geom_point(
    data=subset(p.all, ptype=='ends'),
    aes(x, y, colour=I('#66c2a5'), fill='grey90'),
    shape=21, size=2
  ) +
  geom_point(
    data=subset(p.all, ptype=='child'),
    aes(x, y, colour=I('#fc8d62'))
  ) +
  facet_wrap(~type) +
  coord_fixed() +
  thm.blnk

pdim <- gtable_dim(ggplotGrob(p2))
old.opt <- knitr::opts_chunk$get(c('fig.width', 'fig.height'))
do.call(knitr::opts_chunk$set, list(fig.width=pdim[1], fig.height=pdim[2]))
```
```{r square-vs-diamond-points, echo=FALSE}
p2
do.call(knitr::opts_chunk$set, old.opt)
```

# Action!

Let's look at this in action with the next grid size up for effect.  As we did
in our [earlier visualization](#mesh-anim) we'll track the coordinates in the
left panel side (both "Diagonal" and "Axis", alternating), and the computed mesh
approximation errors in the right hand panel:

<div style='background-color: red;'>Regen video with seq.r line order change</div>

<video id=mesh-anim-vec style='display: block; margin: 0 auto;' controls loop>
<source
  src='/post/2019-08-23-mesh-reduction-1_files/images/out-vec.mp4'
  type="video/mp4"
/>
</video>

Indeed we can see that for each "layer", we start with a repeatable template
sized to the layer.  We then fill a column with it, then the rest of the
surface, and finally we compute errors.  Here is the same thing as a flipbook
with code so we can see how it's actually done.  Remember that in these
flipbooks the state shown is immediately _after_ the highlighted line is
evaluated.  We'll focus on the second set of layers:

<div id='flipbook-vec' class='bgw-wide-window'></div>

`o`, as shown in the first frame[^first-frame] contains the *o*ffsets that
represent the template tile.  These offsets are generated by `init_offsets`,
which in _simplified_[^simplified-offsets] form is:

```{r eval=FALSE}
init_offsets <- function(i, j, n, layers) {
  o <- if(j == 'axis') offset.ax else offset.dg    # raw coords
  o <- o * 2^(i - 1)                               # scale
  array(o[,1,] + o[,2,] * (n + 1) + 1, dim(o)[-2]) # x/y -> id
}
```

`offset.ax` and `offset.dg` contain x/y coordinates for each of the points in
the template tile.  `init_offsets` scales those to the appropriate size for the
layer and collapses them to "linearized" coordinates<span id=linearized></span>:

<!-- really shouldn't be doing these inline styles... -->
<style>
span.bgw-hmid   { padding: .1em 0; background-color: #8da0cb;}
span.bgw-hend   { padding: .1em 0; color: #66c2a5;}
span.bgw-hchild { padding: .1em 0; background-color: #fc8d62;}
</style>

```{r eval=FALSE}
init_offsets(i=2, j='axis', n=16, layers=log2(16))
```
<pre><code>     [,1] [,2] [,3] [,4] [,5]
[1,]  <span class='bgw-hend'>  1    5 </span><span class='bgw-hmid'>   3 </span><span class='bgw-hchild'>  21   19 </span>
[2,]  <span class='bgw-hend'>  5   73 </span><span class='bgw-hmid'>  39 </span><span class='bgw-hchild'>  55   21 </span>
[3,]  <span class='bgw-hend'> 73   69 </span><span class='bgw-hmid'>  71 </span><span class='bgw-hchild'>  53   55 </span>
[4,]  <span class='bgw-hend'> 69    1 </span><span class='bgw-hmid'>  35 </span><span class='bgw-hchild'>  19   53 </span>
</code></pre>
```{r eval=FALSE}
init_offsets(i=2, j='diag', n=16, layers=log2(16))
```
<pre><code>     [,1] [,2] [,3] [,4] [,5] [,6] [,7]
[1,]  <span class='bgw-hend'>  1   73 </span><span class='bgw-hmid'>  37 </span><span class='bgw-hchild'>  71   35    3   39 </span>
[2,]  <span class='bgw-hend'>  9   73 </span><span class='bgw-hmid'>  41 </span><span class='bgw-hchild'>  75   43   39    7 </span>
[3,]  <span class='bgw-hend'> 73  137 </span><span class='bgw-hmid'> 105 </span><span class='bgw-hchild'> 139  107  103   71 </span>
[4,]  <span class='bgw-hend'> 73  145 </span><span class='bgw-hmid'> 109 </span><span class='bgw-hchild'> 143  107   75  111 </span>
</code></pre>

In "linearized" form for our 17 x 17 example, 1 corresponds to (x,y) coordinate
(1,1), 2 to (1,2),  18 to (2,1), 289 to (17,17), etc.[^linearized].  The colors
match those of the visualization.  We can shift "linearized" coordinates,
by simple arithmetic.  For example, adding one to a coordinate will move it up
one row[^shift-with-care].  Adding `(n + 1)`, where `n <- nrow(map) - 1`, shifts
coordinates by one column.  So in:

```{r eval=FALSE}
seq.r <- (seq_len(tile.n) - 1) * n / tile.n
o <- rep(o, each=tile.n) + seq.r
```

We first repeat the template tile four times, and then shift them by `seq.r`:

```{r eval=FALSE}
seq.r
```
```
[1]  0  4  8 12
```

Due to how we repeat `o` each value of `seq.r` will be recycled for every value
of `o`.  The result is to copy the template tile to fill the column.  Similarly:

```{r eval=FALSE}
o <- rep(o, each=tile.n) + seq.r * (n + 1)
```

Will fill the rest of the surface by repeating and shifting the column.

The points are arranged by type in the original offset list so we can easily
subset for a particular type of point from the repeated tile set.  This is
exactly how the visualization colors the points as the point type is never
explicitly stated in the code.  Additionally, the relative positions of parents
and children are also preserved.

# Loopy Vectorization?

Hold on a sec, weren't we going to vectorize the $#!+ out of this?  What about
all those for loops?  They are nested three levels deep!  A veritable viper's
nest of looping control structures.  Did we just turn ourselves around and end
up with more loops than we started with?  Strictly speaking we did, but what
matters is how many R-level calls there are, not how many R-level loops.
Particularly once we start getting to large map sizes, the bulk of the
computations are being done in statically compiled code.

Our vectorized algorithm got through the 17 x 17 map in 240 R-level
steps[^step-count].  The original one requires 40,962 for the same map!  The
vectorized algorithm R-level call count will grow with `$log{n}$` where `n` is
the number of rows/cols in the map.  The original transliteration will grow with
`$n^2 .log(n)$`$.  There are a similar number of calculations overall, but as
you can see in the animation the vectorized version "batches" many of them into
single R-level calls to minimize the R interpreter overhead.  So yes, despite
the `for` loops our code is very vectorized, and it shows up in the timings:

```{r vec-timings, echo=FALSE}
dat <- data.frame(Language=c('R-vec', 'JS', 'C'), Time=c(49.15,20.78,6.81))
ggplot(dat) +
  geom_col(aes(Language, Time)) +
  geom_text(aes(Language, Time, label=Time), vjust=-.5) +
  ylab('Time (ms)') +
  ggtitle('RTIN Error Computation Benchmarks', subtitle='257x257 Grid Size') +
  scale_y_continuous(expand = expand_scale(mult = c(0.05, .1)))
```

Now we're in business.  We're beating neither C nor JavaScript, but
we're in the same conversation, and there is room to do better.  This
initial vectorized implementation is geared for clarity over speed.  There are
some simple changes that will double the speed[^double-speed], but we have even
grander aspirations.

Before we move on, a quick recap of some vectorization concepts we applied here:

1. Identify repeating / parallelizable patterns.
2. Structure the data in such a way that the repeating patterns can be processed
   by internally vectorized functions.
3. Allow R-level loops so long as the number of their iterations is small
   relative to those carried out in internally vectorized code.

To achieve 2. we resorted to carefully structured seed data, and
repeated it either explicitly or implicitly with vector recycling.  We did this
both for the actual data, as well as for the indices we used to subset the data
for use in the vectorized operations.

# Follies in Optimization

If you've read any of my previous blog posts you probably realize by now that I
have a bit of a thing &lt;cough&gt;unhealthy obsession&lt;/cough&gt; for making
my R code run fast.  I wouldn't put what's coming next into the "premature
optimization is the root of all evil" category, but rational people will
justifiable question why anyone would spend as much time as I have trying to
make something that doesn't need to be any faster, faster.

Well, I have an itch I want to scratch and I will go ahead and scratch it,
rationality be damned.

Let's look back at our first vectorized implementation.  In particular at the
child error carry over section at the end:

```{r eval=FALSE}
# Carry over child errors
err.val <- do.call(pmax, err.vals)
err.ord <- order(err.val)
errors[o.m[err.ord]] <- err.val[err.ord]
```

You'll notice a seemingly odd `order` call.  This is necessary because for the
"Axis" tiles there is overlap between tiles.  Compare our earlier example with a
single tile and then with the tile repeated to fill a column:

```{r echo=FALSE}
p.ax1 <- transform(p.ax, sgrp=1)
p.ax2 <- rbind(transform(p.ax1, sgrp=2), transform(p.ax1, y=y + .5, sgrp=3))
ax.lvl <- c('Axis Single', 'Axis Column')

p.ax1[['type']] <- factor('Axis Single', levels=ax.lvl)
p.ax2[['type']] <- factor('Axis Column', levels=ax.lvl)

p.all2 <- rbind(p.ax1, p.ax2)
p.all2[['grp']] <- (seq_len(nrow(p.all2)) - 1L) %% 4L

p.arr <- do.call(
  rbind, lapply(split(p.all2, p.all2[c('grp', 'sgrp')]), to_arrows)
)

tris3 <- subset(tris2, type == 'Axis')
tris3[['type']] <- NULL

p2 <- ggplot(tris3) +
  geom_polygon(aes(x=x, y=y, group=id, fill=I(color)), alpha=.2) +
  geom_point(data=points, aes(x=x, y=y), size=.5, shape=3, color='white') +
  geom_point(
    data=data.frame(
      x=.25, y=.5,
      type=factor('Axis Column', levels=ax.lvl)
    ), aes(x=x, y=y),
    color='green', size=10
  ) +
  geom_segment(
    data=subset(p.arr, ptype=='child'),
    arrow=arrow(length=unit(.05, 'inches'), type='closed', angle=25),
    aes(x, y, xend=xend, yend=yend),
    color='grey50'
  ) +
  geom_segment(
    data=subset(p.arr, ptype=='ends'),
    aes(x, y, xend=xend, yend=yend)
  ) +
  geom_point(
    data=subset(p.all2, ptype=='mid'),
    aes(x, y, colour=I('#8da0cb')),
    size=3
  ) +
  geom_point(
    data=subset(p.all2, ptype=='ends'),
    aes(x, y, colour=I('#66c2a5'), fill='grey90'),
    shape=21, size=2
  ) +
  geom_point(
    data=subset(p.all2, ptype=='child'),
    aes(x, y, colour=I('#fc8d62'))
  ) +
  facet_wrap(~type) +
  coord_fixed() +
  thm.blnk

pdim <- gtable_dim(ggplotGrob(p2))
old.opt <- knitr::opts_chunk$get(c('fig.width', 'fig.height'))
do.call(knitr::opts_chunk$set, list(fig.width=pdim[1], fig.height=pdim[2]))
```
```{r tile-overlap, echo=FALSE}
p2
do.call(knitr::opts_chunk$set, old.opt)
```

<span id='vec-inefficiency'></span>
The midpoint highlighted in green is at the overlap of two tiles.  There are two
not-so-great things about this.  The first is that the midpoint estimate and
errors get computed twice, once for each tile.  The second is that we are
carrying over errors from children in different tiles, which is what requires us
to order `err.val` prior to final insertion into the error matrix.  If we didn't
we risk a lesser error from one tile overwriting a larger error from another.
This issue afflicts all the non-peripheral midpoints in the "Axis" tile
arrangement[^not-diag].

A potential improvement is to directly compute the midpoint locations.  As we
can see here they are arranged in patterns that we should be able to compute
without having to resort to tiling:

```{r echo=FALSE}
axis <- c(3, 7, 19, 23, 27, 39, 43, 55, 59, 63, 75, 79)
diag <- c(11, 13, 15, 17) + rep(0:3, each=4) * 2 * 9

p.ax3 <- transform(ids_to_df(axis, err), type='Axis', pcolor='#8da0cb')
p.dg3 <- transform(ids_to_df(diag, err), type='Diagonal', pcolor='#8da0cb')

p.all <- rbind(p.ax3, p.dg3)

p2 <- ggplot(tris2) +
  geom_polygon(aes(x=x, y=y, group=id, fill=I(color)), alpha=.2) +
  geom_point(data=points, aes(x=x, y=y), size=.5, shape=3, color='white') +
  geom_point(data=p.all, aes(x, y), colour='#8da0cb', size=3) +
  facet_wrap(~type) +
  coord_fixed() +
  thm.blnk

pdim <- gtable_dim(ggplotGrob(p2))
old.opt <- knitr::opts_chunk$get(c('fig.width', 'fig.height'))
do.call(knitr::opts_chunk$set, list(fig.width=pdim[1], fig.height=pdim[2]))
```
```{r midpoints-2, echo=FALSE}
p2
do.call(knitr::opts_chunk$set, old.opt)
```

<!--
Show the midpoints in a 9x9 grid, side by side for each type of point.
-->

For each layer all the children will be at fixed offsets from the midpoints.
The hypotenuses orientations vary within a layer, so we can't use fixed offsets
for them.  However, if we split the midpoints into two groups (A and B below) we
can use fixed offsets within each group.  For "Diagonal" the groups are
bottom-left to top-right (A) and top-left to bottom-right (B), and for "Axis"
vertical (A) and horizontal (B):

```{r echo=FALSE}
p.ax3.a <- cbind(subset(p.ax3, (x * 4) %% 2 == 0), subtype='A')
p.ax3.b <- cbind(subset(p.ax3, (x * 4) %% 2 != 0), subtype='B')

sw.x <- with(p.dg3, ((x * 8 - 1) / 2) %% 2 > 0)
sw.y <- with(p.dg3, ((y * 8 - 1) / 2) %% 2 > 0)
p.dg3.a <- cbind(subset(p.dg3, !xor(sw.x, sw.y)), subtype='A')
p.dg3.b <- cbind(subset(p.dg3, xor(sw.x, sw.y)), subtype='B')

s <- 1/8

dat <- rbind(
  transform(p.ax3.a, ptype='mid'),
  transform(p.ax3.a, y=y+.25, ptype='end'),
  transform(p.ax3.a, y=y-.25, ptype='end'),
  transform(p.ax3.a, x=x+s, y=y+s, ptype='child'),
  transform(p.ax3.a, x=x+s, y=y-s, ptype='child'),
  transform(p.ax3.a, x=x-s, y=y-s, ptype='child'),
  transform(p.ax3.a, x=x-s, y=y+s, ptype='child'),

  transform(p.ax3.b, ptype='mid'),
  transform(p.ax3.b, x=x+.25, ptype='end'),
  transform(p.ax3.b, x=x-.25, ptype='end'),
  transform(p.ax3.b, x=x+s, y=y+s, ptype='child'),
  transform(p.ax3.b, x=x+s, y=y-s, ptype='child'),
  transform(p.ax3.b, x=x-s, y=y-s, ptype='child'),
  transform(p.ax3.b, x=x-s, y=y+s, ptype='child'),

  transform(p.dg3.a, ptype='mid'),
  transform(p.dg3.a, x=x+s, y=y+s, ptype='end'),
  transform(p.dg3.a, x=x-s, y=y-s, ptype='end'),
  transform(p.dg3.a, x=x+s, ptype='child'),
  transform(p.dg3.a, y=y-s, ptype='child'),
  transform(p.dg3.a, x=x-s, ptype='child'),
  transform(p.dg3.a, y=y+s, ptype='child'),

  transform(p.dg3.b, ptype='mid'),
  transform(p.dg3.b, x=x+s, y=y-s, ptype='end'),
  transform(p.dg3.b, x=x-s, y=y+s, ptype='end'),
  transform(p.dg3.b, x=x+s, ptype='child'),
  transform(p.dg3.b, y=y-s, ptype='child'),
  transform(p.dg3.b, x=x-s, ptype='child'),
  transform(p.dg3.b, y=y+s, ptype='child')
)
dat$pcolor <- as.character(dat$pcolor)
dat[dat$ptype == 'child', 'pcolor'] <- '#fc8d62'
dat[dat$ptype == 'end', 'pcolor'] <- '#66c2a5'
dat <- transform(dat, pfill=pcolor, stringsAsFactors=FALSE)
dat[dat$ptype == 'end', 'pfill'] <- 'grey90'
dat[with(dat, x < 0 | x > 1 | y < 0 | y > 1), 'pfill'] <- 'grey90'
psizes <- c(mid=3, child=2, mid=2)
dat$psize <- psizes[dat$ptype]
dat$id <- c(
  rep(c(1:6, rep(1:6, 2), rep(1:6, 4)), 2),
  rep(c(1:8, rep(1:8, 2), rep(1:8, 4)), 2)
)
dat.arrow <- do.call(
  rbind,
  lapply(
    with(dat, split(dat, list(id, type, subtype))),
    function(x) {
      if(nrow(x)) {
        mid <- x[1,c('x', 'y')]
        cbind(
          x[-(1:3), c('x', 'y', 'type', 'subtype', 'id')],
          setNames(mid, c('xend', 'yend'))
        )
      }
} ) )
dat.hyp <- do.call(
  rbind,
  lapply(
    with(dat, split(dat, list(id, type, subtype))),
    function(x) {
      if(nrow(x)) {
        mid <- x[1,c('x', 'y')]
        cbind(
          x[2:3, c('x', 'y', 'type', 'subtype')],
          setNames(mid, c('xend', 'yend'))
        )
      }
} ) )
dat.arrow <-
  transform(dat.arrow, diag=ifelse(xend - x & yend - y, sqrt(2), 1))
arrow.mult <- log2(nrow(err) - 1) * 13
dat.arrow <- transform(
  dat.arrow,
  xend=xend - sign(xend - x) * 1 / (arrow.mult * diag),
  yend=yend - sign(yend - y) * 1 / (arrow.mult * diag)
)
dat.arrow[['pltype']] <- with(dat.arrow,
  ifelse(x < 0 | y < 0 | x > 1 | y > 1, 3, 1))

p2 <- ggplot(tris2, aes(x, y)) +
  geom_polygon(aes(group=id, fill=I(color)), alpha=.2) +
  geom_point(data=points, size=.5, shape=3, color='white') +
  geom_segment(
    data=dat.arrow,
    arrow=arrow(length=unit(.05, 'inches'), type='closed', angle=25),
    aes(xend=xend, yend=yend, linetype=I(pltype)),
    color='grey50'
  ) +
  geom_segment(
    data=dat.hyp, aes(xend=xend, yend=yend)
  ) +
  geom_point(
    data=dat, aes(size=I(psize), color=I(pcolor), fill=I(pfill)),
    shape=21
  ) +
  facet_grid(subtype ~ type) +
  coord_fixed() +
  thm.blnk

old.opt <- knitr::opts_chunk$get(c('fig.width', 'fig.height'))
pdim <- gtable_dim(ggplotGrob(p2), din=c(old.opt$fig.width, old.opt$fig.width))
do.call(knitr::opts_chunk$set, list(fig.width=pdim[1], fig.height=pdim[2]))
```
```{r vec2-points-all, echo=FALSE}
p2
do.call(knitr::opts_chunk$set, old.opt)
```

All is not perfect though: "Axis" is again a problem as the peripheral midpoints
end up generating out-of-bounds children, shown as hollow points / dahsed arrows
above.  I won't get into the details, but handling the out-of-bounds children
requires separate treatment for the left/top/right/bottom periphery as well as
the inner midpoints.  We need to handle far more corner cases than with the
template approach which makes the code much [uglier][11].  It is also faster:

```{r vec-timings-2, echo=FALSE}
dat <- data.frame(Language=c('R-vec-2', 'JS', 'C'), Time=c(15.11,20.78,6.81))
ggplot(dat) +
  geom_col(aes(Language, Time)) +
  geom_text(aes(Language, Time, label=Time), vjust=-.5) +
  ylab('Time (ms)') +
  ggtitle('RTIN Error Computation Benchmarks', subtitle='257x257 Grid Size') +
  scale_y_continuous(expand = expand_scale(mult = c(0.05, .1)))
```

And something remarkable happens as we increase grid sizes:

```{r vec-timings-3, echo=FALSE, warning=FALSE}
library(patchwork)
times <- readRDS('../../static/data/rtini-data.RDS')
names(times)[3] <- 'Language'

p1 <- ggplot(
  times,
  aes(
    x=factor(
      ind, levels=as.character(sort(as.integer(as.character(unique(ind)))))
    ),
    y=values, color=Language
  )
) +
  geom_point(position=position_dodge(width=.6), alpha=.5) +
  ggtitle("Normal Scale") +
  ylab('Seconds') +
  xlab('Grid Size') +
  scale_color_manual(values=c('#66c2a5','#fc8d62','#8da0cb')) +
  NULL

p2 <- ggplot(
  times,
  aes(
    x=factor(
      ind, levels=as.character(sort(as.integer(as.character(unique(ind)))))
    ),
    y=values, color=Language
  )
) +
  geom_point(position=position_dodge(width=.6), alpha=.5) +
  scale_y_log10() +
  ggtitle("Log Scale") +
  ylab('Seconds (log10)') +
  xlab('Grid Size') +
  scale_color_manual(values=c('#66c2a5','#fc8d62','#8da0cb')) +
  NULL

p1 + p2 + plot_layout(guides='collect') +
  plot_annotation(title='RTIN Error Computation Benchmarks vs. Grid Size') &
  theme(legend.position='bottom')
```

The R implementation beats both the C and JS implementations by a factor of
2-3x.  At the lower sizes the overhead of the R calls holds us back, but as the
overall computations increase by `$N^2$`, the number of R calls only increase by
`$log(N)$`.

Another observation is that JS is faster than C at the largest
grid sizes, even though the C implementation is almost identical to the
JS one.  You can tell from the logged plot that it appears that JS has some more
initial overhead, but the scaling rate is slower.  This might be because we use
64-bit doubles in C so we can interface with R, whereas the JS version uses
32-bit floats[^c-vs-js].

# Too Square

You might have noticed by now that we've been cutting `volcano` off at the
knees, and even inventing some of it.

# Conclusions

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<p id='feedback-cont'></p>

# Appendix

## Acknowledgments

## Session Info

```{r child='../../static/script/_lib/flipbook/flipbook.Rmd', results='asis'}
```
<script type='text/javascript'>
const imgDir = '/post/2020-01-11-mesh-red-vec_files/images/flipbook-vec/';
const fps = 4;
new BgFlipBook({
  targetId: 'flipbook-vec', imgDir: imgDir,
  imgStart: 65, imgEnd: 0119,
  imgPad: "0000", fps: fps, loop: false
})
</script>


