---
title: "Hydra Chronicles Part V: Loose Ends"
author: ~
date: '2019-08-22'
slug: hydra-loose-ends
categories: [r]
tags: [group-stats,optim,hydra]
image:
  /post/2019-07-26-hydra-loose-ends_files/user-imgs/loose-ends-square.png
imagerect:
  /post/2019-07-26-hydra-loose-ends_files/user-imgs/loose-ends-rect.png
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: "A few more tricks for fast group statistics in base R, and
benchmarks against C implementations."
---

```{r echo=FALSE}
old.opt <- options(digits=3)
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
suppressMessages(library(ggplot2))
suppressMessages(library(viridisLite))
suppressMessages(library(data.table))
setDTthreads(1)
```
```{r echo=FALSE, comment="", results='asis'}
old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)
options(fansi.term.cap=c('bright', '256', 'truecolor'))

make.plot.theme <- theme(
  panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
  axis.title.x=element_blank(), axis.text.x=element_blank(),
  axis.ticks.x=element_blank(),
  axis.title.y=element_blank(), axis.text.y=element_blank(),
  axis.ticks.y=element_blank()
)

```

# Almost Done!

<!-- this needs to become a shortcode -->
<img
  id='front-img'
  src='/post/2019-07-26-hydra-loose-ends_files/user-imgs/loose-ends-square.png'
  class='post-inset-image'
/>

Fingers crossed this will be the last post of the [Hydra Chronicles][100], a.k.a.
"Everything You Didn't Want To Know About Group Statistics But Let Me Tell You
Anyway".  Well, it's more of a salvage job of tidbits from earlier posts that
ended up on the cutting room floor that I couldn't quite bring myself to trash.

One of our early "discoveries" about group statistics is that if we can compute
the group sum quickly, we can use it to compute many other group statistics
quickly as well.  In this post we'll go over a few of the other group sum
strategies I tested out before I settled on the [`cumsum` based approach][110]
we used to "beat" `data.table`.

# rowsum

I did mention this one [in passing][120], but it bears re-examining:
`base::rowsum` is a remarkable creature in the R.  As far as I know, it is the
only base R function that computes group statistics on unequal group sizes in
statically compiled code.  Despite this it is arcane, especially when compared
with its popular cousin `base::rowSums`.

`rowsum` and `rowSums` are near indistinguishable on name alone, but they
do quite different things.  `rowsum` collapses rows together by group, leaving
column count unchanged, whereas `rowSums` collapses all columns leaving row
count unchanged:

```{r rowsum-vs-rowsum, echo=FALSE, fig.width=7, fig.height=2, warning=FALSE}
set.seed(1)
vals <- sample(1:10)
group <- c(1,1,2,3,3)
dat.rowsum <- data.table(
  x=rep(1:2, each=5),
  y=5:1,
  val=vals,
  group=NA,
  facet='rowsum'
)
dat.rowsum.res <- data.table(
  x=rep(1:2, each=3) + 7,
  y=rep(4:2, 2),
  val=c(rowsum(matrix(vals, ncol=2), group)),
  group=unique(group),
  facet='rowsum'
)
dat.rowSums <- copy(dat.rowsum)[, `:=`(facet='rowSums', x=x+2, group=NA)]
dat.rowSums.res <- data.table(
  x=8,
  y=5:1,
  val=rowSums(matrix(vals, ncol=2)),
  group=NA,
  facet='rowSums'
)
dat.grp <- dat.rowsum[x == 1][, `:=`(x=x+3, group=NULL, val=NA)]
dat.grp[, group:=group]
text.rowsum <- data.table(
  x=c(.5, 3, 4.5),
  y=3,
  hjust=c(1, .5, 0),
  label=c("rowsum(", ",", ")"),
  facet='rowsum'
)
text.rowSums <- data.table(
  x=c(1.5, 3.5) + 1,
  y=3,
  hjust=c(1, 0),
  label=c("rowSums(", ")"),
  facet='rowSums'
)
text.arrows <- data.table(
  x=6, y=3, facet=c('rowsum', 'rowSums'), hjust=0.5,
  label="phantom()%->%phantom()"
)
tsize <- 6
lsize <- 4
lpad <- unit(0.25/2, "lines")
scale.fill.2 <- scale_fill_manual(
  values=c("#440154FF", "#21908CFF", "#CCC604FF"),
  guide=FALSE, na.value='grey35'
)
dat <- rbind(
  dat.rowsum, dat.rowsum.res, dat.rowSums, dat.rowSums.res, dat.grp
)
ggplot(dat, aes(x, y)) +
  geom_tile(aes(fill=as.factor(group)), color='#AAAAAA') +
  geom_text(aes(label=val), size=lsize, color='white') +
  geom_text(data=text.arrows, aes(label=label), size=tsize, parse=TRUE) +
  geom_text(
    data=text.rowsum, aes(label=label, hjust=hjust),
    size=tsize, family='mono'
  ) +
  geom_text(
    data=text.rowSums, aes(label=label, hjust=hjust),
    size=tsize, family='mono'
  ) +
  facet_wrap(~facet) +
  coord_fixed(xlim=c(-5, 10)) +
  # coord_fixed(xlim=c(-3.5, 21)) +
  scale.fill.2 +
  make.plot.theme +
  NULL
```

In the single column/vector case, `rowsum(x, grp)` computes the sum of `x` by
the groups in `grp`.  Typically this operation is done with `tapply(x, grp,
sum)` (or `vapply(split(x, grp), sum, 0)`):

```{r rowsum-vs-tapply, echo=FALSE, fig.height=6}
tsize <- 5
x.res <- 16
end0 <- 3.5
mid <- 8
end1 <- x.res - .5
end2 <- x.res + .5

dat.rowsum <- dat.rowsum[1:5][, group:=NULL][, group:=group]
dat.grp <- dat.rowsum[x == 1][, `:=`(x=x+2, facet='rowsum', group=group)]
dat.rowsum.res <- data.table(
  x=rep(x.res, each=3),
  y=rep(4:2, 1),
  val=c(rowsum(matrix(vals[1:5], ncol=1), group)),
  group=unique(group),
  facet='rowsum'
)

dat.tapply <- copy(dat.rowsum)[, facet:='tapply']
dat.tapply.grp <- copy(dat.grp)[, `:=`(facet='tapply')]
text.rowsum <- data.table(
  x=c(.5, 2, end0),
  y=3,
  hjust=c(1, .5, 0),
  label=c("rowsum(", ",", ")"),
  facet='rowsum'
)
text.tapply <- data.table(
  x=c(.5, 2, end0),
  y=3,
  hjust=c(1, .5, 0),
  label=c('tapply(', ',', ', sum)'),
  facet='tapply'
)
tall.bracket <- 'bgroup("%s",
  atop(
    atop(
      phantom(0),
      atop(
          atop(phantom(0), phantom(0)),
          atop(phantom(0), phantom(0))
    ) ),
    atop(
      phantom(0),
      atop(
          atop(phantom(0), phantom(0)),
          atop(phantom(0), phantom(0))
    ) )
  ),
  "%s"
)'
text.tapply.a <- data.table(
  x=c(end1, end2),
  y=3,
  hjust=c(1, 0),
  label=c(
    sprintf('paste("sapply(list", %s)', sprintf(tall.bracket, "(", "")),
    sprintf('paste(%s, ", sum)")', sprintf(tall.bracket, "", ")"))
  ),
  facet='tapply'
)
text.tapply2 <- data.table(
  x=c(
         end1, end2,
         end1, end2,
         end1, end2
  ),
  y=c(rep(-2, 2),
      rep(-4, 2),
      rep(-6, 2)
  ),
  hjust=c(
        1, 0,
        1, 0,
        1, 0
  ),
  label=c(
            'paste("sum", "(", )', 'paste("),")',
          sprintf('paste("c",%s, "sum(")', sprintf(tall.bracket, "(", "")),
          sprintf('paste("),", %s)', sprintf(tall.bracket, "", ")")),
            'paste("sum", "(", )', 'paste(")")'
  ),
  facet='tapply'
)
text.arrows <- data.table(
  x=mid, y=c(3,3,-4, -9.5), hjust=0.5, label='phantom() %->% phantom()',
  facet=c('rowsum', rep('tapply', 3))
)

dat.lapply <- copy(dat.tapply.grp)[, `:=`(x=x.res, y=y+c(.5,.5,0,-.5, -.5))]
dat.lapply.2 <- copy(dat.lapply)[, `:=`(y=y-7)]
dat.lapply.2.res <- copy(dat.rowsum.res)[, `:=`(y=y-12.5, facet='tapply')]

dat <- rbind(
  dat.rowsum, dat.grp, dat.rowsum.res, dat.tapply, dat.tapply.grp,
  dat.lapply, dat.lapply.2, dat.lapply.2.res
)
dat[x==1, group:=NA]
dat[x==3, val:=NA]
dat[, label:=ifelse(is.na(val), "", as.character(val))]
dat.text <- rbind(text.rowsum, text.tapply)
dat.text.parse <- rbind(text.arrows, text.tapply2, text.tapply.a)

ggplot(dat, aes(x, y)) +
  geom_tile(
    aes(fill=as.factor(group)), color='#AAAAAA',
    width=1, height=1
  ) +
  geom_text(aes(label=label), color='white', size=3) +
  geom_text(
    data=dat.text, aes(label=label, hjust=hjust),
    size=tsize, family='mono'
  ) +
  geom_text(
    data=dat.text.parse, aes(label=label, hjust=hjust),
    size=tsize, family='mono', parse=TRUE
  ) +
  facet_grid(facet~., scales='free_y', space='free_y') +
  # coord_cartesian(xlim=c(-5, 26)) +
  coord_cartesian(xlim=c(-3.5, 21)) +
  scale.fill.2 +
  make.plot.theme +
  NULL

```

As illustrated above `tapply` must split the vector by group, explicitly call
the  R-level `sum` on each group, and simplify the result to a
vector[^liberties].  `rowsum` can just compute the group sums directly in
statically compiled code.  This makes it substantially faster, although
obviously limits it to computing sums.

Let's look at some examples and timings with our beaten-to-death 10MM row, ~1MM
group [data set](#data), and our timing function [`sys.time`](#sys-time).  We'll
order the data first as [that is fastest][140] even when including the time to
order:

```{r eval=FALSE}
sys.time({
  o <- order(grp)
  go <- grp[o]
  xo <- x[o]
})
```
```
   user  system elapsed
  0.690   0.003   0.696
```

`tapply` for reference:

```{r eval=FALSE}
sys.time(gsum.0 <- tapply(xo, go, sum))
```
```
   user  system elapsed
  2.273   0.105   2.383
```

And now `rowsum`:

```{r eval=FALSE}
sys.time(gsum.1 <- rowsum(xo, go))
```
```
   user  system elapsed
  0.507   0.038   0.546
```

```{r eval=FALSE}
all.equal(c(gsum.0), c(gsum.1), check.attributes=FALSE)
```
```
[1] TRUE
```

This is a ~4-5x speedup for the summing step, or a ~3x speedup if we include the
time to order.  `data.table` is faster for this task.

> All `data.table` timings are single threaded (`setDTthreads(1)`)[^dt-single].

```{r group-sum-times, echo=FALSE}
step.levels <- c('order',  'rle*', 'sum')
funs <- c('tapply', 'tapply', 'rowsum', 'rowsum', 'data.table')
steps <- c('order', 'sum', 'order', 'sum', NA)
times <- data.frame(
  Function=factor(funs, levels=unique(funs)),
  # Version=c('data.table', 'group_slope', 'data.table', 'group_slope'),
  Step=factor(steps, levels=step.levels[-2]),
  time=c(.696, 2.383, .696, .546, 1.025)
)
brewer3 <- c('#66c2a5','#fc8d62','#8da0cb')
ggplot(times, aes(x=Function, y=time)) +
  geom_col(aes(fill=Step)) +
  # facet_grid(.~Function, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  geom_label(
    data=data.table(times)[!is.na(Step)][
      order(Step, decreasing=TRUE),
      .(time=time, timec=cumsum(c(0,time[-length(time)]))),
      .(Function)
    ],
    aes(label=sprintf("%0.2f", time), y=timec + time/2)
  ) +
  geom_text(
    data=data.table(times)[, .(time=sum(time)), .(Function)],
    aes(label=sprintf("%0.2f", time)), vjust=-.2
  ) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1))) +
  scale_fill_manual(values=brewer3[-2], na.value='grey50', drop=FALSE)
```

The slow step for `data.table` is also the ordering, but we don't have a
good way to break that out.

# colSums

`base::rowSums`, the aforementioned better-known cousin of `rowsum`, also
computes group statistics with statically compiled code.  Well, it kind of does
if you consider matrix rows to be groups. `base::colSums` does the same except
for columns.  Suppose we have three equal sized ordered groups:

```{r}
(G <- rep(1:3, each=2))
```

And values that belong to them:

```{r}
set.seed(1)
(a <- runif(6))
```

We can compute the group sums using `colSums` by wrapping our vector into a
matrix with as many columns as there are groups.  Since R internally stores
matrices as vectors in column-major order, this is a natural operation (and also
why we use `colSums` instead of `rowSums`):

```{r}
(a.mx <- matrix(a, ncol=length(unique(G))))
colSums(a.mx)
```

This is equivalent to[^rowsum-matrix]:

```{r}
c(rowsum(a, G))
```

We run into problems as soon as we have uneven group lengths, but there is a
workaround.  The idea is to use clever indexing to embed the values associated
with each group into columns of a matrix.  We illustrate this process with a
vector with 95 elements in ten groups.  For display purposes we wrap the vector
column-wise every ten elements, and designate the groups by a color.  The values
of the vector are represented by the tile heights:

<!--
For how we built the 3D animation see:

static/post/2019-07-26-hydra-loose-ends_files/scripts/threed-plot.R
-->
<div id='flipbook1'></div>
```{r child='../../static/script/_lib/flipbook/flipbook.Rmd'}
```
<script type='text/javascript'>
new BgFlipBook({
  targetId: 'flipbook1',
  imgDir: '/post/2019-07-26-hydra-loose-ends_files/user-imgs/flip-book/',
  imgEnd: 15, fps: 1, loop: true, loopDelay: 1
})
</script>

You can also view the flipbook [as a video][320] if you prefer.

The embedding step warrants additional explanation.  The trick is to generate a
vector that maps the positions from our irregular vector into the regular
matrix.  There are several ways we can do this, but the one that we'll use today
takes advantage of the underlying vector nature of matrices.  In particular, we
will index into our matrices as if they were vectors, e.g.:

```{r}
(b <- 1:4)
(b.mx <- matrix(b, nrow=2))
b[3]
b.mx[1, 2]   # matrix indexing
b.mx[3]      # vector indexing on matrix
```

Let's look at our 95 data before and after embedding, showing the indices in
vector format for both our ordered vector (left) and the target matrix (right):

```{r echo=FALSE}
library(data.table)
library(ggplot2)
library(gganimate)
# RNGversion("3.5.2");
set.seed(1)
gn <- 10
g2 <- sample(seq_len(gn), 95, replace=TRUE)
x <- runif(length(g2))
o <- order(g2)
go <- g2[o]
xo <- x[o]

 # Horrible horrible, but it works
labels <- c(
  'main',                  # original colored squares
  'main-back',             # background for original
  'group-lab',             # labels for first el in group
  'group-long',            # the path tracing longest group
  'group-long-end',        # the points at the end
  'pad-line',              # the lines showing the padding
  'pad-end',               # end of padding lines
  'embed',                 # matrix to embed in
  'embed-idx',             # embedding indices
  #'embed-main',           # outline of embedding area
  NULL
)

 # labels are:
 # 0: original colored squares
 # 1: first set of re-allocation (first column)
 # 3: rest of reallocation
 # 4: background for colored squares
 # 5: original colored squares, but after the move
 # 6: the "path" that traces the group

gx <- (seq_along(g2) - 1) %/% 10
gy <- -((seq_along(g2) - 1) %% 10)
faint <- 0.2

 # Main elements

dat10 <- data.table(
  step=1L, id=seq_along(g2), g2, x=gx, y=gy, alpha=1, label='main',
  glabel=NA_character_, type='tile', tile.color=NA_character_,
  point.shape=NA_character_
)
dat10 <- rbind(copy(dat10)[, label:='main-back'], dat10)

 # Order foreground and add background cells

dat20 <- rbind(
  dat10[label == 'main-back'][order(g2)][, `:=`(x=gx, y=gy)],
  dat10[label == 'main'][order(g2)][, `:=`(x=gx, y=gy)]
)
dat20[, step:=2L]
g.rle <- rle(dat20[label == 'main'][['g2']])
g.max <- max(g.rle[['lengths']])
g.n <- length(g.rle[['lengths']])

 # group labels

dat30 <- rbind(
  dat20,
  dat20[label == 'main'][which(c(TRUE, diff(g2) != 0))][,
    `:=`(glabel=as.character(g2), label='group-lab', type='label')
  ]
)
dat30[, step:=3L]

 # move group labels

g.off <- 12L
x.off <- seq_len(g.n) + g.off - 1
g.long.x <- g.off

dat40 <- rbind(
  dat30[label != 'group-lab'],
  dat30[label == 'group-lab'][, `:=`(x=x.off, y=0)]
)
dat40[, step:=4L]


 # longest-group

dat50 <- rbind(fill=TRUE,
  dat40[label == 'main' & g2 == which.max(g.rle[['lengths']])][,
    `:=`(
      label='group-longest', type='point', id=seq_len(g.max),
      point.shape='a'
    )
  ],
  dat40
)
dat50[, step:=5L]

 # move longest group

dat60 <- copy(dat50)
dat60[label == 'group-longest',
  `:=`(
      x=g.off,
      y=-c(0, 10:1, 11:13)
  )
]
dat60[, step:=6L]

 # alloc matrix

dat70 <- rbind(fill=TRUE,
  data.table(
    id=seq_len(length(g.rle[['lengths']])),
    x=rep(x.off, g.max), y=-rev(rep(seq_len(g.max), each=g.n)) + 1L,
    label='embed', type='tile'
  ),
  dat60
)
dat70[, step:=7L]

 # compute padding

pads <- data.table(
  x=rep(g.off + (seq_len(g.n) - 1)),
  g.len=g.rle[['lengths']],
  id=rep(seq_len(g.n))
)
pads <- pads[,.(y=-head(seq(from=g.len, g.max, by=+1), -1L)), by=.(x)]

dat80 <- rbind(fill=TRUE,
  dat70[!label %in% c('group-longest', 'group-lab')],
  copy(pads)[, `:=`(label='pad', type='point', point.shape='b')]
  # dat50[label == 'main'][, `:=`(
  #     x=g.n + g2 + 0,
  #     y=-sequence(g.rle[['lengths']]) + 1, g2=g.n+1L,
  #     label='embed-main'
  #   )
  # ]
)
dat80[, step:=8L]

 # compute embedding indices

glen2 <- head(g.rle[['lengths']], -1L)
pad <- g.max - glen2
id0 <- rep(1L, length(g2))
id0[cumsum(glen2) + 1L] <- pad + 1L
id1 <- cumsum(id0)

dat90 <- rbind(fill=TRUE,
  copy(dat80),
  dat80[label == 'main'][, `:=`(
      x=g.off + g2 - 1,
      y=-sequence(g.rle[['lengths']]) + 1, g2=g.n+1L,
      label='embed-idx',
      type='label', glabel=as.character(id1)
) ] )
dat90[, step:=9L]

 # copy into embedding

dat100 <- rbind(
  dat90[!label %in% c('main', 'pad', 'embed-idx')],
  dat90[label == 'main'][, `:=`(
    x=g.off + g2 + -1, y=-sequence(g.rle[['lengths']]) + 1
) ] )
dat100[, step:=10L]

 # colSums

dat110 <- rbind(
  dat100[!label %in% c('main', 'embed', 'embed-idx')],
  dat100[label %in% c('main', 'embed')][, y:=0]
)
dat110[, step:=11L]

 # wrapup

dat120 <- dat110[label %in% c('main', 'main-back')][, `:=`(alpha=0)]
dat120[, step:=12L]

steps <- c(
  "An Algorithm to Compute Group Sums with colSums",
  "Start With Data in Random Groups",
  "Order by Group",
  "Count Groups",
  "One Column Per Group",
  "Length of Longest Group",
  "One Row Per Element In Longest Group",
  "Allocate Embedding Matrix",
  "Compute Padding in Embedding Matrix",
  "Compute Embedding Indices ...",
  "... And Use Them to Copy Each Group Into Its Column",
  "Compute colSums...",
  "... And Admire Them in 3D",
  NULL
)
dat05 <- copy(dat10)[, step:=0]
dat120 <- copy(dat110)[, step:=max(step) + 1]
dat <- rbind(
  dat05, dat10, dat20, dat30, dat40, dat50, dat60,
  dat70, dat80, dat90, dat100, dat110, dat120
)
dat[is.na(alpha), alpha:=1]
# dcast(dat[, .N, .(step, label)], label ~ step, fill=0)
stopifnot(all(dat[, type] %in% c('tile', 'label', 'path', 'point', 'text')))
dat[, Step:=factor(steps[match(step, unique(sort(step)))], levels=steps)]
dat[, g2f:=factor(g2, levels=as.character(sort(unique(g2))))]

 # Code to line up with the steps

code <- c(
'set.seed(1)

g <- sample(10, 95, r=TRUE)   # Group is shown as color
x <- runif(95)                # Value is 3D tile height',

'set.seed(1)

g <- sample(10, 95, r=TRUE)
x <- runif(95)',

'o <- order(g)
go <- g[o]
xo <- x[o]',

"g.rle <- rle(go)
g.lens <- g.rle[['lengths']]

# group count
g.n <- length(g.lens)",

"g.rle <- rle(go)
g.lens <- g.rle[['lengths']]

# group count
g.n <- length(g.lens)",

"g.rle <- rle(go)
g.lens <- g.rle[['lengths']]

# longest group
g.max <- max(g.lens)",

"g.rle <- rle(go)
g.lens <- g.rle[['lengths']]

# longest group
g.max <- max(g.lens)",

"res <- matrix(
  0, ncol=g.n, nrow=g.max
)",

"pad <- max(g.lens) - g.lens + 1L",

"g.lens.hd <- head(g.lens, -1L)
id0 <- rep(1L, length(xo))
id0[(cumsum(g.lens.hd) + 1L)] <- pad

id.embed <- cumsum(id0)",

"res[id.embed] <- xo",

"colSums(res)",

"# Group is shown as tile color
# Value is 3D tile height

colSums(res)",

# "",

NULL
)
dat.code <- data.table(text=code, Step=factor(steps))

 # dat <- subset(dat, step==steps[[5]])
 # dat.code <- subset(dat.code, step==steps[[5]])

library(ggplot2)
library(gganimate)
scale.fill <- scale_fill_manual(
  guide=FALSE, na.value='#F3F3F3',
  values=setNames(
    c(viridisLite::viridis(g.n), 'grey50'),
    as.character(seq_len(g.n + 1L))
  )
)
make_plot <- function(dat, dat.code, text.size=16, lab.size=5) {
  p <- ggplot(mapping=aes(x, y, alpha=alpha, group=paste(label, id))) +
    geom_tile(
      data=subset(dat, type=='tile'), aes(fill=g2f),
      color='#AAAAAA'
    ) +
    geom_point(
      data=subset(dat, type=='point'), size=4,
      aes(shape=point.shape)
    ) +
    # geom_polygon(data=subset(dat, type=='path')) +
    geom_label(
      data=subset(dat, type=='label'), aes(label=glabel), size=lab.size
    ) +
    # geom_text(data=subset(dat, type=='text'), aes(label=glabel), size=5) +
    # we save the last group color for an off-scale color
    scale.fill +
    scale_alpha(
      guide=FALSE, rescaler=function(x, to, from, ...) x, range=c(0,1)
    ) +
    scale_shape_manual(values=c(a=16, b=1), guide=FALSE) +
    make.plot.theme +
    theme(plot.title=element_text(size=text.size)) +
    NULL
    if(!is.null(dat.code)) {
      p <- p + geom_text(
        data=dat.code,
        aes(label=text, group=NULL, alpha=NULL, x=NULL, y=NULL),
        x=-0.5, y=-10,
        hjust=0, vjust=1, size=9, family='mono'
      )
    }
    p
}
```
```{r echo=FALSE, eval=FALSE}
 ## gganimate

steps.core <- tail(head(seq_len(nrow(dat.code)), -1L), -1L)
steps.new <- steps[steps.core]
dat.sub <- dat[!step %in% range(step)]
dat.sub[, Step:=droplevels(Step)]
dat.code.sub <- dat.code[steps.core]
dat.code.sub[, Step:=droplevels(Step)]

p <- make_plot(dat.sub, dat.code.sub, text.size=24)
p.anim <- p +
    coord_fixed(xlim=c(-0.5,21.5), ylim=c(0,-13)) +
    transition_states(Step, wrap=FALSE) +
    labs(title = "{next_state}") +
    # ease_aes('cubic-in-out') +
    enter_grow() +
    exit_shrink() +
    NULL

 # anim_save(
 #   '~/Downloads/colsums-anim-2.gif',
 #   p.anim, width=1280, height=800, nframes=40, fps=5
# )
animate(
  p.anim,
  nframes = 400, fps=25, device = "png",
  renderer = file_renderer(
    "~/Downloads/colsums-anim/", prefix = "gganim-img", overwrite = TRUE
  ),
  width=1280, height=800
)

i.off <- -1
png.root <- '~/Downloads/colsums2/img-%03d.png'

for(i in seq_along(steps)) {
  file <- sprintf(png.root, i + i.off)
  png(file, width=1280, height=800)
  dat.sub <- subset(dat, Step==steps[[i]])
  dat.code.sub <- subset(dat.code, Step==steps[[i]])

  p <- make_plot(dat.sub, dat.code.sub, text.size=24) +
    ggtitle(steps[[i]]) +
    coord_fixed(xlim=c(-0.5,21.5), ylim=c(0,-13))
  print(p)
  dev.off()
  # remove color profile
  tmp <- png::readPNG(file)
  png::writePNG(tmp, file)
}
```
```{r index-compare, echo=FALSE, fig.width=8.89, fig.height=5.56, dpi=144}
 ## Additional frames

 # dat.sub <- rbind(
 #   subset(dat, Step==steps[[5]] & label!=0),
 #   transform(subset(dat, Step==steps[[5]] & label==0), alpha=0.3),
 #   transform(
 #     subset(dat, Step==steps[[4]] & label==0), Step=steps[[5]], label=5
 #   )
 # )

#dat.code.sub <- subset(dat.code, step==11)

dat.sub <- dat[Step == steps[11]]
dat.sub.ann <- rbind(
  dat.sub,
  dat.sub[label == 'main-back'][, `:=`(
    glabel=abs(y) + 1 + x * g.n, label='main-id', type='label'
  ) ],
  dat.sub[label == 'main'][, `:=`(
    glabel=abs(y) + 1 + (x - g.off) * g.max, label='main-back-id', type='label'
  ) ]
)
dat.text <- dat[Step == steps[10] & label == 'pad'][, `:=`(
  label='pad-id', type='text',
  glabel=abs(y) + 1 + (x - g.off) * g.max
)]

p <- make_plot(dat.sub.ann, NULL, lab.size=3) +
  geom_label(data=dat.text, aes(label=glabel), size=3, color='grey65')
p

 # idx1 <- subset(dat.sub, label==5, select=c(x, y, g2))
 # idx2 <- subset(dat.sub, label%in%c(0,1,3), select=c(x, y, g2))
 # idx2 <- idx2[with(idx2, order(x, -y, g2)),]
 # idx2.dup <- duplicated(idx2[1:2])
 # idx2 <- idx2[!idx2.dup,]
 #
 # idx <- rbind(
 #   transform(idx1[with(idx1, order(x, -y)),], label=seq_along(x)),
 #   transform(idx2[with(idx2, order(x, -y)),], label=seq_along(x))
 # )
 # p + geom_text(data=idx, aes(x, y, label=label, colour=factor(g2))) +
 #   scale_colour_manual(
 #     guide=FALSE, values=rep(c('white', 'black'), c(2, 8)), na.value='black'
 #   )
```

The indices corresponding to each group diverge after the first group due
to the unused elements of the embedding matrix, shown in grey above. What we're
looking for is a fast way to generate the indices in the colored cells in the
matrix on the right.  In other words, we want to generate the `id1` (`id.embed`
in the animation) vector below.  For clarity we only show it for the first three
groups:

```{r pad-index, echo=FALSE, fig.height=1.25}
g13 <- sum(g.rle[['lengths']][1:3])
g13id <- seq_len(g13)
id.dat <- data.table(
  x=rep(g13id, 2),
  y=rep(c(1, 0), each=g13),
  glabel=c(seq_along(g13id), id1[g13id]),
  g2=go[g13id]
)
grp.lab <- data.frame(
  x=cumsum(c(1, g.rle[['lengths']][1:2])) - .5,
  y=2,
  lab=paste('Group ', 1:3)
)
el.lab <- data.frame(x=-1, y=c(1, 0), lab=paste0('id', 0:1))
el.ellipse <- data.frame(x=23.5, y=0:2, lab="...")

ggplot(id.dat, aes(x, y)) +
  geom_tile(aes(fill=as.factor(g2)), color='#AAAAAA') +
  geom_text(data=grp.lab, aes(label=lab), hjust=0, vjust=.75) +
  geom_text(data=el.lab, aes(label=lab), hjust=0, family='mono') +
  geom_text(data=el.ellipse, aes(label=lab), family='mono', vjust=0.5) +
  geom_label(aes(label=glabel), size=2) +
  coord_fixed(xlim=c(-3,24), ylim=c(-1,2.5)) +
  scale.fill + make.plot.theme
```
We can emphasize the relationship between these by looking at the element by
element difference in each index vector, e.g. using `diff`:<span
id=diff-id1></span>

```{r pad-index-diff, echo=FALSE, fig.height=1.75}
id.dat.2 <- copy(id.dat)[y == 0, y:=-1]
el.ellipse <- data.frame(x=23.5, y=-2:2, lab="...")
diff.dat <- data.table(
  x=rep(tail(g13id, -1L), 2),
  y=rep(c(0,-2), each=g13-1),
  lab=c(diff(seq_along(g13id)), diff(id1[g13id]))
)
el.lab.2 <- data.frame(
  x=0, y=1:(-2), lab=c('id0', 'diff(id0)', 'id1', 'diff(id1)')
)
ggplot(id.dat.2, aes(x, y)) +
  geom_tile(aes(fill=as.factor(g2)), height=1, color='#AAAAAA') +
  geom_text(data=diff.dat, aes(label=lab, color=lab>1), size=3) +
  geom_text(data=grp.lab, aes(label=lab), hjust=0, vjust=.75) +
  geom_text(data=el.lab.2, aes(label=lab), hjust=1, family='mono') +
  geom_label(aes(label=glabel), size=2) +
  geom_text(data=el.ellipse, aes(label=lab), family='mono', vjust=0.5) +
  coord_fixed(xlim=c(-3,24), ylim=c(-3,2.5)) +
  scale.fill + make.plot.theme +
  scale_color_manual(values=c('black','green'), guide=FALSE)
```
```{r echo=FALSE, eval=FALSE}
g2.rle <- rle(sort(g2))

rle.len <- g2.rle[['lengths']][1:3]
gs <- rep(g2.rle[['values']][1:3], g2.rle[['lengths']][1:3])
max.g <- max(g2.rle[['lengths']])

g.pad <- max.g - rle.len + 1L
idx.raw <- rep(1L, length(gs))
idx.raw[(cumsum(rle.len) + 1L)] <- g.pad
idx <- cumsum(idx.raw[-length(idx.raw)])

xx <- rbind(gs, seq_along(idx), idx)
dimnames(xx) <- list(c('group', 'orig', 'embed'), rep('  ', ncol(xx)))
xx <- rbind(gs, seq_along(idx), c(NA, diff(seq_along(idx))), idx, c(NA, diff(idx)))
dimnames(xx) <- list(c('group', 'orig', 'orig ∆', 'embed', 'embed ∆'), rep('  ', ncol(xx)))
xx
```

The indices always increment by one, except at group transitions for the
embedding indices as shown in green above.  There they increment by `$1 + pad$`.
"`$pad$`" is how much empty space there is between the end of the group and the
end of the column it is embedded in.  The name of the game is to compute
"`$pad$`", which thankfully we can easily do by using the output of `rle`:

```{r echo=FALSE}
g <- sort(g2)
```
```{r}
g.rle <- rle(sort(g))
g.rle
```

`rle` gives us the length of runs of repeated values, which in our sorted group
vector gives us the size of each group.  Padding is then the difference between
each group's size and that of the largest group:

```{r}
g.max <- max(g.rle[['lengths']])      # largest group
(pad <- g.max - g.rle[['lengths']])
```

To compute the embedding vector we start by a vector of the differences which
as a baseline are all 1:

```{r}
id0 <- rep(1L, length(g))
```

We then add the padding at each group transition.  Conveniently, the group
transitions are just one element past the length of the previous element, so we
can add the padding at the positions following the cumulative sum of the group lengths:

```{r}
id0[cumsum(g.rle[['lengths']]) + 1L] <- pad + 1L
head(id0, 22)   # first three groups
```

You'll notice this is essentially the same thing as `diff(id1)` [from
earlier](#diff-id1).  We reproduce `id1` by applying `cumsum`:

```{r}
head(cumsum(id0), 22)
```
A distinguishing feature of these manipulations other than possibly inducing
death-by-boredom is that they are all in fast vectorized code.  This gives us
another reasonably fast group sum function.  We split it up into a function that
[calculates the embedding indices](#embed-fun) and one that does the [embedding
and sum](#col-sum-fun), for reasons that will become obvious later.  Assuming
sorted inputs[^sorted-inputs]:

```{r eval=FALSE}
sys.time({
  emb <- og_embed_dat(go)   # compute embedding indices
  og_sum_col(xo, emb)       # use them to compute colSums
})
```
```
   user  system elapsed
  0.502   0.195   0.699
```

Most of the run time is actually the embedding index calculation:

```{r eval=FALSE}
sys.time(emb <- og_embed_dat(go))
```
```
   user  system elapsed
  0.369   0.141   0.510
```

One drawback is the obvious wasted memory taken up by the padding in the
embedding matrix.  This could become problematically large if a small number of
groups are much larger than the rest.  It may be possible to mitigate this by
breaking up the data into by group size[^colsums-breakup].

Overall this is a little slower than `rowsum` for the simple group sum, but
as we'll see shortly there are benefits to this approach.

# Pedal To The Metal

For the sake of an absolute benchmark I wrote a [C version of `rowsum`,
`og_sum_C`](#og_sum_C) that takes advantage of group-ordered data to compute the
group sums and counts[^group-order].  Once the data is sorted, this takes
virtually no time:

```{r eval=FALSE}
sys.time(og_sum_C(xo, go))
```
```
   user  system elapsed
  0.039   0.001   0.041
```
<!--
sys.time({
  o <- order(grp)
  og_sum_C(x[o], grp[o])
})
   user  system elapsed
  0.735   0.004   0.743
-->

The only way to make this substantially faster is to make the sorting faster, or
come up with an algorithm that can do the group sums without sorting and somehow
do it faster than what we have here.  Both of these are beyond my reach.

Let's compare against all the different methods, including the [original
`cumsum` based group sum][110] (`cumsum-1`), and the [precision corrected two
pass version][170] (`cumsum-2`):

<!-- See static/..._files/scripts/benchmark.Rmd -->
```{r sum-benchmarks, echo=FALSE, warning=FALSE}
types <- rep(c('base',  'C', 'data.table'), c(5, 1, 1) * 3)
funs <- c(
  'tapply', 'cumsum-1', 'cumsum-2', 'rowsum', 'colSums',
  'og_sum_C',
  'simple'
)
steps <- c(rep(c('order',  'rle*', 'sum'), 6), rep(NA, 3))
times <- data.frame(
  Function=factor(rep(funs, each=3), levels=unique(funs)),
  Type=factor(types, levels=unique(types)),
  Step=factor(steps, levels=step.levels),
  time=c(
    .696,    NA, 2.624, # tapply
    .696, 0.514, 0.047, # cumsum-1
    .696, 0.514, 0.231, # cumsum-2
    .696,    NA, 0.583, # rowsum
    .696, 0.510, 0.189, # colSums
    .696,    NA, 0.044, # sum-only
   #.696  # full
      NA,    NA, 1.023  # simple
   #  NA,    NA,        # group-join-group
   #  NA,    NA,        # reformulate
  )
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col(aes(fill=Step)) +
  # facet_wrap(~Type, scales='free_x') +
  facet_grid(.~Type, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  # geom_label(
  #   data=data.table(times)[
  #     order(Step, decreasing=TRUE),
  #     .(time=time, timec=cumsum(c(0,time[-length(time)]))),
  #     .(Function, Type)
  #   ],
  #   aes(label=sprintf("%0.2f", time), y=timec + time/2)
  # ) +
  geom_text(
    data=data.table(times)[, .(time=sum(time, na.rm=TRUE)), .(Function, Type)],
    aes(label=sprintf("%0.2f", time)), vjust=-.2
  ) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1))) +
  scale_x_discrete(drop=TRUE) +
  scale_fill_manual(values=brewer3, na.value='grey50', drop=FALSE) +
  ggtitle("Group Sums, By Method")
```

We're actually able to beat `data.table` with our custom C code, although that
is only possible because `data.table` [contributed its fast radix sort to
R][180], and `data.table` requires more complex code to be able to run a broader
set of statistics.

The pattern to notice is that for several of the methods the time spent
doing the actual summing is small[^sum-contains].  For example, for `colSums`
most of the time is ordering and computing the run length encoding / embedding
indices (`rle*`).  This is important because those parts can be re-used if the
grouping is the same across several calculations.  It doesn't help for single
variable group sums, but if we have more variables or more complex statistics it
becomes a factor.

Let's see how helpful re-using the group-based data is with the calculation of
the slope of a bivariate regression:

$$\frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i -
\bar{x})^{2}}$$

The calculation requires four group sums, two to compute `$\bar{x}$`  and
`$\bar{y}$`, and another two shown explicitly with the `$\sum$` symbol.  At the
same time we only need to compute the ordering and the `rle` / embedding once
because the grouping is the same across all calculations.  You can see this
clearly in the [`colSums` based group slope calculation in the
appendix](#g_sum_col).

When we compare all the previous implementations of the group slope
calculation, against the new `rowsum` and `colSums` implementations the
advantage of re-using the group information becomes apparent:

<!-- See static/..._files/scripts/benchmark.Rmd -->
```{r slope-benchmarks, echo=FALSE, warning=FALSE}
types <- rep(c('base',  'C', 'data.table'), c(5, 2, 3))
funs <- c(
  'vapply', 'cumsum-1', 'cumsum-2', 'rowsum', 'colSums',
  'sum-only', 'full',
  'simple', 'optim', 'reformulated'
)
times <- data.frame(
  Function=factor(funs, levels=funs),
  Type=factor(types, levels=unique(types)),
  time=c(
   8.473, # tapply
   2.355, # cumsum-1
   3.020, # cumsum-2
   3.694, # rowsum
   2.765, # colSums
   1.641, # sum-only
   1.092, # full
   6.981, # simple
   3.117, # group-join-group
   1.516 # reformulate
  )
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col() +
  facet_grid(.~Type, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  # geom_label(
  #   data=data.table(times)[
  #     order(Step, decreasing=TRUE),
  #     .(time=time, timec=cumsum(c(0,time[-length(time)]))),
  #     .(Function, Type)
  #   ],
  #   aes(label=sprintf("%0.2f", time), y=timec + time/2)
  # ) +
  geom_text(
    data=data.table(times)[, .(time=sum(time, na.rm=TRUE)), .(Function, Type)],
    aes(label=sprintf("%0.2f", time)), vjust=-.2
  ) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1))) +
  ggtitle("Group Slopes, By Method")
```

Even though `rowsum` was the fastest group sum implementation, it is the
slowest of the base options outside of `split/vapply` because none of the
computation components other than the re-ordering can be re-used.  `colSums`
does pretty well and has the advantage of not suffering from the precision
issues of `cumsum-1`[^cumsum-1], and additionally of naturally handling NA and
non-finite values.  `cumsum-2`[^cumsum-2] might be the best bet of the "base"
solutions as it is only slightly slower than `colSums` method, but should scale
better if there are some groups that are much larger than most[^na-inf].

`data.table` gets pretty close to the C implementation, but only in the
reformulated form which comes [with challenging precision issues][200].

# That's All Folks

<!-- this needs to become a shortcode -->
<a href='#image-credits' title='Click for image credits.' class=image-credit>
<img
  id='front-img'
  src= "/post/2019-05-17-pixie-dust_files/user-imgs/hydra-white.png"
  class='post-inset-image'
/>
</a>

Phew, finally done.  I never imagined how out of hand some silly benchmarks
against `data.table` would get. I learned way more than I bargained for, and
come away from the whole thing with a renewed admiration for what R does: it
can often provide near-statically-compiled performance in an interpreted
language right out of the box[^near-statically].  It's not always easy to
achieve this, but in many cases it is possible with a little thought and care.

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<p id='feedback-cont'></p>

This post is the last post of the [Hydra Chronicles][100] post series.

# Appendix

## Acknowledgments

These are post-specific acknowledgments.  This website owes many additional
thanks to [generous people and organizations][270] that have made it possible.

* [Thomas Lin Pedersen][300] and [Tyler Morgan Wall][280] for [`gganimate`][310]
  and [`rayshader`][290][^rayshader-mostly] respectively with which I made the
  `colSums` group sum animation frames, and the FFmpeg team for [FFmpeg][270]
  with which I stitched the frames into [the video][320].
* [Matt Dowle][201] and the other [`data.table` authors][204] for contributing
  the radix sort to R without which none of the fast group statistic methods
  would be fast.
* [Oleg Sklyar][123], [Dirk Eddelbuettel][122], [Romain François][121], etal. for
  [`inline`][410] for easy integration of C code into ad-hoc R functions.
* [Hadley Wickham][202] and the [`ggplot2` authors][203] for `ggplot2` with
  which I made many the plots in this post.
* Simon Urbanek for the [PNG
  package](https://cran.r-project.org/web/packages/png/index.html) which I used
  in the process of manipulating the animation frames.
* [Simon Garnier][350] etal. for [`viridisLite`][370], and by extension the
  [`matplotlib`][360] team from which it was ported.
* [Cynthia Brewer][390] for color brewer palettes, [one of which][380] I used in
  some plots, and [axismaps][400] for the web tool for picking them.

## Image Credits

* [Hydra With Fedora(s)][52], by [Larry Wentzel][190], under CC BY-NC 2.0,
  haberdashery and haberdasher removed.

## Session Info

```{r eval=FALSE}
sessionInfo()
```
```
R version 3.6.0 (2019-04-26)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS Mojave 10.14.6

Matrix products: default
BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] data.table_1.12.2 gganimate_1.0.3   ggplot2_3.2.0     rayshader_0.11.5 

loaded via a namespace (and not attached):
 [1] rgl_0.100.19            Rcpp_1.0.1              prettyunits_1.0.2      
 [4] png_0.1-7               ps_1.3.0                assertthat_0.2.1       
 [7] rprojroot_1.3-2         digest_0.6.19           foreach_1.4.4          
[10] mime_0.7                R6_2.4.0                imager_0.41.2          
[13] tiff_0.1-5              plyr_1.8.4              backports_1.1.4        
[16] evaluate_0.14           blogdown_0.12           pillar_1.4.1           
[19] rlang_0.4.0             progress_1.2.2          lazyeval_0.2.2         
[22] miniUI_0.1.1.1          rstudioapi_0.10         callr_3.2.0            
[25] bmp_0.3                 rmarkdown_1.12          desc_1.2.0             
[28] devtools_2.0.2          webshot_0.5.1           servr_0.13             
[31] stringr_1.4.0           htmlwidgets_1.3         igraph_1.2.4.1         
[34] munsell_0.5.0           shiny_1.3.2             compiler_3.6.0         
[37] httpuv_1.5.1            xfun_0.8                pkgconfig_2.0.2        
[40] pkgbuild_1.0.3          htmltools_0.3.6         readbitmap_0.1.5       
[43] tidyselect_0.2.5        tibble_2.1.3            bookdown_0.10          
[46] codetools_0.2-16        crayon_1.3.4            dplyr_0.8.3            
[49] withr_2.1.2             later_0.8.0             grid_3.6.0             
[52] xtable_1.8-4            jsonlite_1.6            gtable_0.3.0           
[55] magrittr_1.5            scales_1.0.0            cli_1.1.0              
[58] stringi_1.4.3           farver_1.1.0            fs_1.3.1               
[61] promises_1.0.1          remotes_2.0.4           doParallel_1.0.14      
[64] testthat_2.1.1          iterators_1.0.10        tools_3.6.0            
[67] manipulateWidget_0.10.0 glue_1.3.1              tweenr_1.0.1           
[70] purrr_0.3.2             hms_0.4.2               crosstalk_1.0.0        
[73] jpeg_0.1-8              processx_3.3.1          pkgload_1.0.2          
[76] parallel_3.6.0          colorspace_1.4-1        sessioninfo_1.1.1      
[79] memoise_1.1.0           knitr_1.23              usethis_1.5.0          
```

## Data

```{r child='../../static/chunks/grp-dat.Rmd'}
```

## Functions

### sys.time

```{r child='../../static/chunks/sys-time.Rmd'}
```

### og_sum_col

The `og_` prefix stands for "Ordered Group".

Compute indices to embed group ordered data into a regular matrix:<span
id='embed-fun'></span>

```{r eval=FALSE}
og_embed_dat <- function(go) {
  ## compute run length encoding
  g.rle <- rle(go)
  g.lens <- g.rle[['lengths']]
  max.g <- max(g.lens)

  ## compute padding
  g.lens <- g.lens[-length(g.lens)]
  g.pad <- max.g - g.lens + 1L

  ## compute embedding indices
  id0 <- rep(1L, length(go))
  id0[(cumsum(g.lens) + 1L)] <- g.pad
  id1 <- cumsum(id0)

  list(idx=id1, rle=g.rle)
}
```

And use `colSums` to compute the group sums:<span id='col-sum-fun'></span>

```{r eval=FALSE}
og_sum_col <- function(xo, embed_dat, na.rm=FALSE) {
  ## group sizes
  rle.len <- embed_dat[['rle']][['lengths']]

  ## allocate embedding matrix
  res <- matrix(0, ncol=length(rle.len), nrow=max(rle.len))

  ## copy data using embedding indices
  res[embed_dat[['idx']]] <- xo
  setNames(colSums(res, na.rm=na.rm), embed_dat[['rle']][['values']])
}
```

### og_sum_C

Similar to `rowsum`, except it requires ordered input, and it returns group
sizes as an attribute.  Group sizes allow us to either compute means or recycle
the result statistic back to the input length.

This is a limited, lightly tested, implementation that only works for double `x`
values and relies completely on the native code to handle NA/Infinite values.
It will ignore dimensions of matrices, and has undefined behavior if any group
has more elements than than `INT_MAX`.

Inputs must be ordered in increasing order by group, with if it exists the NA
group last.  The NA group will be treated as a single group (i.e. NA==NA is
TRUE).

```{r group_sum, eval=FALSE}
og_sum_C <- function(x, group) {
  stopifnot(
    typeof(x) == 'double', is.integer(group), length(x) == length(group)
  )
  tmp <- .og_sum_C(x, group)
  res <- setNames(tmp[[1]], tmp[[2]])
  attr(res, 'grp.size') <- tmp[[3]]
  res
}
.og_sum_C <- inline::cfunction(
  sig=c(x='numeric', g='integer'),
  body="
  R_xlen_t len, i, len_u = 1;
  SEXP res, res_x, res_g, res_n;
  int *gi = INTEGER(g);
  double *xi = REAL(x);
  len = XLENGTH(g);
  if(len != XLENGTH(x)) error(\"Unequal Length Vectors\");
  res = PROTECT(allocVector(VECSXP, 3));

  if(len > 1) {
    // count uniques
    for(i = 1; i < len; ++i) {
      if(gi[i - 1] != gi[i]) {
        ++len_u;
    } }
    // allocate and record uniques
    res_x = PROTECT(allocVector(REALSXP, len_u));
    res_g = PROTECT(allocVector(INTSXP, len_u));
    res_n = PROTECT(allocVector(INTSXP, len_u));

    double *res_xi = REAL(res_x);
    int *res_gi = INTEGER(res_g);
    int *res_ni = INTEGER(res_n);
    R_xlen_t j = 0;
    R_xlen_t prev_n = 0;

    res_xi[0] = 0;
    for(i = 1; i < len; ++i) {
      res_xi[j] += xi[i - 1];
      if(gi[i - 1] == gi[i]) {
        continue;
      } else if (gi[i - 1] < gi[i]){
        res_gi[j] = gi[i - 1];
        res_ni[j] = i - prev_n;  // this could overflow int; undefined?
        prev_n = i;
        ++j;
        res_xi[j] = 0;
      } else error(\"Decreasing group order found at index %d\", i + 1);
    }
    res_xi[j] += xi[i - 1];
    res_gi[j] = gi[i - 1];
    res_ni[j] = i - prev_n;

    SET_VECTOR_ELT(res, 0, res_x);
    SET_VECTOR_ELT(res, 1, res_g);
    SET_VECTOR_ELT(res, 2, res_n);
    UNPROTECT(3);
  } else {
    // Don't seem to need to duplicate x/g
    SET_VECTOR_ELT(res, 0, x);
    SET_VECTOR_ELT(res, 1, g);
    SET_VECTOR_ELT(res, 2, PROTECT(allocVector(REALSXP, 0)));
    UNPROTECT(1);
  }
  UNPROTECT(1);
  return res;
")
```

### g_slope_col

Compute the slope using the [`colSums` based group sum](#og_sum_col).  Notice
how we compute the embedding indices once, and re-use them for all four group
sums.

```{r eval=FALSE}
g_slope_col <- function(x, y, group) {
  ## order
  o <- order(group)
  go <- group[o]
  xo <- x[o]
  yo <- y[o]

  ## compute group means for x/y
  emb <- og_embed_dat(go)                    # << Embedding
  lens <- emb[['rle']][['lengths']]
  ux <- og_sum_col(xo, emb)/lens             # << Group Sum #1
  uy <- og_sum_col(yo, emb)/lens             # << Group Sum #2

  ## recycle means to input vector length and
  ##  compute (x - mean(x)) and (y - mean(y))
  gi <- rep(seq_along(ux), lens)
  x_ux <- xo - ux[gi]
  y_uy <- yo - uy[gi]

  ## Slope calculation
  gs.cs <-
    og_sum_col(x_ux * y_uy, emb) /           # << Group Sum #3
    og_sum_col(x_ux ^ 2, emb)                # << Group Sum #4
  setNames(gs.cs, emb[['rle']][['vaues']])
}
sys.time(g_slope_col(x, y, grp))
```
```
   user  system elapsed
  2.268   0.497   2.765
```

### g_slope_C

This is a very lightly tested all C implementation of the group slope.

```{r group_slope, eval=FALSE}
g_slope_C <- function(x, y, group) {
  stopifnot(
    typeof(x) == 'double', is.integer(group), length(x) == length(group),
    typeof(y) == 'double', length(x) == length(y)
  )
  o <- order(group)
  tmp <- .g_slope_C(x[o], y[o], group[o])
  res <- setNames(tmp[[1]], tmp[[2]])
  res
}
.g_slope_C <- inline::cfunction(
  sig=c(x='numeric', y='numeric',  g='integer'),
  body="
  R_xlen_t len, i, len_u = 1;
  SEXP res, res_x, res_g, res_y;
  int *gi = INTEGER(g);
  double *xi = REAL(x);
  double *yi = REAL(y);
  len = XLENGTH(g);
  if(len != XLENGTH(x)) error(\"Unequal Length Vectors\");
  res = PROTECT(allocVector(VECSXP, 2));

  if(len > 1) {
    // First pass compute unique groups
    for(i = 1; i < len; ++i) {
      if(gi[i - 1] != gi[i]) {
        ++len_u;
    } }
    // allocate and record uniques
    res_x = PROTECT(allocVector(REALSXP, len_u));
    res_y = PROTECT(allocVector(REALSXP, len_u));
    res_g = PROTECT(allocVector(INTSXP, len_u));

    double *res_xi = REAL(res_x);
    double *res_yi = REAL(res_y);
    int *res_gi = INTEGER(res_g);
    R_xlen_t j = 0;
    R_xlen_t prev_i = 0, n;

    // Second pass compute means

    double xac, yac;
    yac = xac = 0;
    for(i = 1; i < len; ++i) {
      xac += xi[i - 1];
      yac += yi[i - 1];
      if(gi[i - 1] == gi[i]) {
        continue;
      } else if (gi[i - 1] < gi[i]){
        n = i - prev_i;
        res_xi[j] = xac / n;
        res_yi[j] = yac / n;
        res_gi[j] = gi[i - 1];
        prev_i = i;
        yac = xac = 0;
        ++j;
      } else error(\"Decreasing group order found at index %d\", i + 1);
    }
    xac += xi[i - 1];
    yac += yi[i - 1];
    n = i - prev_i;
    res_xi[j] = xac / n;
    res_yi[j] = yac / n;
    res_gi[j] = gi[i - 1];

    // third pass compute slopes

    double xtmp, ytmp;
    yac = xac = xtmp = ytmp = 0;
    j = 0;

    for(i = 1; i < len; i++) {
      xtmp = xi[i - 1] -  res_xi[j];
      ytmp = yi[i - 1] -  res_yi[j];
      xac += xtmp * xtmp;
      yac += ytmp * xtmp;

      if(gi[i - 1] == gi[i]) {
        continue;
      } else {
        res_xi[j] = yac / xac;
        yac = xac = 0;
        ++j;
      }
    }
    xtmp = xi[i - 1] -  res_xi[j];
    ytmp = yi[i - 1] -  res_yi[j];
    xac += xtmp * xtmp;
    yac += ytmp * xtmp;
    res_xi[j] = yac / xac;

    SET_VECTOR_ELT(res, 0, res_x);
    SET_VECTOR_ELT(res, 1, res_g);
    UNPROTECT(3);
  } else {
    // Don't seem to need to duplicate x/g
    SET_VECTOR_ELT(res, 0, x);
    SET_VECTOR_ELT(res, 1, g);
    SET_VECTOR_ELT(res, 2, PROTECT(allocVector(REALSXP, 0)));
    UNPROTECT(1);
  }
  UNPROTECT(1);
  return res;
")
```

[^rowsum-matrix]: `rowsum` returns a matrix even in the one column case so we
  drop the dimensions with `c` to make the comparison to `colSums` more obvious.
[^dt-single]: I use single threaded timings as those are more stable, and
  it allows apples-to-apples comparisons.  While it is a definite advantage that
  `data.table` comes with its own built-in parallelization, enabling its use
  means the benchmarks are only relevant for R processes that are themselves not
  parallelized.
[^sorted-inputs]: We could have the function sort the inputs itself, but doing
  it this way allows us to compare to the other functions for which we pre-sort,
  and to re-use the ordering data when summarizing multiple variables.
[^colsums-breakup]: I implemented a test version to test feasibility and it had
  comparable performance
[^near-statically]: I consider ~3x close given that typically differences
  between statically compiled and interpreted coder are more on the order of
  ~10-100x.
[^group-order]: With group-ordered data we can detect group transitions any time
  the value of  the group vector changes.  We can use that as a trigger to
  transfer the accumulator value to the result vector and reset it.
  We did something  similar with our [re-implementation of `unique` for sorted
  data][160].
[^sum-contains]: `sum` time is essentially all time that is neither ordering,
  `rle`, or embedding index calculation, so for some functions it includes steps
  other than just summing.
[^cumsum-1]: Original [single pass `cumsum`][110] based group sum.
[^cumsum-2]: `cumsum` based group sum with [second precision correcting
  pass][170].  In this case we use the second pass for every one of the four
  group sums.
[^liberties]: `tapply` calls `lapply` internally, not `sapply`, but the
  semantics of the default `simplify=TRUE` case are equivalent.  Additionally,
  the simplification isn't done with `c` as suggested by the diagram, but in
  this case with scalar return values from `sum` it's also semantically
  equivalent.
[^rayshader-mostly]: I used `rayshader` primarily to compute the shadows on the
  texture of the 3D plots.  Obviously the idea of the 3D `ggplot` comes from
  `rayshader` too, but since I do not know how to control it precisely enough to
  merge the frames from its 3D plots into those of `gganimate` I ended up using
  my own half-baked 3D implementation.  I've [recorded the code][340] for the
  curious, but be prepared to recoil in horror.  For an explanation of how the
  3D rendering works see the [Stereoscopic post][330].
[^na-inf]: Note we'll really need to use the more complex and probably slightly
  slower [version that handles NAs and non-finite values][420].

[100]: /tags/hydra/
[110]: /2019/06/10/base-vs-data-table/#group-sums
[120]: /2019/06/10/base-vs-data-table/#rowsums
[130]: /2019/05/17/pixie-dust/#loose-ends
[140]: /2019/05/17/pixie-dust/
[150]: /2019/06/10/base-vs-data-table/#rowsums
[160]: /2019/06/10/base-vs-data-table/#interlude-better-living-through-sorted-data
[170]: /2019/06/18/hydra-precision/#a-new-hope
[180]: https://twitter.com/BrodieGaslam/status/1106231241488154626

<!-- Original Hydra Link
Larry Wentzel
https://www.flickr.com/photos/wentzelepsy/
Attribution-NonCommercial 2.0 Generic (CC BY-NC 2.0)
-->
[52]: https://www.flickr.com/photos/wentzelepsy/15431618539/in/photolist-pvD5yB-orMGkS-oD9tMr-6Hy4vZ-dHArXF-4MkiSq-fmXesZ-fnKybA-7PdCYt-nKGMUu-9DwyBF-4RGwKf-95vYq-4YHwip-69ckvt-FwgCQm-95vLz-6w9W6f-bVZLqM-95vWc-4YHsJc-4YJhhV-fnvhfn-9DzB8h-4YHxAB-9db7nP-fnvrrP-Ww91p9-8Yhwn3-4tLnpy-bsHuHt-4RGJSU-8nLdkA-4MkjF5-4tLnzC-5Zr3e9-fnKFf7-69gAij-5sR6jQ-9DwEzx-gfesaD-fnf39A-6w5JmX-69gz5o-5xYUVS-6MLZD9-5N2EcN-X463Df-G23wk-fmXqjH
[190]: https://www.flickr.com/photos/wentzelepsy/
[200]: /2019/07/24/hydra-reformulate/#slow-down-cowboy
[260]: /about/#acknowledgments
[270]: http://ffmpeg.org/about.html
[280]: https://twitter.com/tylermorganwall
[290]: https://github.com/tylermorganwall/rayshader
[300]: https://twitter.com/thomasp85
[310]: https://github.com/thomasp85/gganimate
[320]: /post/2019-07-26-hydra-loose-ends_files/scripts/anim.html
[330]: /2018/12/12/three-d-pipeline/
[340]: /post/2019-07-26-hydra-loose-ends_files/scripts/threed-plot.R

[202]: https://github.com/hadley
[203]: https://cran.r-project.org/web/packages/ggplot2/index.html
[121]: https://github.com/romainfrancois

[122]: https://github.com/eddelbuettel
[123]: https://github.com/osklyar
[350]: https://github.com/sjmgarnier
[360]: https://github.com/matplotlib/matplotlib/
[370]: https://cran.r-project.org/web/packages/viridisLite/
[380]: http://colorbrewer2.org/#type=qualitative&scheme=Set2&n=3
[390]: http://www.personal.psu.edu/cab38/
[400]: https://www.axismaps.com/
[410]: https://github.com/eddelbuettel/inline

[201]: https://github.com/mattdowle
[204]: https://cloud.r-project.org/web/packages/data.table/
[420]: /2019/06/10/base-vs-data-table/#cumulative-group-sum-with-na-and-inf
