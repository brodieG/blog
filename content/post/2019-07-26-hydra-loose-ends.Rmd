---
title: "Hydra Chronicles Part V: Loose Ends"
author: ~
date: '2019-07-26'
slug: hydra-loose-ends
categories: [r]
tags: [group-stats,optim,hydra]
image: /front-img/default.png
imagerect: ~
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: Front page summary
---

```{r echo=FALSE}
old.opt <- options(digits=3)
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
suppressMessages(library(ggplot2))
suppressMessages(library(viridisLite))
suppressMessages(library(data.table))
setDTthreads(1)
```
```{r echo=FALSE, comment="", results='asis'}
old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)
options(fansi.term.cap=c('bright', '256', 'truecolor'))
```

# Almost Done!

<!-- this needs to become a shortcode -->
<img
  id='front-img' src='/front-img/default.png'
  class='post-inset-image'
/>

Fingers crossed this will be the last post of the [Hydra Chronicles][100], a.k.a.
"Everything You Didn't Want To Know About Group Statistics But Let Me Tell You
Anyway".  Well, it's more of an end note rather than a real post: a few tidbits
that ended up on the cutting floor in the earlier posts.

One of our early "discoveries" about group statistics is that if we can compute
the group sum, we can build on that to compute many other group statistics.
We'll go over here a few of the other strategies I tested out before I settled
on the [`cumsum` based approach][110] we used to "beat" `data.table`.

# rowsum

I did mention this one [in passing][120], but it bears re-examining:
`base::rowsum` is a remarkable creature in the base R ecosystem.  As far as I
know, it is the only R function that can compute group statistics on unequal
group sizes in statically compiled C code.  Despite this it is relatively
unknown, especially when compared with its popular cousin `base::rowSums`.  

It likely doesn't help that the names of the two functions are barely
distinguishable, despite the difference in behavior.  `rowsum` collapses rows
together by group, leaving column count unchanged, whereas `rowSums` collapses
all columns in each row into a single scalar value:

```{r}
(mx <- matrix(rep(c(1, 2, 3), 2), nrow=3))
rowsum(mx, c('a', 'b', 'b'))
matrix(rowSums(mx))   # rowSums returns a vector
```

In the single column/vector case, `rowsum(x, grp)` is essentially equivalent to
`tapply(x, grp, sum)` (or `vapply(split(x, grp), sum, 0)`).

Let's look at some examples and timings with our beaten-to-death 10MM row, ~1MM
group [data set](#data), and our timing function [`sys.time`](#sys-time).  We'll
order the data first as [that is fastest][140] even when including the time to
order:

```{r eval=FALSE}
sys.time({
  o <- order(grp)
  go <- grp[o]
  xo <- x[o]
})
```
```
   user  system elapsed 
  0.714   0.007   0.729 
```

`tapply` first for reference:

```{r eval=FALSE}
sys.time(gsum.0 <- tapply(xo, go, sum))
```
```
   user  system elapsed 
  2.482   0.129   2.624 
```

And now `rowsum`:

```{r eval=FALSE}
sys.time(gsum.1 <- rowsum(xo, go))
```
```
   user  system elapsed 
  0.537   0.043   0.583 
```

```{r eval=FALSE}
all.equal(c(gsum.0), c(gsum.1), check.attributes=FALSE)
```

`rowsum` provides a noticeable speed-up over `tapply` for the same task, though
`data.table` remains faster.

```{r eval=FALSE}
setDTthreads(1)   # single thread data.table
sys.time(data.table(x, grp)[, sum(x), keyby=grp][, setNames(V1, grp)])
```
```
   user  system elapsed 
  0.997   0.021   1.023 
```

`rowsum` may be fast enough for some applications where `tapply` is not.  To
summarize:

```{r echo=FALSE}
step.levels <- c('order',  'rle*', 'sum')
funs <- c('tapply', 'tapply', 'rowsum', 'rowsum', 'data.table')
steps <- c('order', 'sum', 'order', 'sum', NA)
times <- data.frame(
  Function=factor(funs, levels=unique(funs)),
  # Version=c('data.table', 'group_slope', 'data.table', 'group_slope'),
  Step=factor(steps, levels=step.levels[-2]),
  time=c(.729, 2.624, .729, .583, 1.025)
)
brewer3 <- c('#66c2a5','#fc8d62','#8da0cb')
ggplot(times, aes(x=Function, y=time)) +
  geom_col(aes(fill=Step)) +
  # facet_grid(.~Function, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  geom_label(
    data=data.table(times)[!is.na(Step)][
      order(Step, decreasing=TRUE),
      .(time=time, timec=cumsum(c(0,time[-length(time)]))),
      .(Function)
    ],
    aes(label=sprintf("%0.2f", time), y=timec + time/2)
  ) +
  geom_text(
    data=data.table(times)[, .(time=sum(time)), .(Function)],
    aes(label=sprintf("%0.2f", time)), vjust=-.2
  ) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1))) +
  scale_fill_manual(values=brewer3[-2], na.value='grey50', drop=FALSE)
```

The slow step for `data.table` is also the ordering too, but we don't have a
good way to break that out.

# colSums

`base::rowSums`, the aforementioned better-known cousin of `rowsum`, also
computes group statistics with statically compiled code.  Well, it kind of does
if you consider matrix rows to be groups. `base::colSums` does the same except
for columns.  Suppose we have three equal sized ordered groups:

```{r}
(G <- rep(1:3, each=2))
```

And values that belong to them:

```{r}
set.seed(1)
(a <- runif(6))
```

We can compute the group sums using `colSums` by wrapping our vector into a
matrix with as many columns as there are groups.  Since R internally stores
matrices as vectors in column-major order, this is a natural operation (and also
why we use `colSums` instead of `rowSums`):

```{r}
(a.mx <- matrix(a, ncol=length(unique(G))))
colSums(a.mx)
```

This is equivalent to:

```{r}
c(rowsum(a, G))
```

We run into problems as soon as we have uneven group lengths, but there is a
workaround.  The idea is to use clever indexing to embed the values associated
with each group into columns of a matrix.  We illustrate this process with a
vector with 95 elements in ten groups.  For display purposes we wrap the vector
column-wise every ten elements, and designate the groups by a color.  The values
of the vector are not shown explicitly.

<!--
See static/post/2019-07-26-hydra-loose-ends_files/scripts/ for the code used to
generate the flipbook and animation.
-->

<script type='text/javascript'>
var img_dir = '/post/2019-07-26-hydra-loose-ends_files/user-imgs/flip-book/';
var fps_def = 0.5;
var img_n = 6;
var playing = true;
</script>
```{r child='../../static/chunks/flipbook.Rmd'}
```

The embedding step warrants additional explanation.  The trick is to generate a
vector that maps the positions in our irregular input vector into the regular
matrix.  There are several ways we can do this, but the one that we'll use today
takes advantage of the underlying vector nature of matrices.  In particular, we
will index into our matrices as if they were vectors, e.g.:

```{r}
(b <- 1:4)
(b.mx <- matrix(b, 2))
b[3]
b.mx[3]  # index vector underlying matrix directly
```

Let's look at our 95 data before and after embedding, showing the indices in
vector format for both our ordered vector and the target matrix:

```{r echo=FALSE}
set.seed(1)
gn <- 10
g2 <- sample(seq_len(gn), 95, replace=TRUE)
 # Horrible horrible, but it works
levels <- c('start', 'order', 'longest', 'allocate', 'embed', 'colSums')

 # labels are:
 # 0: original colored squares
 # 1: first set of re-allocation (first column)
 # 3: rest of reallocation
 # 4: background for colored squares
 # 5: original colored squares, but after the move

dat0 <- data.frame(
  step=1L, id=seq_along(g2), g2,
  x=(seq_along(g2) - 1) %/% 10, y=-((seq_along(g2) - 1) %% 10),
  alpha=1, label=0
)
dat10 <- rbind(transform(dat0, label=4, g2=NA), dat0)
dat20 <- transform(dat10, step=2L)
dat20 <- rbind(
  subset(dat20, label==4),
  transform(
    subset(dat20, label==0)[order(g2),],
    x=(seq_along(g2) - 1) %/% 10, y=-((seq_along(g2) - 1) %% 10)
  )
)
g.rle <- with(subset(dat20, label==0), rle(g2))

dat30 <- transform(
  dat20, step=3L,
  alpha=ifelse(
    g2==g.rle[['values']][which.max(g.rle[['lengths']])], 1, .2
  )
)
g.max <- nrow(subset(dat30, alpha==1))
dat30 <- rbind(
  dat30,
  transform(subset(dat30, alpha==1), g2=NA, label=1)
)
dat30 <- rbind(
  subset(dat30, label%in%c(0,4)),
  transform(
    subset(dat30, label==1), x=gn + 1, y=-(seq_along(y) - 1)
  )
)
dat30 <- do.call(
  rbind,
  c(
    list(dat30),
    replicate(
      gn - 1L, transform(subset(dat30, label==1), label=3), simplify=FALSE)
) )
dat30[dat30[['label']] == 3, 'id'] <- seq_along(which(dat30[['label']] == 3))
dat50 <- transform(
  dat30,
  step=4L,
  alpha=1
)
dat50[dat50[['label']] %in% c(1,3),] <- transform(
  subset(dat50, label %in% c(1,3)),
  x=rep((seq_len(gn)) + gn, each=g.max)
)
dat60 <- transform(
  rbind(
    subset(dat50, label %in% c(1,3,4)),
    transform(
      subset(dat50, label==0), x=gn + g2, y=-sequence(g.rle[['lengths']]) + 1
    )
  ),
  step=5L
)
dat70 <- transform(
  rbind(
    subset(dat60, label %in% c(1, 3, 4)),
    transform(subset(dat60, label == 0), y=0)
  ),
  step=6L
)
steps <- c(
  "Start",
  "Order by Group",
  "Compute Length of Longest Group",
  "Alloc Matrix w/ Enough Rows to Fit Longest Group in One Col",
  "Copy Each Group Into a Col",
  "Compute colSums"
)
dat <- transform(
  rbind(dat10, dat20, dat30, dat50, dat60, dat70),
  step=factor(steps[step], levels=steps)
)
 # Code to line up with the steps

code <- c(
'set.seed(1)
g <- sample(10, 95, r=TRUE) # groups
x <- runif(95)              # values',

'o <- order(g)
go <- g[o]
xo <- x[o]',

"g.rle <- rle(go)
g.lens <- g.rle[['lengths']]
g.max <- max(g.lens)",

"res <- matrix(
  0, ncol=length(g.lens), nrow=g.max
)",

"pad <- g.max - g.lens + 1L
id0 <- rep(1L, length(xo))
id0[(cumsum(g.lens) + 1L)] <- pad
id1 <- cumsum(head(id0, -1L))
res[id1] <- xo",

"colSums(res)"
)
dat.code <- data.frame(text=code, step=factor(steps))
dat <- dat[with(dat, order(step, -label)),]

 # dat <- subset(dat, step==steps[[5]])
 # dat.code <- subset(dat.code, step==steps[[5]])

library(ggplot2)
library(gganimate)

make_plot <- function(dat, dat.code) {
  ggplot(dat) +
    geom_tile(
      aes(x, y, fill=g2, alpha=I(alpha), group=paste(label, id)),
      color='#AAAAAA'
    ) +
    scale_fill_viridis_c(guide=FALSE, na.value='#F3F3F3') +
    scale_alpha(guide=FALSE, rescaler=function(x, to, from, ...) x) +
    theme(
      panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
      axis.title.x=element_blank(), axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.title.y=element_blank(), axis.text.y=element_blank(),
      axis.ticks.y=element_blank(),
      plot.title=element_text(size=16)
    ) + if(!is.null(dat.code)) {
      geom_text(
        data=dat.code, aes(label=text), x=-0.5, y=-10,
        hjust=0, vjust=1, size=6, family='mono'
      )
    }
}
p <- make_plot(dat, dat.code)

# ## gganimate
# 
# p + facet_wrap(~step)
# anim_save(
#   '~/Downloads/colsums-anim.gif',
#   p +  transition_states(step, wrap=FALSE, state_length=4) +
#     labs(title = "{closest_state}"),
#   width=800, height=557, end_pause=5, nframes=350, fps=25
# )
# 
# ## pngs for flipbook
# 
# png.root <- '~/Downloads/colsums/img-%03d.png'
# 
# for(i in 1:6) {
#   png(sprintf(png.root, i), width=800, height=557)
#   dat.sub <- subset(dat, step==steps[[i]])
#   dat.code.sub <- subset(dat.code, step==steps[[i]])
# 
#   p <- make_plot(dat.sub, dat.code.sub) +
#     ggtitle(steps[[i]]) +
#     coord_cartesian(xlim=c(0,20), ylim=c(0,-13))
#   print(p)
#   dev.off()
# }

## Additional frames

dat.sub <- rbind(
  subset(dat, step==steps[[5]] & label!=0),
  transform(subset(dat, step==steps[[5]] & label==0), alpha=0.3),
  transform(
    subset(dat, step==steps[[4]] & label==0), step=steps[[5]], label=5
  )
)
dat.code.sub <- subset(dat.code, step==steps[[5]])

p <- make_plot(dat.sub, NULL)

idx1 <- subset(dat.sub, label==5, select=c(x, y, g2))
idx2 <- subset(dat.sub, label%in%c(0,1,3), select=c(x, y, g2))
idx2 <- idx2[with(idx2, order(x, -y, g2)),]
idx2.dup <- duplicated(idx2[1:2])
idx2 <- idx2[!idx2.dup,]

idx <- rbind(
  transform(idx1[with(idx1, order(x, -y)),], label=seq_along(x)),
  transform(idx2[with(idx2, order(x, -y)),], label=seq_along(x))
)
p + geom_text(data=idx, aes(x, y, label=label, colour=factor(g2))) +
  scale_colour_manual(
    guide=FALSE, values=rep(c('white', 'black'), c(2, 8)), na.value='black'
  )
```

The indices corresponding to each group diverge after the first group due
to the unused elements of the embedding matrix.  What we're looking for is a
fast way to generate the indices in the colored cells in the matrix on the
right.  In other words, we want to generate the `id1` vector below (we show
the first three groups worth of it here):

```{r echo=FALSE, eval=FALSE}
g2.rle <- rle(sort(g2))

rle.len <- g2.rle[['lengths']][1:3]
gs <- rep(g2.rle[['values']][1:3], g2.rle[['lengths']][1:3])
max.g <- max(g2.rle[['lengths']])

g.pad <- max.g - rle.len + 1L
idx.raw <- rep(1L, length(gs))
idx.raw[(cumsum(rle.len) + 1L)] <- g.pad
idx <- cumsum(idx.raw[-length(idx.raw)])

xx <- rbind(gs, seq_along(idx), idx)
dimnames(xx) <- list(c('group', 'orig', 'embed'), rep('  ', ncol(xx)))
xx <- rbind(gs, seq_along(idx), c(NA, diff(seq_along(idx))), idx, c(NA, diff(idx)))
dimnames(xx) <- list(c('group', 'orig', 'orig ∆', 'embed', 'embed ∆'), rep('  ', ncol(xx)))
xx
```
```{r echo=FALSE}
writeLines('group      1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  3  3  3  3  3  3  3 
id0       \033[37;48;2;67;01;83m 1  2  3  4  5  6  7  8 \033[48;2;71;39;119m 9 10 11 12 13 14 15 \033[48;2;62;64;136m16 17 18 19 20 21 22 \033[m
id1       \033[37;48;2;67;01;83m 1  2  3  4  5  6  7  8 \033[48;2;71;39;119m15 16 17 18 19 20 21 \033[48;2;62;64;136m29 30 31 32 33 34 35 \033[m')
```

A helpful way to emphasize the relationship between these is to look at the
element by element difference in each index vector, e.g. using `diff`:

```{r echo=FALSE}
writeLines('group      1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  3  3  3  3  3  3  3 
id0       \033[37;48;2;67;01;83m 1  2  3  4  5  6  7  8 \033[48;2;71;39;119m 9 10 11 12 13 14 15 \033[48;2;62;64;136m16 17 18 19 20 21 22 \033[m
diff(id0) NA  1  1  1  1  1  1  1 \033[32m 1 \033[m 1  1  1  1  1  1 \033[42m 1 \033[m 1  1  1  1  1  1 
id1       \033[37;48;2;67;01;83m 1  2  3  4  5  6  7  8 \033[48;2;71;39;119m15 16 17 18 19 20 21 \033[48;2;62;64;136m29 30 31 32 33 34 35 \033[m
diff(id1) NA  1  1  1  1  1  1  1 \033[32m 7 \033[m 1  1  1  1  1  1 \033[42m 8 \033[m 1  1  1  1  1  1
')
```

The indices always increment by one, except for the embedded vector, where they
increment by `$1 + pad$`, where `$pad$` is how much empty space there is between
the end of the group and the end of the column it is embedded in.  Flipping this
around, this means all we need to be able to compute the embedding indices is
figure out the size of the column padding, which will be a function of the group
size and the largest group size.  Thankfully the `rle` computes the lengths of
repeated sequences, which in the case of our ordered group vector corresponds
exactly to group sizes:

```{r echo=FALSE}
g <- sort(g2)
```
```{r}
g.rle <- rle(sort(g))
g.rle
```

Padding is the difference between each group's size and that of the largest
group:

```{r}
g.max <- max(g.rle[['lengths']])  # largest group
pad <- g.max - g.rle[['lengths']]
```

To compute the embedding vector we start by a vector of the differences which
as a baseline are all 1:

```{r}
id0 <- rep(1L, length(g))
```

We then add the padding at each group transition.  Conveniently, the group
transitions are just one element past the length of the previous element, so we
can add the padding at the positions following the cumulative sum of the group lengths:

```{r}
id0[cumsum(g.rle[['lengths']]) + 1L] <- pad + 1L
head(id0, 22)   # first three groups
```

You'll notice this is essentially the same thing as `diff(id1)` from earlier.
Thus, if we apply `cumsum` we reproduce `id1`:

```{r}
head(cumsum(id0), 22)
```
A distinguishing feature of these manipulations other than possibly inducing
death-by-boredom is that they are all in fast vectorized code.  This gives us
another reasonably fast group sum function.  We split it up into a function that
calculates the embedding index and one that does the embedding and sum, for
reasons that will become obvious later.  Assuming sorted inputs[^sorted-inputs]:

```{r eval=FALSE}
og_embed_dat <- function(go) {
  ## compute run length encoding
  g.rle <- rle(go)
  max.g <- max(g.rle[['lengths']])

  ## compute embedding indices
  rle.len <- g.rle[['lengths']]
  g.pad <- max.g - rle.len + 1L
  id0 <- rep(1L, length(go))
  id0[(cumsum(rle.len) + 1L)] <- g.pad
  id1 <- cumsum(id0[-length(id0)])

  list(idx=id1, rle=g.rle)
}
og_sum_col <- function(xo, embed_dat, na.rm=FALSE) {
  rle.len <- embed_dat[['rle']][['lengths']]
  res <- matrix(0, ncol=length(rle.len), nrow=max(rle.len))
  res[embed_dat[['idx']]] <- xo
  setNames(colSums(res, na.rm=na.rm), embed_dat[['rle']][['values']])
}
sys.time(og_sum_col(xo, og_embed_dat(go)))
```
```
   user  system elapsed 
  0.544   0.214   0.763 
```

Most of the run time is actually the embedding index calculation:

```{r eval=FALSE}
sys.time(emb <- og_embed_dat(go))
```
```
   user  system elapsed 
  0.405   0.139   0.546 
```

This is a little slower than `rowsum` for the simple group sum, but there are
benefits to this approach.  The main drawback is the potential for the embedding
matrix to become inefficiently large if a small number of groups are much larger
than the rest.  It may be possible to mitigate this by breaking up the data into
sub by groups[^colsums-breakup].

# Pedal To The Metal

Out of curiosity I wrote a [C version of `rowsum`,
`og_sum_C`](#og_sum_C) that takes advantage of group ordered
data to compute the group sums and counts, similar to our [re-implementation of
`unique` for sorted data][160].  We can detect group transitions any time the
group vector changes, and use that to store group values/counts and
reset the accumulators.  Once the data is sorted, this takes virtually no time:

```{r}
sys.time(og_sum_C(xo, go))
```
```
   user  system elapsed 
  0.039   0.001   0.041 
```

Let's compare against all the different methods, including the [original
`cumsum` based group sum][110] (`cumsum-1`), and the [precision corrected two
pass version][170] (`cumsum-2`):

<!-- See static/..._files/scripts/benchmark.Rmd -->
```{r sum-benchmarks, echo=FALSE, warning=FALSE}
types <- rep(c('base',  'C', 'data.table'), c(5, 1, 1) * 3)
funs <- c(
  'tapply', 'cumsum-1', 'cumsum-2', 'rowsum', 'colSums',
  'og_sum_C',
  'simple'
)
steps <- c(rep(c('order',  'rle*', 'sum'), 6), rep(NA, 3))
times <- data.frame(
  Function=factor(rep(funs, each=3), levels=unique(funs)),
  Type=factor(types, levels=unique(types)),
  Step=factor(steps, levels=step.levels),
  time=c(
    .729,    NA, 2.624, # tapply
    .729, 0.514, 0.047, # cumsum-1
    .729, 0.514, 0.231, # cumsum-2
    .729,    NA, 0.583, # rowsum
    .729, 0.546, 0.217, # colSums
    .729,    NA, 0.131, # sum-only
   #.729  # full
      NA,    NA, 1.023  # simple
   #  NA,    NA,        # group-join-group
   #  NA,    NA,        # reformulate
  )
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col(aes(fill=Step)) +
  # facet_wrap(~Type, scales='free_x') +
  facet_grid(.~Type, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  # geom_label(
  #   data=data.table(times)[
  #     order(Step, decreasing=TRUE),
  #     .(time=time, timec=cumsum(c(0,time[-length(time)]))),
  #     .(Function, Type)
  #   ],
  #   aes(label=sprintf("%0.2f", time), y=timec + time/2)
  # ) +
  geom_text(
    data=data.table(times)[, .(time=sum(time, na.rm=TRUE)), .(Function, Type)],
    aes(label=sprintf("%0.2f", time)), vjust=-.2
  ) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1))) +
  scale_x_discrete(drop=TRUE) +
  scale_fill_manual(values=brewer3, na.value='grey50', drop=FALSE) +
  ggtitle("Group Sums, By Method")
```

We're actually able to beat `data.table` with our custom C code, although that
is only possible because `data.table` [contributed its fast radix sort to
R][180], and `data.table` requires more complex code to be able to run a broader
set of statistics.

But the pattern to notice here is that for several of the methods the time spent
doing the actual summing is small.  For example, for `colSums`, most of the time
is spent ordering and computing the run length encoding / embedding indices
(`rle*`).  This is important because those parts are a function of the grouping,
so they only need to be calculated once per group.  It doesn't help for single
variable group sums, but if we have more variables or more complex statistics it
can make a big difference.

Let's see how helpful re-using the group-based data is with the calculation of
the slope of a bivariate regression:

$$\frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i -
\bar{x})^{2}}$$

For illustrative purposes we'll show the implementation that uses the `colSums`
based group sum (`og_sum_col`).  Don't worry too much about the
details.  The key thing to note is that we use `og_sum_col` four times,
but we only compute the embedding data once:

```{r eval=FALSE}
g_slope_col <- function(x, y, group) {
  ## order
  o <- order(group)
  go <- group[o]
  xo <- x[o]
  yo <- y[o]

  ## compute group means for x/y
  emb <- og_embed_dat(go)
  lens <- emb[['rle']][['lengths']]
  ux <- og_sum_col(xo, emb)/lens
  uy <- og_sum_col(yo, emb)/lens

  ## recycle means to input vector length and compute
  ## (x - mean(x)) and (y - mean(y))
  gi <- rep(seq_along(ux), lens)
  x_ux <- xo - ux[gi]
  y_uy <- yo - uy[gi]

  ## Slope calculation
  gs.cs <- og_sum_col(x_ux * y_uy, emb) / og_sum_col(x_ux ^ 2, emb)
  setNames(gs.cs, emb[['rle']][['vaues']])
}
sys.time(g_slope_col(x, y, grp))
```
```
   user  system elapsed 
  2.268   0.497   2.765 
```

We can compare this to all the previous implementations of the group slope
calculation, as well as variations on the above based on `rowsum`, 
[`og_sum_C`](#pedal-to-the-metal) (the C version of `rowsum` that operates
explicitly on group ordered ordered data), and finally a full C
version of the slope calculation to define the limit of how fast we can go:

<!-- See static/..._files/scripts/benchmark.Rmd -->
```{r slope-benchmarks, echo=FALSE, warning=FALSE}
types <- rep(c('base',  'C', 'data.table'), c(5, 2, 3))
funs <- c(
  'vapply', 'cumsum-1', 'cumsum-2', 'rowsum', 'colSums',
  'sum-only', 'full',
  'simple', 'group\njoin\ngroup', 'reformulated'
)
times <- data.frame(
  Function=factor(funs, levels=funs),
  Type=factor(types, levels=unique(types)),
  time=c(
   8.473, # tapply
   2.355, # cumsum-1
   3.020, # cumsum-2
   3.694, # rowsum
   2.765, # colSums
   1.641, # sum-only
   1.092, # full
   6.981, # simple
   3.117, # group-join-group
   1.516 # reformulate
  )
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col() +
  facet_grid(.~Type, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  # geom_label(
  #   data=data.table(times)[
  #     order(Step, decreasing=TRUE),
  #     .(time=time, timec=cumsum(c(0,time[-length(time)]))),
  #     .(Function, Type)
  #   ],
  #   aes(label=sprintf("%0.2f", time), y=timec + time/2)
  # ) +
  geom_text(
    data=data.table(times)[, .(time=sum(time, na.rm=TRUE)), .(Function, Type)],
    aes(label=sprintf("%0.2f", time)), vjust=-.2
  ) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1))) +
  ggtitle("Group Slopes, By Method")
```

# Conclusions

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id='feedback-cont'></div>

# Appendix

## Acknowledgments

* This blog would not exist but for amazing contributions from many ...
* Rcpp/inline
* data.table (computations)
* ggplot
* viridisLite
* Color Brewer for the palette. http://colorbrewer2.org/#type=qualitative&scheme=Set2&n=3

   
## Data

```{r child='../../static/chunks/grp-dat.Rmd'}
```

## Functions

### sys.time

### og_sum_C

Similar to `rowsum`, except it requires ordered input, and it returns group
sizes as an attribute.  Group sizes allow us to either compute means or recycle
the result statistic back to the input length.

This is a limited, lightly tested, implementation that only works for double `x`
values and relies completely on the native code to handle NA/Infinite values.
It will ignore dimensions of matrices, and has undefined behavior if any group
has more elements than than `INT_MAX`.

Inputs must be ordered in increasing order by group, with if it exists the NA
group last.  The NA group will be treated as a single group (i.e. NA==NA is
TRUE).

```{r group_sum, eval=FALSE}
og_sum_C <- function(x, group) {
  stopifnot(
    typeof(x) == 'double', is.integer(group), length(x) == length(group)
  )
  tmp <- .og_sum_C(x, group)
  res <- setNames(tmp[[1]], tmp[[2]])
  attr(res, 'grp.size') <- tmp[[3]]
  res
}
.og_sum_C <- inline::cfunction(
  sig=c(x='numeric', g='integer'),
  body="
  R_xlen_t len, i, len_u = 1;
  SEXP res, res_x, res_g, res_n;
  int *gi = INTEGER(g);
  double *xi = REAL(x);
  len = XLENGTH(g);
  if(len != XLENGTH(x)) error(\"Unequal Length Vectors\");
  res = PROTECT(allocVector(VECSXP, 3));

  if(len > 1) {
    // count uniques
    for(i = 1; i < len; ++i) {
      if(gi[i - 1] != gi[i]) {
        ++len_u;
    } }
    // allocate and record uniques
    res_x = PROTECT(allocVector(REALSXP, len_u));
    res_g = PROTECT(allocVector(INTSXP, len_u));
    res_n = PROTECT(allocVector(INTSXP, len_u));

    double *res_xi = REAL(res_x);
    int *res_gi = INTEGER(res_g);
    int *res_ni = INTEGER(res_n);
    R_xlen_t j = 0;
    R_xlen_t prev_n = 0;

    res_xi[0] = 0;
    for(i = 1; i < len; ++i) {
      res_xi[j] += xi[i - 1];
      if(gi[i - 1] == gi[i]) {
        continue;
      } else if (gi[i - 1] < gi[i]){
        res_gi[j] = gi[i - 1];
        res_ni[j] = i - prev_n;  // this could overflow int; undefined?
        prev_n = i;
        ++j;
        res_xi[j] = 0;
      } else error(\"Decreasing group order found at index %d\", i + 1);
    }
    res_xi[j] += xi[i - 1];
    res_gi[j] = gi[i - 1];
    res_ni[j] = i - prev_n;

    SET_VECTOR_ELT(res, 0, res_x);
    SET_VECTOR_ELT(res, 1, res_g);
    SET_VECTOR_ELT(res, 2, res_n);
    UNPROTECT(3);
  } else {
    // Don't seem to need to duplicate x/g
    SET_VECTOR_ELT(res, 0, x);
    SET_VECTOR_ELT(res, 1, g);
    SET_VECTOR_ELT(res, 2, PROTECT(allocVector(REALSXP, 0)));
    UNPROTECT(1);
  }
  UNPROTECT(1);
  return res;
")
```

### g_slope_C

This is lightly tested.

```{r group_slope, eval=FALSE}
g_slope_C <- function(x, y, group) {
  stopifnot(
    typeof(x) == 'double', is.integer(group), length(x) == length(group),
    typeof(y) == 'double', length(x) == length(y)
  )
  o <- order(group)
  tmp <- .g_slope_C(x[o], y[o], group[o])
  res <- setNames(tmp[[1]], tmp[[2]])
  res
}
.g_slope_C <- inline::cfunction(
  sig=c(x='numeric', y='numeric',  g='integer'),
  body="
  R_xlen_t len, i, len_u = 1;
  SEXP res, res_x, res_g, res_y;
  int *gi = INTEGER(g);
  double *xi = REAL(x);
  double *yi = REAL(y);
  len = XLENGTH(g);
  if(len != XLENGTH(x)) error(\"Unequal Length Vectors\");
  res = PROTECT(allocVector(VECSXP, 2));

  if(len > 1) {
    // First pass compute unique groups
    for(i = 1; i < len; ++i) {
      if(gi[i - 1] != gi[i]) {
        ++len_u;
    } }
    // allocate and record uniques
    res_x = PROTECT(allocVector(REALSXP, len_u));
    res_y = PROTECT(allocVector(REALSXP, len_u));
    res_g = PROTECT(allocVector(INTSXP, len_u));

    double *res_xi = REAL(res_x);
    double *res_yi = REAL(res_y);
    int *res_gi = INTEGER(res_g);
    R_xlen_t j = 0;
    R_xlen_t prev_i = 0, n;

    // Second pass compute means

    double xac, yac;
    yac = xac = 0;
    for(i = 1; i < len; ++i) {
      xac += xi[i - 1];
      yac += yi[i - 1];
      if(gi[i - 1] == gi[i]) {
        continue;
      } else if (gi[i - 1] < gi[i]){
        n = i - prev_i;
        res_xi[j] = xac / n;
        res_yi[j] = yac / n;
        res_gi[j] = gi[i - 1];
        prev_i = i;
        yac = xac = 0;
        ++j;
      } else error(\"Decreasing group order found at index %d\", i + 1);
    }
    xac += xi[i - 1];
    yac += yi[i - 1];
    n = i - prev_i;
    res_xi[j] = xac / n;
    res_yi[j] = yac / n;
    res_gi[j] = gi[i - 1];

    // third pass compute slopes

    double xtmp, ytmp;
    yac = xac = xtmp = ytmp = 0;
    j = 0;

    for(i = 1; i < len; i++) {
      xtmp = xi[i - 1] -  res_xi[j];
      ytmp = yi[i - 1] -  res_yi[j];
      xac += xtmp * xtmp;
      yac += ytmp * xtmp;

      if(gi[i - 1] == gi[i]) {
        continue;
      } else {
        res_xi[j] = yac / xac;
        yac = xac = 0;
        ++j;
      }
    }
    xtmp = xi[i - 1] -  res_xi[j];
    ytmp = yi[i - 1] -  res_yi[j];
    xac += xtmp * xtmp;
    yac += ytmp * xtmp;
    res_xi[j] = yac / xac;

    SET_VECTOR_ELT(res, 0, res_x);
    SET_VECTOR_ELT(res, 1, res_g);
    UNPROTECT(3);
  } else {
    // Don't seem to need to duplicate x/g
    SET_VECTOR_ELT(res, 0, x);
    SET_VECTOR_ELT(res, 1, g);
    SET_VECTOR_ELT(res, 2, PROTECT(allocVector(REALSXP, 0)));
    UNPROTECT(1);
  }
  UNPROTECT(1);
  return res;
")
```

[^sorted-inputs]: We could have the function sort the inputs itself, but doing
  it this way allows us to compare to the other functions for which we pre-sort,
  and to re-use the ordering data when summarizing multiple variables.
[^colsums-breakup]: I implemented a test version to test feasibility and it had
  comparable performance
[^c-times]: Timing ordering and running `og_sum_C` separately versus together
  produces slightly different times, likely because by running the pieces
  separately it is possible to side-step a garbage collection event.  The
  timings in the plot are from running things together, and subtracting out the
  sorting / rle times.

[100]: /tags/hydra/
[110]: /2019/06/10/base-vs-data-table/#group-sums
[120]: /2019/06/10/base-vs-data-table/#rowsums
[130]: /2019/05/17/pixie-dust/#loose-ends
[140]: /2019/05/17/pixie-dust/
[150]: /2019/06/10/base-vs-data-table/#rowsums
[160]: /2019/06/10/base-vs-data-table/#interlude-better-living-through-sorted-data
[170]: /2019/06/18/hydra-precision/#a-new-hope
[180]: https://twitter.com/BrodieGaslam/status/1106231241488154626

