---
title: hydra-reformulate
author: ~
date: '2019-07-01'
slug: hydra-reformulate
categories: []
tags: [group-stats,optimization]
image: /post/2019-06-23-hydra-reformulate_files/user-imgs/var-reform-neg.png
imagerect: /post/2019-06-23-hydra-reformulate_files/user-imgs/var-reform-wide-neg.png
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: Front page summary
---

```{r echo=FALSE}
options(digits=3, datatable.print.topn=2)
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
```

# When Computing You Are Allowed to Cheat

<figure class='post-inset-image'>
<div class='post-inset-image-frame'><img
  id='front-img' 
  src='/post/2019-06-23-hydra-reformulate_files/user-imgs/var-reform-neg.png'
  class='post-inset-image'
/></div>
<figcaption>Reformulation of variance.<figcaption>
</figure>

Throughout the [Hydra Chronicles][100] we have explored why group statistics
are problematic in R and how to how to optimize their calculation.  R calls are
slow, but normally this is not a problem because R will quickly hand off to
native routines to do the heavily lifting.  With group statistics there must be
at least one R call per group.  When there are many groups as in the [10MM row
and ~1MM group data set](#data) we've been using, the R-level overhead adds up.

Both `data.table` and `dplyr` resolve the problem by providing native routines
to calculate per-group the most common statistics such as sum, mean, etc.  This
relies on the group expression being in the form `statistic(variable)` so that
the packages may easily recognize the attempt to compute known group statistics
and substitute the native code.  There is a more complete discussion of this
["Gforce" optimization in an earlier post][140].  We can see it an action here:

```{r eval=FALSE}
library(data.table)
setDTthreads(1)            # for more stable timings

sys.time({                 # wrapper around sytem.time; see appendix
  DT <- data.table(x, grp)
  var.0 <- DT[, .(var=var(x)), keyby=grp]
})
```
```
   user  system elapsed
  1.890   0.024   1.923
```

`sys.time` is a [wrapper around `system.time` defined in the
appendix](#sys-time).

```{r eval=FALSE}
var.0
```
```
            grp    var
     1:       1 0.0793
     2:       2 0.0478
    ---               
999952:  999999 0.1711
999953: 1000000 0.0668
```

This is fast because simple expressions such as `var(x)` can be "Gforce"
optimized.  But what if we wanted the plain second central moment instead of the
unbiased estimate produced by `var`:

$$E\left[\left(X - E[X]\right)^2\right]$$

Let's start with the obvious method[^alternate-unbiased]:

```{r eval=FALSE}
sys.time({
  DT <- data.table(x, grp)
  var.1 <- DT[, .(var=mean((x - mean(x)) ^ 2)), keyby=grp]
})
```
```
   user  system elapsed 
  4.663   0.049   4.728 
```
```{r eval=FALSE}
var.1
```
```
            grp    var
     1:       1 0.0721
     2:       2 0.0409
    ---               
999952:  999999 0.1466
999953: 1000000 0.0594
```

"Gforce" does not work with more complex expressions, so this ends up
substantially slower.  We can with some care break up the calculation into
simple steps so that ["Gforce" can be used][160], but this requires two passes
and an expensive join.  There is an alternative: as [Michael Chirico points
out][110], sometimes we can reformulate a statistic into a different easier to
compute form.

With the algebraic expansion:

$$(x - y)^2 = x^2 - 2 x y + y^2$$

And `$E[X]$` the expected value (mean) of a variable (vector) `$X$`, `$c$` a constant (scalar), and the following identities:

$$
\begin{align*}
E[c] &= c && Expect value (EV) of a constant is the constant.\\
E\left[E[X]\right] &= E[X] && EV of an EV is itself.\\
E[c X] &= c E[X] && EV of product of a constant and a variable is the product of
that constant and the EV of the variable.\\
E[X + Y] &= E[X] + E[Y] && EV of the sum of two variables is the sum of the EVs
of each.
\end{align*}
$$

And the expected value identities where `$X$` is a variable and
`$c$` a constant[^var-vec]:

We can [reformulate the variance calculation][150]:

$$
\begin{align*}
Var(X) = &E\left[(X - E\left[X\right])^2\right]\\
       = &E\left[X^2 - 2X E\left[X\right] + E\left[X\right]^2\right] \\
       = &E\left[X^2\right] - 2 E\left[X\right]E\left[X\right] + E\left[X\right]^2 \\
       = &E\left[X^2\right] - 2 E\left[X\right]^2 + E\left[X\right]^2 \\
       = &E\left[X^2\right] - E\left[X\right]^2 \\
\end{align*}
$$

The critical aspect of this reformulation is that there are no nested statistics
anymore, so we can compute all the component statistics in a single pass.  Let's
try it:

```{r eval=FALSE}
sys.time({
  DT <- data.table(x, grp)
  DT[, x2:=x^2]
  var.2 <- DT[,
    .(ux2=mean(x2), ux=mean(x)), keyby=grp  # grouping step
  ][,
    .(grp, var=ux2 - ux^2)                  # final calculation
  ]
})
```
```
   user  system elapsed 
  1.159   0.114   1.277 
```
```{r eval=FALSE}
all.equal(var.1, var.2)
```
```
[1] TRUE
```

Wow, that's even faster than the "Gforce" optimized "var".  There is only one
grouping step, and in that step all expressions are of the form
`statistic(variable)` so they can be "Gforce" optimized.

These concepts [apply equally to `dplyr`](#dplyr-reformulate).

# Slow Down Cowboy

I took the [reformulation for the variance][150] from wikipedia, which
additionally provides a rather scary warning:

> This equation should not be used for computations using floating point
> arithmetic because it suffers from catastrophic cancellation if the two
> components of the equation are similar in magnitude.

Uh-oh.  Let's see:

```{r eval=FALSE}
quantile(var.2 - var.3)
```
```
       0%       25%       50%       75%      100%
-4.72e-16 -4.16e-17  0.00e+00  4.16e-17  5.01e-16
```
```{r eval=FALSE}
quantile(((var.2 - var.3) / var.2), na.rm=TRUE)
```
```
       0%       25%       50%       75%      100%
-2.09e-07 -5.91e-16  0.00e+00  5.94e-16  2.91e-09
```

We did lose some precision, but certainly nothing catastrophic.  So under what
circumstances do we actually need to worry?  Let's cook up an example that
actually causes a problem:

```{r}
X <- c(2^52 + 1, 2^52 - 1)
mean(X)
mean((X - mean(X))^2)            # "normal" calc
mean(X^2) - mean(X)^2            # "reformulated" calc
```

We picked the numbers on purpose to be on the [edge of precision][170] of double
precision numbers.

If we carry out the algebraic expansion of the first element of `$X^2$` we can
see that problems crop up well before the subtraction:

$$
\begin{align*}
(X_1)^2 = &(2^{52} + 2^0)^2\\
        = &(2^{52})^2 + 2 \times 2^{52} \times 2^0 + (2^0)^2\\
        = &2^{104} + 2^{53} + 2^0
\end{align*}
$$

Double precision floating point numbers only have 53 bits of precision in the
fraction, which means the difference between the highest power of two and the
lowest one contained in a number cannot be greater than 52 without loss
of precision.  Yet, here we are trying to add `$2^0$` to a number that contains
`$2^{104}$`!  This is completely out of the realm of what double precision
numbers can handle:

```{r}
identical(
  2^104 + 2^53,
  2^104 + 2^53 + 2^0
)
```

One might think, "oh, that's just the last term, it doesn't matter", but it
does:

$$
\require{cancel}
\begin{align*}
E[X^2]  = &\frac{1}{N}\sum X^2\\
        = &\frac{1}{2}\left(\left(2^{52} + 2^0\right)^2 + \left(2^{52} - 2^0\right)^2\right)\\
        = &\frac{1}{2} \left(\left(\left(2^{104} \cancel{+ 2^{53}} + 2^0\right)\right) + \left((2^{104} \cancel{- 2^{53}} + 2^0)\right)\right)\\
        = &2^{104} + 2^0
\end{align*}
$$

The contributions from the middle terms cancel out, leaving only the last term
as the difference between `$E[X^2]$` and `$E[X]^2$`.

Generally we need to worry when `$\left|E[X]\right| \gg Var(X)$`.  The point at
which catastrophic precision loss happens will be around `$\left|E[X]\right| /
Var(X) \approx 26$`, as that is when squaring `$X$` values will cause most or
all the variance precision to overflow.  We can illustrate with a sample with
`$n = 100$`, `$Var(X) = 1$`, and varying ratios of 
`$\left|E[X]\right| / Var(X)$`, repeating the experiment 100 times for each
ratio:

<figure class='aligncenter' style='max-width: 100%;'>
```{r echo=FALSE, warning=FALSE}
#ns <- c(10, 100, 1000)
ns <- c(100)
ratios <- c(0, 10, 15, 20, 22, 24, 25, 26, 27, 28, 29, 30, 40, 50)
reps <- 100
res <- array(
  numeric(), dim=c(reps, length(ratios), length(ns), 3),
  dimnames=list(NULL, paste0("2^", ratios), ns, c('normal', 'reformulate','mean'))
)
dat <- array(
  list(), dim=c(reps, length(ratios), length(ns)),
  dimnames=list(NULL, paste0("2^", ratios), ns)
)
set.seed(42)
for(i in seq_len(reps)) {
  for(ri in seq_along(ratios)) {
    for(ni in seq_along(ns)) {
      X <- rnorm(ns[ni], mean=2^ratios[ri], sd=1)
      UX <- mean(X)
      dat[[i, ri, ni]] <- X
      res[i, ri, ni, 'normal'] <- mean((X - UX)^2)
      res[i, ri, ni, 'reformulate'] <- mean(X^2) - UX^2
      res[i, ri, ni, 'mean'] <- UX
    }
  }
}
res.rat <- (res[,,1,'normal'] - res[,,1,'reformulate'])/res[,,1,'normal']
library(reshape2)
res.df <- melt(res.rat)
library(ggplot2)
library(ggbeeswarm)
ggplot(res.df) +
  geom_quasirandom(aes(Var2, y=abs(value)), alpha=0.2) +
  scale_y_log10() +
  ylab(expression(paste("|", (Var[norm] - Var[ref]) / Var[norm], "|", " (log10)"))) +
  xlab('|E[X]| / Var(X)')
```
<figcaption>Relative error of reformulated variance calculation.</figcaption>
</figure>

The flat lines that start forming around `$2^{26}$` correspond to when the
reformulation produces exactly zero for the variance.  Beyond that point the
errors get dramatic.  Either the variance is computed as zero, or grossly
overestimated.

The overestimation of the variance means we cannot just measure the value of
`$\left|E[X]\right| / Var(X)$` to assess ex-post if our calculation suffers
substantial precision loss.  Under particularly egregious actual values of
`$\left|E[X]\right| / Var(X)$` the measured values appear perfectly reasonable,
as in when the actual ratio is `$2^{50}$`, but the measured ratio is as low as
`$2^2$`:

<figure class='aligncenter' style='max-width: 100%;'>
```{r echo=FALSE, warning=FALSE}
res.comp <- data.frame(
  reformulate=c(res[,,1,'mean']/res[,,1,'reformulate']),
  normal=c(res[,,1,'mean']/res[,,1,'normal'])
)
format_log_2 <- function(x) parse(text=paste0('2^', x))
ggplot(subset(res.comp, is.finite(reformulate))) +
  geom_point(aes(log2(normal), log2(reformulate))) +
  ylab('Reformulated') +
  xlab('Normal') +
  scale_y_continuous(label=format_log_2) +
  scale_x_continuous(label=format_log_2) +
  ggtitle("|E[X]| / Var(X), Reformulated vs. Normal")
```
</figure>

Infinite values (i.e. zero measured variance) are removed from the plot.

Another way to look at the precision loss is to examine the [underlying
binary representation][170] of the floating point numbers.  We do so here for
each of the 100 samples of each of the attempted `$\left|E[X]\right| /
Var(X)$` ratios:

<figure class='aligncenter' style='max-width: 100%;'>
```{r echo=FALSE, child='../../static/chunks/bin-rep.Rmd'}
```
```{r var-bin-rep, echo=FALSE, fig.height=2}
plot.new()
res2 <- as.data.frame(res[,,'100','reformulate'])
dat <- res2[paste0('2^', c(0, 15, 20, 24, 26, 30))]
lim <- 320
nrow <- 1
old.par <- par(mfrow=c(nrow,ceiling(ncol(dat)/nrow)), mar=c(.5, .25, 2, .25))
for(i in names(dat)) {
  suppressWarnings(plot(bin_rep(head(dat[[i]], lim))))
  title(i)
}
```
<figcaption>Binary representation of reformulated variance value.</figcaption>
</figure>

The precision loss shows up as trailing zeroes in those numbers (light green).
This view makes it obvious that there is essentially no precision by the time we
get to `$\left|E[X]\right| / Var(X) \approx 26$`, which is why the errors in the
reformulation start getting so wild.  We can estimate a lower bound on the
precision by inspecting the bit pattern, but since there may be legitimate
trailing zeroes this will likely be an under-estimate.  See the appendix for a
[method that checks](#check-bit-precision) a number has at least `n` bits of
precision.

We cannot reliably estimate precision loss from the reformulation, but in
practice data for which `$\left|E[X]\right| \gg Var(X)$` should be reasonably
rare.  In cases where that might hold true, we will often have a rough
order-of-magnitude estimate of `$E[X]$` which we can subtract from the data
prior to computation, though this is only useful if the mean is roughly the same
across all groups[^sub-penalty].

# Beyond Variance

While it's utterly fascinating (to some) that we can reformulate the variance
computation, it's of little value since both `data.table` and `dplyr` have
compiled code that obviates the need for it.  More generally this method should
be applicable to any expression that can be expanded and decomposed without
grouping such that every term is a scalar or a basic "Gforce" optimized
statistic of a vector.

With a little elbow grease we can reformulate the third moment:

$$
\begin{align*}
Skew(X) = &E\left[\left(X - E[X]\right)^3\right]\\
= & E\left[X^3 - 3 X^2 E\left[X\right] + 3 X E\left[X\right]^2 -
  E\left[X\right]^3\right]\\
                        = &
  E\left[X^3\right] - 3 E\left[X^2\right] E\left[X\right] +
  3 E\left[X\right] E\left[X\right]^2 - E\left[X\right]^3\\
                        = &
  E\left[X^3\right] - 3 E\left[X^2\right] - 2E\left[X\right]^3
\end{align*}
$$

And to confirm:

```{r}
X <- runif(100)
all.equal(
  mean((X - mean(X))^3),
  mean(X^3) - 3 * mean(X^2) * mean(X) + 2 * mean(X) ^3
)
```

Just beware that precision issues will become increasingly unforgiving with
larger polynomials as is the case here:

```{r echo=FALSE, warn=FALSE}
res <- array(
  numeric(), dim=c(reps, length(ratios), length(ns), 2),
  dimnames=list(NULL, paste0("2^", ratios), ns, c('normal', 'reformulate'))
)
set.seed(42)
for(i in seq_len(reps)) {
  for(ri in seq_along(ratios)) {
    for(ni in seq_along(ns)) {
      X <- rnorm(ns[ni], mean=2^ratios[ri], sd=1)
      res[i, ri, ni, 'normal'] <- mean((X - mean(X))^3)
      res[i, ri, ni, 'reformulate'] <-
        mean(X^3) - 3 * mean(X^2) * mean(X) + 2 * mean(X) ^3
    }
  }
}
res.rat <- (res[,,'100','normal'] - res[,,'100','reformulate'])/
  res[,,'100','normal']
library(reshape2)
res.df <- melt(res.rat)
library(ggplot2)
library(ggbeeswarm)
ggplot(res.df) +
  geom_quasirandom(aes(Var2, y=abs(value)), alpha=0.2) +
  scale_y_log10() +
  ylab(expression(paste("|", (Skw[norm] - Skw[ref]) / Skw[norm], "|", " (log10)"))) +
  xlab('|E[X]| / |Skw(X)|')
```

Similarly and what set me off on this blogpost, is that it is possible to
reformulate the bivariate linear regression slope.  What [Michael Chirico
saw][110] that was not immediately obvious to me is that:

$$
\begin{align*}
Slope(X,Y) 
= &\frac{Cov(X,Y)}{Var(X)}\\
= &\frac{
  E\left[
      \left(X - E\left[X\right]\right)
      \left(Y - E\left[Y\right]\right)
   \right]
  }{E\left[\left(X - E\left[X\right]\right)^2\right]}\\
= &\frac{
E\left[XY\right] - E\left[X\right]E\left[Y\right]}{
E\left[X^2\right] - E\left[X\right]^2}
\end{align*}
$$

In `data.table`-speak:

```{r eval=FALSE}
DT <- data.table(x, y, xy=x*y, x2=x^2, grp)
slope.dt.re <- DT[,
  .(ux=mean(x), uy=mean(y), uxy=mean(xy), ux2=mean(x2)),
  keyby=grp
][,
  .(grp, slope=(uxy - ux*uy)/(ux2 - ux^2))
]
```
```
   user  system elapsed
  1.377   0.126   1.507
```

As with all the reformulations we've seen the key feature of this one is
there is no interaction between grouped statistics and ungrouped ones, allowing
a one-pass group calculation.  With the one-pass reformulation `data.table` is
able to beat the `cumsum` based base-R method [we studied in the last
post][180][^prev-bench]:

```{r gs-timings-reform, echo=FALSE}
funs <- c('2-pass original', '2-pass original', 'reformulated', 'reformulated')
times <- data.frame(
  Function=factor(funs, levels=unique(funs)),
  Version=c('data.table', 'base cumsum', 'data.table', 'base cumsum'),
  time=c(3.139, 2.312, 1.507, 1.957)
)
ggplot(times, aes(x=Version, y=time)) +
  geom_col() +
  facet_grid(.~Function, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

In this case you cannot just examine the binary representation of the slope
value to look for precision issues.  The division of the covariance by the
variance may produce numbers that look like they are precise based on the bit
pattern, when in reality that is just the result of the interaction of one or
two imprecise numbers.  Instead, you will want to look at the covariance and the
variance values separately.

More generally we should be able to apply this expansion to any nested
statistics involving `sum` or `mean`, which will cover a broad range of
interesting statistics.  In the case of `sum`, keep in mind that:

$$
\sum_i^n c = n c
$$

And similarly:

$$
\sum_i^n \left(c + X\right) = n c + \sum_i^n X
$$

# Conclusions

Reformulations is a powerful tool for dealing with group statistics, but
appropriate care is required to understand and mitigate the associated precision
issues.  I haven't checked, but I suspect that a strategy based on looking at
bit patterns and recomputing the suspicious groups will end up slower than a two
pass approach.  The problem is that the bit pattern check will produce false
positives because any large enough number of groups will randomly have some of
them with statistics with  many trailing zeroes.  The cost of retrieving those,
recomputing them with a two pass calculation, and replacing the results will be
high.  And there is the risk of misinterpreting the bid patterns.  As noted
earlier, with the slope statistic we must look at the bit patterns of the
subcomponents of the statistic, not the statistic itself.



with 

This requires understanding the statistic and its
reformulation well enough to 

Given the difficulty


should be possible 
The reformulation approach to group statistics 
Using the reformulation approach could be useful in circumstances 


<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id='feedback-cont'></div>

# Appendix

## Acknowledgments

Special thanks to [Michael Chirico][205] for making me see what should have been
obvious from the get go.  The best optimization is always a better algorithm,
and I was too far removed from my econ classes to remember the expected value
identities.  So far removed in fact that my first reaction on seeing that his
calculations actually produced the same result was "how is that even
possible"...

Additionally:

* @coolbutuseless for showing me that `writeBin` can write directly to a raw
  vector.

## Software

`data.table`
`ggplot2`
`ggbeeswarm`


## Code

### dplyr Reformulate

We can apply the reformulation to `dplyr`, but the slow grouping holds it
back.

```{r eval=FALSE}
sys.time({
  var.2a <- tibble(x, grp) %>%
    mutate(x2=x^2) %>%
    group_by(grp) %>%
    summarise(ux2=mean(x2), ux=mean(x)) %>%
    mutate(var=ux2 - ux^2) %>%
    select(grp, var)
})
```
```
   user  system elapsed 
 11.171   0.299  11.660 
```
```{r eval=FALSE}
all.equal(as.data.frame(var.2), as.data.frame(var.2a))
```
```
[1] TRUE
```
### Data

```{r eval=FALSE, child='../../static/chunks/grp-dat.Rmd'}
```

### sys.time

```{r eval=FALSE, child='../../static/chunks/sys-time.Rmd'}
```

### Check Bit Precision

The following function will check whether doubles have at least `bits` precision
as implied by their underlying binary encoding.  The precision is defined by the
least significant "one" present in the fraction component of the number.  There
is no guarantee that the least significant "one" correctly represents the
precision of the number.  There may be trailing zeroes that are significant, or
a number may gain random trailing noise that is not a measure of precision.

While this implementation is reasonably fast, it is memory inefficient.

```{r}
# requires double `x` of length > 0, and positive scalar `bits`
# assumes little endian byte order.  Not tested with NA or infinite values.

has_precision_bits <- function(x, bits) {
  n <- 53 - min(c(bits-1, 53))
  mask.full <- n %/% 8
  mask.extra <- n %% 8
  mask <- as.raw(c(
    rep(255, mask.full),
    if(mask.extra) 2^(mask.extra) - 1,
    rep(0, 8 - (mask.full + (mask.extra > 0)))
  ) )
  colSums(matrix(as.logical(writeBin(x, raw()) & mask), nrow=8)) > 0
}
```

[^alternate-unbiased]: We could also multiply each group variance by `.N/(.N-1)`
  to undo the de-biasing, but that's beside the point of what we're trying to
  explore.
[^sub-penalty]: In the case where each group has means that are very different
  from the others this is no longer a useful correction as matching each group
  to its mean-estimate is costly.  At that point and we might as well revert to
  a two [pass solution][160].
[^var-vec]: For our purposes, we treat vectors as variables and scalars as
  constants.
[^bit-pattern]: Be careful though, if you operate on a number with many trailing
  zeroes the result may well gain trailing ones that are no longer indicative of
  true precision.
[^prev-bench]: What we are calling here the two-pass cumsum version is not the
  precision corrected one.  In this context, two-pass means we compute the group
  mean first, and then subtract it from the original variables.  For the
  precision corrected benchmarks see the [Catastrophic Imprecision][190] post.

[100]: /tags/group-stats/
[110]: https://twitter.com/michael_chirico/status/1138092237550579712
[120]: https://twitter.com/michael_chirico
[130]: /2019/06/10/base-vs-data-table/#the-ring-and-a-warmup
[140]: /2019/02/24/a-strategy-for-faster-group-statisitics/#what-is-this-sorcery
[150]: https://en.wikipedia.org/wiki/Variance#Definition
[160]: /2019/02/24/a-strategy-for-faster-group-statisitics/#blood-from-a-turnip
[170]: /2019/06/18/hydra-precision/#interlude-ieee-754
[180]: /2019/06/10/base-vs-data-table/#so-you-think-you-can-group-stat
[190]: /2019/06/18/hydra-precision/#verdict

