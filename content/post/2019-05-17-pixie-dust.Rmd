---
title: "Hydra Chronicles, Part I: Pixie Dust"
author: ~
date: '2019-05-17'
slug: pixie-dust
categories: [r]
tags: [optimization]
image: "/post/2019-05-15-pixiedust_files/user-images/cpu-die_westmere-6core_edited.jpg"
imagerect: "/post/2019-05-15-pixiedust_files/user-images/cpu-die_westmere-6core_small.jpg"
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: "How cache and other microarchitectural factors affect the
performance of R code, how the fast ordering contributed by data.table
mitigates potential problems, and opens up the possibility of faster
algorithms."
---

```{r echo=FALSE}
options(digits=3, crayon.enabled=TRUE)
suppressMessages(library(ggplot2))
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
```
```{r echo=FALSE, comment="", results='asis'}
old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)
```
```{r, echo=FALSE}
writeFansi <- function(x) {
  writeLines(
    paste0(
      "<pre></pre><pre><code>",
      paste0(fansi::sgr_to_html(x), collapse="\n"),
      "</code></pre>"
  ) )
}
```

# Often the Road Least Travelled is so For Good Reasons

<!-- this needs to become a shortcode -->
<img
  id='front-img' 
  src= "/post/2019-05-17-pixie-dust_files/user-imgs/hydra-white.png"
  class='post-inset-image'
/>

It wasn't supposed to be this way.  You shouldn't be looking at drawing of a
[Lernean Hydra][51] laughing its ass off.  This post was going to be short,
sweet, and useful.  Instead it turned into a many-headed monster that is smugly
sitting on my chest smothering all productivity out of me.  What follows here is
a re-telling of my flailing attempt at chopping off one its heads.

It all started on a lark to see if I could make base R competitive with the
undisputed champ `data.table` [at computing group statistics][21].  That thought
alone is scoff-worthy enough that I deserve little sympathy for what happened
next.  **Warning**: this is an optimization post in a blog about R, so please
set your expectations for riveting stories about adventures appropriately.

> TL;DR: ordering data can have a huge impact on how quickly it can be processed
> because of micro-architectural factors and because of better algorithms that
> become available.

Let's skip back to the care-free, hydra-less days when I should have known
better:

```{r echo=FALSE}
blogdown::shortcode("tweet", "1106231241488154626")
```

"More on this next week"... &#x1f923;&#x1f923;&#x1f923;.  Aaaah, what a fool.
That was two months ago.  The key observations back then were that:

* `data.table` contributed its radix sort to R in R3.3.0.
* As a result, `order` became 50x faster under at least some work loads.

I was also alluding that something fairly remarkable happens when you order data
before you process it.  Let's illustrate with 10MM items in ~1MM groups:

```{r warning=FALSE}
RNGversion("3.5.2"); set.seed(42)
n     <- 1e7
n.grp <- 1e6
grp   <- sample(n.grp, n, replace=TRUE)
x     <- runif(n)
```
```{r sys-time, echo=FALSE}
sys.time <- function(exp, reps=11) {
  res <- matrix(0, reps, 5)
  time.call <- quote(system.time({NULL}))
  time.call[[2]][[2]] <- substitute(exp)
  gc()
  for(i in seq_len(reps)) {
    res[i,] <- eval(time.call, parent.frame())
  }
  structure(res, class='proc_time2')
}
print.proc_time2 <- function(x, ...) {
  print(
    structure(
      x[order(x[,3]),][floor(nrow(x)/2),],
      names=c("user.self", "sys.self", "elapsed", "user.child", "sys.child"),
      class='proc_time'
) ) }
```

In my case I was splitting data to compute group statistics:

```{r eval=FALSE}
sys.time(gs <- split(x, grp))
```
```
   user  system elapsed 
  5.643   0.056   5.704 
```

`split` will split our vector `x` into a list of vectors as per `grp`, so the
following displays the first three split vectors:

```{r eval=FALSE}
str(head(gs, 3))
```
```
List of 3
 $ 1: num [1:11] 0.216 0.537 0.849 0.847 0.347 ...
 $ 2: num [1:7] 0.0724 0.1365 0.5364 0.0517 0.1887 ...
 $ 3: num [1:7] 0.842 0.323 0.325 0.49 0.342 ...
```

`sys.time` is a wrapper around `system.time`[^sys-time].   Compare to:

```{r eval=FALSE}
sys.time({
  o <- order(grp)
  xo <- x[o]
  go <- grp[o]
  gso <- split(xo, go)
})
```
```
   user  system elapsed 
  1.973   0.060   2.054 
```

Sorting values by group prior to running `split` makes it twice as fast,
**including the time required to sort the data**.  We just doubled the speed of
an important workhorse R function without touching the code.  And we get the
exact same results:

```{r eval=FALSE}
identical(gs, gso)
```
```
[1] TRUE
```

Mind blowing.  I remember peeking at the [most popular QA on Stackoverflow][3] a
few years ago and thinking "oh, that's not a problem I need to worry about as an
R user".  I'm tickled that I was wrong and I get to see this type of effect in
play first hand, although here there are more issues than just branch
prediction.

Two months ago I reached a fork in the road and I could have taken the "oh,
neat, let's move on and do something useful" one.  Instead I took the other one.
The one leading down the rabbit hole[^rabbit-hole].  And here I am, flat on my
back with a fat hydra sitting on my chest, licking its paws rather unconcerned
with my flailing efforts to cut off its remaining heads.

# The Memory System Sucks

<figure class='post-inset-image'>
<div class='post-inset-image-frame'><img
  id='front-img' 
  src='/post/2019-03-03-faster-group-stats-in-base-r_files/images/coal-train.jpg'
/></div>
<figcaption>Just one lump of coal please</figcaption>
</figure>

_Something_ is going on underneath the hood that dramatically affects the
performance of the random scenario. Over the decades CPU processing speed has
grown faster than memory latency has dropped.  Nowadays it may take ~200 CPU
cycles to have a CPU data request fulfilled from main memory.  Memory bandwidth
on the other hand has kept up.  The over-used analogy is that of a freight
train: it is slow to start moving and get to you (latency), but if you can have
it properly loaded it will bury you in goods once it arrives (bandwidth).

There are two classes of tricks memory systems use to mask the latency and
leverage the bandwidth.  The first one involves having some amount of low
latency memory known as cache.  This memory is very limited because it requires
a more expensive design _and_ it must be close to the CPU, typically on the
limited real estate of the CPU die:  

<figure class='aligncenter' style='max-width: 500px;'>
<img
  id='cpu-die' 
  src= "/post/2019-05-17-pixie-dust_files/user-imgs/cpu-die_westmere-6core_annotated2.jpeg"
/>
<figcaption>Half of a Westmere 6 Core Xeon Die</figcaption>
</figure>

We can see three levels of cache labeled L1, L2, and L3[^cache-labels], with the
first two inside the cores, and the last shared across all cores.  Each core
will have its own L1 and L2 cache, though we only label them within one core.
L1 cache is split between data cache (L1d) and instruction cache (L1i).

<span id=second-trick></span>The second latency concealing trick involves
interpreting instructions in such a way that the bandwidth "freight train" can
be fully loaded before it leaves the depot[^freight-train].  This includes
preloading memory areas the CPU is likely to need based on prior access
patterns, executing operations out of order so that memory requests for later
operations are initiated while earlier ones wait for theirs to be fulfilled,
guessing at which conditional branch will be executed before its condition is
known, and other optimizations.

In many cases the combination of these tricks is sufficient to conceal or at
least amortize the latency over many accesses, but there are pathological
operations that defeat them.  `split`ting the randomly ordered vector is an
example of such an operation.  [`treeprof`][22] gives us detailed timings for
the random and sorted `split` calls[^treeprof-manip]: <span
id=split-times></span>

```{r echo=FALSE}
string <- "times in milliseconds
                                         random  |      sorted
split --------------------------- : 7239 -    0  |  1573 -   0
    split.default --------------- : 7239 - \033[43m1997\033[m  |  1573 - \033[43m137\033[m
        as.factor --------------- : 5242 - 2471  |  1436 - 630
            sort ---------------- : 2771 -    0  |   806 -   0
                unique.default -- : 2705 - 2705  |   789 - 789
                sort.default ---- :   67 -    0  |    17 -   0
                    sort.int ---- :   67 -   27  |    17 -   7
                        order --- :   40 -   40  |     9 -   9"
writeLines(string)
```

There are many differences between the two runs, but let's focus on the
highlighted numbers to start.  These correspond to the internal C code that
scans through the input and processed group vectors[^proc-group] and copies each
input value into the result vector indicated by the group[^split-internal].  The
code will take the same exact number of steps irrespective of input order, yet
there is a ~15x speed-up between the sorted (137ms) and randomized versions
(1,997ms).

The following analogy may be helpful in understanding what is going on.  Imagine
we have been given a board full of tokens, each identified by one of eighteen
two letter combinations[^game-size].  We have also been given a box with
eighteen slots, each one labeled with one of those letter combinations.  Our job
is to transfer each token into the slot bearing its label:

```{r echo=FALSE, eval=FALSE}
strings <- sample(do.call(paste0, expand.grid(LETTERS, LETTERS)), 18)
strings.r <- strings.s <- matrix(sample(rep(strings, 8)), ncol=12)
strings.s[] <- sort(strings.s)
for(i in seq_len(nrow(strings.s))) cat(strings.s[i,], '\n')
for(i in seq_len(nrow(strings.r))) cat(strings.r[i,], '\n')
for(i in sample(unique(c(strings.s)))) cat(i, "[ ]\n")
```
```
              TOKENS                            SLOTS
              ------                            -----
CF DT EQ FD GE JO PF PL QS RM UA UE
CF DT EQ FD GE JO PF PL QS RM UA UE       PF->[  ]  DT->[  ]
CF DT EQ FD GE JO PF PL QS RM UA UE       QO->[  ]  KI->[  ]
CF DT EQ FD GE JO PF PL QS RM UA UE       RM->[  ]  GE->[  ]
CF DW EQ FM GE KI PF QO QS SQ UA UZ       SQ->[  ]  UZ->[  ]
CF DW EQ FM GE KI PF QO QS SQ UA UZ       UE->[  ]  QS->[  ]
CF DW EQ FM GE KI PF QO QS SQ UA UZ       FD->[  ]  FM->[  ]
CF DW EQ FM GE KI PF QO QS SQ UA UZ       PL->[  ]  CF->[  ]
DT DW FD FM JO KI PL QO RM SQ UE UZ       JO->[  ]  UA->[  ]
DT DW FD FM JO KI PL QO RM SQ UE UZ       DW->[  ]  EQ->[  ]
DT DW FD FM JO KI PL QO RM SQ UE UZ
DT DW FD FM JO KI PL QO RM SQ UE UZ
```

We pick up the first token and scan the slots until we find the one with the
"CF" label and drop the token in it.  The next token down is also labeled "CF".
This time, since we remember where we just dropped the last token we can repeat
the action for it and the subsequent six "CF" tokens.  Each time we hit a new
token label we will re-scan the slots.  In this analogy latency is the time it
takes to find a slot, and the bandwidth the rate at which we can transfer tokens
into the slot once we locate it.

Imagine instead if the board we had been given looked as follows:

```
              TOKENS                            SLOTS
              ------                            -----
KI DW CF EQ JO GE PF FM UE UA UE CF
SQ QS JO QS QS QO GE DT QS PL FD DT        PF->[  ]  DT->[  ]
CF RM QO SQ KI FD QS GE DW PF FD FD        QO->[  ]  KI->[  ]
UA SQ DW FM QO EQ SQ UE UE UZ SQ QO        RM->[  ]  GE->[  ]
RM UE FM JO GE FD KI PL UA UZ RM PL        SQ->[  ]  UZ->[  ]
QO PF JO PF GE CF UA UA DT QO GE DW        UE->[  ]  QS->[  ]
KI JO PL PL DW DT QO FD FM UZ DT RM        FD->[  ]  FM->[  ]
EQ UE SQ RM PL CF UZ EQ DT KI UZ FM        PL->[  ]  CF->[  ]
JO RM KI UZ QS UZ DT PL KI JO PL UA        JO->[  ]  UA->[  ]
FD FM QO EQ FM CF CF SQ DW RM QS FM        DW->[  ]  EQ->[  ]
DT CF UE PF KI DW FD GE UA EQ UE GE
UZ RM JO PF QS DW UA SQ PF PF EQ EQ
```

Whatever we do we'll be stuck re-scanning the slots with each
token[^scan-slots].  If our short term memory is good we might remember the
location for a few of the labels and transfer those without a scan, but unless
we can remember the location of every slot it will take us much longer than in
the sorted case.

Computers will also struggle with randomly order input for similar reasons.  The
`split` process is akin to our tokens and slots, except with ten million tokens
and ~one million slots.  Like us, computers have fast but limited short term
memory in the CPU cache, but the problem is too large for the cache so the high
latency main memory must be used.  There are additional complexities that defeat
the second class of optimizations (better packing of freight train) that we will 
discuss in the next section.

# Modelling Cache Effects

My system has a 3 level cache hierarchy as in the [pictured die](#cpu-die).
Each level is larger but slower than the previous one[^mem-latency]:

<!--
Some question whether 250 cycle for main memory is right.  Most quote 100ns,
which maybe with the processors 
-->

```{r fig.width=3, fig.height=4, out.width='45%', echo=FALSE, out.extra="style='display: inline;'"}
labels <- c('L1', 'L2', 'L3', 'Main')
# access.times <- c(3, 15, 40, 250)
access.times <- c(4, 14, 40, 250)
sizes <- c(2^15, 2^18, 2^21, 2^33)

library(ggplot2)
ggplot(data.frame(labels=labels, cycles=access.times), aes(labels, cycles)) +
  geom_col() + xlab(NULL) +
  geom_label(aes(label=access.times), vjust=-.25) +
  ylab('Access Time (CPU Cycles)') +
  scale_y_continuous(expand=expand_scale(mult=c(0.05, 0.15)))
ggplot(data.frame(labels=labels, sizes=sizes/2^20), aes(labels, sizes)) +
  geom_col(color='grey35') +
  geom_col() +
  ylab('Memory Size (MB)') + xlab(NULL) +
  geom_label(
    aes(
      label=c('32KB', '256KB', '2MB', '8GB^^'),
      y=c(head(sizes, -1), 290)
    ), vjust=-.25
  ) +
  # expand doesn't work with ylim
  # scale_y_continuous(expand=expand_scale(mult=c(0.05, 0.10)))
  coord_cartesian(ylim=c(0, 300*1.1/1.05))
  # coord_cartesian(ylim=c(0,2^4))
```

L1 cache is quite fast, but at 32KB[^l1-data-vs-inst], like our short term
memory, it is completely insufficient to hold any meaningfully large working
set.

Given the cache structure of my system we can we can build a model of memory
access time by working set size.  For reference we show our current working set
size as a vertical dashed line[^working-set-size]:

```{r echo=FALSE}
library(ggplot2)
library(ggbeeswarm)
library(reshape2)

cache.sizes <- c(15, 18, 22, log2(1e7 * (8 + 4/10)))
cache.lab <- c('L1d\n32KB', 'L2\n256KB', 'L3\n4MB', '10MM\nItems')
cache.styles <- c(rep('solid', 3), 'dashed')
test.sizes <- c(9:25)
mem.sizes <- c(L1=15, L2=18, L3=22, Main=33)
set.sizes.1 <- setNames(2^test.sizes * 8, test.sizes)
set.sizes.2 <- setNames(2^test.sizes * 4 / 10, test.sizes)

cache_times <- function(set.sizes, mem.sizes, access.times) {
  hit.rates <- outer(
    set.sizes, 2^mem.sizes, function(x, y) 1 - pmax(0, x - y) / x
  )
  times <- matrix(numeric(), nrow=nrow(hit.rates), ncol=ncol(hit.rates))
  mult <- rep(1, nrow(hit.rates))
  for(i in seq_len(ncol(hit.rates))) {
    times[,i] <- hit.rates[,i] * access.times[i] * mult
    mult <- mult * (1 - hit.rates[,i])
  }
  times
}
times1 <- cache_times(set.sizes.1, mem.sizes, access.times)
times2 <- cache_times(set.sizes.2, mem.sizes, access.times)
times <- rowSums(times1) + rowSums(times2)

mdl.dat <- data.frame(
  x=log2(rep(set.sizes.1 + set.sizes.2, 2)),
  y=log2(c(times + 4 * 4, rep(6 * 4, length(times)))),
  variable=rep(c('random', 'sorted'), each=length(times)),
  shape='Single'
)
  # actually data, here so we can use same ymax
base <- ('../../static/data/group-stat-base-split-times%s.RDS')
res1 <- readRDS(sprintf(base, ''))
res2 <- readRDS(sprintf(base, '-2'))
res3 <- readRDS(sprintf(base, '-3'))

res.all <- c(res1, res2, res3)
proc_times <- function(res) {
  colnames <- unique(names(res))
  res.mx <- matrix(res, ncol=4, dimnames=list(NULL, colnames), byrow=TRUE)
  mx.lens <- vapply(res.mx[,1], nrow, 0)
  resdf <- data.frame(
    random=unlist(lapply(res.mx[, 1], function(x) x[,3])),
    sorted=unlist(lapply(res.mx[, 2], function(x) x[,3])),
    n=rep(unlist(res.mx[, 3]), mx.lens),
    times=rep(unlist(res.mx[, 4]), mx.lens)
  )
  resdfm <- transform(
    melt(resdf, id.vars=c('n', 'times')),
    # time.per=ifelse(variable=='ratio', value, value/(n*times)),
    time.per=value/(n*times)/(1/1.2e9),
    shape='Single'
  )
  resdfm
}
resdfm <- proc_times(res.all)
ymax <- max(c(times, resdfm$time.per))
dat.label <- as.data.frame(
  c(
    values=lapply(log2(ymax), rep, each=length(cache.sizes)),
    list(size=unname(cache.sizes)),
    list(label=cache.lab), list(style=cache.styles)
) )

format_log_2 <- function(x) parse(text=paste0('2^', x))
format_mb <- function(x)
  ifelse(
    x >= 20,
    paste0(round(2^(x-20)), 'MB'),
    paste0(round(2^(x-10)), 'KB')
  )

format_cycles <- function(x) round(2^x)
clrs <- c(random='#cccc00', sorted='#33ee33', random2='#3333ee')
extra <- list(
  scale_y_continuous(label=format_cycles),
  scale_x_continuous(label=format_mb, expand=expand_scale(c(0.05, 0.10))),
  scale_shape_manual(values=c(Single=16, Median=18)),
  scale_color_manual(values=clrs),
  scale_fill_manual(values=clrs),
  ylab("CPU Cycles Per Item, log2"),
  xlab("Working Set Size, log2"),
  theme(legend.title=element_blank()) #legend.position='bottom',
)

ggplot(mdl.dat, aes(x, y)) +
  extra +
  geom_line(aes(color=variable)) +
  geom_text(
    data=dat.label,
    aes(
      x=size - .125, y=values, label=label, color=NULL, group=NULL,
      shape=NULL
    ),
    color='black', hjust=1
  ) +
  geom_vline(data=NULL, xintercept=cache.sizes, linetype=cache.styles) + NULL
  # coord_cartesian(ylim=c(4, 9))
```

`split.default` has six memory accesses[^split-ops].  When the working set fits
in L1 cache they are all done at L1 speeds, which explains why the chart starts
at 24 cycles per element in both cases[^access-dominates].  In the token/slots
analogy, a working set that fits in L1 is akin to a set of distinct labels small
enough that we can memorize the slot locations for each of them.

Of the six accesses four will always be sequential.  The other two are
sequential only for the sorted set.  We assume that all sequential accesses can
be met at L1 cache speeds without actually using L1 cache due to prefetching /
stream buffers[^streams].  This is one of those ["pack the freight train"
optimizations](#second-trick) where the memory system detects sequential
accesses and starts sending the likely-to-be-needed-data before it is actually
needed.  The first element is still subject to the full latency, but subsequent
elements will follow in quick succession.

Once we exceed L1 cache size with the random set, things slows down as an
increasing number of the non-sequential accesses must come from slower memory.
Each transition to a slower level of memory is manifests as an increase in
per-element time increase that asymptotically[^asymptotic-latency] approaches
the slower memory's incremental latency[^access-anomaly].

This simple model predicts the random inputs will be ~13 times slower to process
than the sorted ones for 10MM long inputs, which is close to the [ observed ~15
times in `split.default`](#split-times).  We tested different working set sizes:


```{r echo=FALSE, fig.height=5}
time_med <- function(x) {
  resmm <- tapply(x$time.per, x[c('variable', 'n')], median)
  resdfmm <- melt(resmm)
  names(resdfmm)[3] <- 'time.per'
  transform(
    resdfmm,
    # facet=ifelse(variable=='ratio', 'Time Ratios (random/sorted)', 'Time'),
    shape='Median'
  )
}
resdfmm <- time_med(resdfm)
# ymax <- stack(tapply(log2(resdfm$time.per), resdfm$facet, max))
ymax <- max(log2(resdfm$time.per))

ggplot(
  resdfm,
  aes(
    x=log2(n * (8 + 4/10)), y=log2(time.per),
    color=variable, shape=shape, fill=variable
  )
  #aes(x=log2(n * (8 + 4/10)), y=log2(time.per), color=variable, shape=shape)
) +
  geom_quasirandom(position='dodge', alpha=0.20) +
  extra +
  geom_text(
    data=dat.label,
    aes(
      x=size - .125, y=values, label=label, color=NULL, group=NULL,
      shape=NULL, fill=NULL
    ),
    color='black', hjust=1
  ) +
  geom_point(data=resdfmm, size=3) +
  geom_line(data=mdl.dat, aes(x, y)) +
  geom_vline(data=NULL, xintercept=cache.sizes, linetype=cache.styles)
  # facet_wrap(~facet, scales='free_y')
```

Outside of a few anomalies[^model-anomalies] the model works remarkably well.
This implies that none of the tricks other than cache are having a substantive
effect on performance.  It is also possible the reality is more complicated and
there are offsetting factors cancelling each other out[^model-caveat].

A possible reason why culprit is that the algorithm uses an offset vector to
track where in each group vector to write the next value and it is updated in
each iteration.  This introduces a dependency across iterations[^loop-dep],
which likely hamstrings out-of-order execution.



# Randomness Is Not Completely Pathological

,
which is surprising because computers use many tricks outside of sequential
access streams and cache to conceal latency, and our model does not account for
them.  These normally include things such as out-of-order / speculative
execution, parallel memory access, and other tricks. 
While this explains why `split` works so much faster with sorted inputs, we are
still left to deal with the unsorted inputs:

```{r eval=FALSE}
o <- order(grp)
go <- grp[o]
xo <- x[o]
split.default(xo, go)
```

Even ignoring the `order` call, the indexing steps required to order `x` and
`grp` into `xo` and `go` employ random accesses.  I ran the timings for `x[o]`
as well as for a version with `x` and `o` pre-sorted:

```{r echo=FALSE, fig.height=5}
test.sizes <- 12:30 - 3
set.sizes <- setNames(2^test.sizes * 8, test.sizes)

times <- rowSums(cache_times(set.sizes, mem.sizes, access.times))

mdl.dat <- data.frame(
  x=log2(rep(set.sizes, 2)),
  y=log2(c(times + 2*4, rep(3 * 4, length(times)))),
  variable=rep(c('random', 'sorted'), each=length(times)),
  shape='Single'
)
resdfm <- proc_times(readRDS(sprintf(base, '-4')))
ymax <- max(resdfm$time.per, 2^mdl.dat$y)
dat.label <- as.data.frame(
  c(
    values=lapply(log2(ymax), rep, each=length(cache.sizes)),
    list(size=unname(cache.sizes)),
    list(label=cache.lab), list(style=cache.styles)
) )
resdfmm <- time_med(resdfm)

ggplot(
  resdfm,
  aes(
    x=log2(n * 8), y=log2(time.per),
    color=variable, shape=shape, fill=variable
  )
  #aes(x=log2(n * (8 + 4/10)), y=log2(time.per), color=variable, shape=shape)
) +
  geom_quasirandom(position='dodge', alpha=0.20) +
  extra +
  geom_text(
    data=dat.label,
    aes(
      x=size - .125, y=values, label=label, color=NULL, group=NULL,
      shape=NULL, fill=NULL
    ),
    color='black', hjust=1
  ) +
  geom_line(data=mdl.dat, aes(x, y)) +
  geom_point(data=resdfmm, size=3) +
  geom_vline(data=NULL, xintercept=cache.sizes, linetype=cache.styles)
```

There are many interesting things going on, but the most notable one is that the
random case substantially outperforms our naive model.  Most likely this is
happening because of out-of-order execution and memory interleaving /
pipelining.  A pseudo code version of the [subset C code][31] might look like:

```{r eval=FALSE}
res <- numeric(length(grp))
for(i in seq_along(grp)) {
  idx <- o[i]        # fetch group offset
  val <- x[i]        # fetch x value
  res[idx] <- val    # write x value at offset
}
```

```{r echo=FALSE}
resdfm2 <- proc_times(readRDS(sprintf(base, '-7')))
resdfm3 <- rbind(
  resdfm, 
  transform(subset(resdfm2, variable=='random'), variable='random2')
)
ymax <- max(resdfm3$time.per, 2^mdl.dat$y)
dat.label <- as.data.frame(
  c(
    values=lapply(log2(ymax), rep, each=length(cache.sizes)),
    list(size=unname(cache.sizes)),
    list(label=cache.lab), list(style=cache.styles)
) )
resdfmm <- time_med(resdfm3)

ggplot(
  resdfm3,
  aes(
    x=log2(n * 8), y=log2(time.per),
    color=variable, shape=shape, fill=variable
  )
  #aes(x=log2(n * (8 + 4/10)), y=log2(time.per), color=variable, shape=shape)
) +
  geom_quasirandom(position='dodge', alpha=0.20) +
  extra +
  geom_text(
    data=dat.label,
    aes(
      x=size - .125, y=values, label=label, color=NULL, group=NULL,
      shape=NULL, fill=NULL
    ),
    color='black', hjust=1
  ) +
  geom_line(data=mdl.dat, aes(x, y)) +
  geom_point(data=subset(resdfmm, !is.na(time.per)), size=3) +
  geom_vline(data=NULL, xintercept=cache.sizes, linetype=cache.styles)
```

The slow step is `res[idx] <- val` as `idx` can point to any location in the
result vector and will not be met from cache at large enough working set sizes.
But the nice thing about this loop is that each iteration is independent of all
others.  This allows the processor to execute multiple loop cycles concurrently,
using cycles that would be idle waiting for the slow step to complete to process
subsequent iterations.  Even before the first result value is recorded there
will be multiple `res[idx] <- val` commands with different `idx` values en route
to main memory.  Since the accesses are random they will each likely point to
different memory banks, so requests can be interleaved so that idle banks can be
used without waiting on the busy ones.

As we saw previously, the latency of the first random access will still be the
full main memory latency, but we can amortize this cost by the lesser of how
many iterations processor cycles we can get ahead while waiting for the slow
step, and how many different memory banks our requests points to and how many of
them can be served simultaneously.  I have no way of knowing that this is what
is happening, but both out-of-order command execution and memory interleaving /
pipelining are well established techniques that can explain faster-than-memory
random accesses[^ooo-interleave].

So why did these optimizations not work in the `split.default` case?  Let's look
at the pseudo code:

```{r eval=FALSE}
# setup

grpi <- match(as.factor(grp), unique(sort(grp)))
offsets <- integer(length(levels(grpf)))
sizes <- integer(length(levels(grpf)))
for(i in grpi) sizes[i] <- sizes[i] + 1L
res <- lapply(sizes, numeric)

#actual work

for(i in seq_along(grpi)) {

  res[


}

offsets <- 
res <- numeric(length(grp))

```
The exact pattern of time per element versus working set size will vary
depending on memory system architecture and other factors.  Even though our
model appears to approximate real timings well, it may be a coincidence.

There are many factors that might affect memory access times that we are not
modeling.  Despite the complexity of memory systems there are a two points that
should be generally applicable:

1. Performance can vary greatly with work load size.
2. Performance can vary greatly with work load type.
    * Sequential accesses are likely to be highly optimized.
    * Random accesses will almost always be pathological.

by pipelining subsequent loop iterations into different
memory 

I have no way of knowing this is actually what is happening 

in
the random case we can keep up L2 speeds up to 4x the L2
cache size, and we can stay well below main memory latency for working sets up
to 1GB, the largest we tried.  Presumably the values of `o` are recognized as
the pointer offsets they are
index
access pattern is simple and common enough (read offsets from indexing vector,
fetch values using offsets) that the memory system can prefetch and hide a good
amount of the latency associated with the random accesses[^random-prefetch].
Do note that final YMMV: it is conceivable that some system are not able to
conceal access latency even in the simple random indexing case, in which case
ordering before the split may fail to improve or even degrade performance.

One final note: I clearly remember peeking at the [most popular QA on
Stackoverlfow][3] a few years ago and thinking "oh, that's not a problem I need
to worry about as an R user".  I'm kind of tickled that I was wrong and I got to
see these effects at play first hand.

<!-- Notes on Hash Tables in R - For Integers

Haven't thought too hard about distinctions between long vs not long

They are sized to be the closest power of 2 that is larger than twice the
length of the input vector.

https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/unique.c#L355

Do I have this right?  This is insane if that's the case, as we can just create
a vector that size and use the vector values as offsets?  Ah, no, because the
vector value could be e.g. 2^31 - 1 in a vector of length 1000.

One of the mindblowing things is that the hash table is self overlapping.  This
allows the hash table to be a single integer vector instead of a list of cons
cells.  It does lead to the odd situation that if you hit a hash code for the
first time that spot may be occupied from a previous overflow.

Split

1. Factor vector read in sequence?  No, we don't need this again.
2. Input numeric vector
3. Output numeric vector

For non-long vectors:

1. Read factor level     (int)
2. Read offset in group  (int)xx  Random seek in lvls long vector
3. Read input            (dbl)
4. Read list element     (ptr)xx
5. Read group element    (dbl)x   In most cases should be from after read above
6. Write group elment    (dbl)    Random seek in lvls long vec of vecs
7. Increment offset      (int).
8. Write offset          (int)    Should still be in cache

Vs.

1. Read offset
2. Read input            (dbl)xx
3. Write to output       (dbl)

   source value sequentially - this need not be cached
4. Write value           (double)*



-->




# Conclusions

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id='feedback-cont'></div>

# Appendix

## Function Definitions

```{r sys-time, echo=FALSE}
```

[^model-caveat]: I have no easy way of knowing that my inferences about what is
  happening are correct.  I'm relying exclusively on the simple model producing
  reasonably results, and common sense based on the information I have been able
  to gather.  Ideally I would be able to query performance counters to confirm
  that cache misses can indeed explain the observed timings, but those are
  hard to access on OS X, and even harder still for code that is run as part of
  R.  One element that is almost undoubtedly influencing performance is
  Translation Lookaside Buffer (TLB) misses, which we do not model at all.
[^mem-latency]: The latency figures are approximate.  In particular, the main
  memory latency may be incorrect.  For cache we use the numbers provided in
  [Agner Fog's "The microarchitecture of Intel, AMD and VIA CPUs"][57].  For L3
  we picked a seemingly reasonable number in the provided range.
[^streams]: [Nima Honarmand's "Memory Prefetching"][28] lecture strongly suggests
  data streams can be supported without polluting cache (p13,21-22), as does the
  [Paul Wicks "Miss Caches, Victim Caches and Stream Buffers" presentation][59]
  (p14-15), both of them based off of the Jouppi paper.  But the term "stream
  buffer" does not appear to be an Intel term, and the [Intel 64 and IA-32
  Optimization Reference Manual][58] is not super clear about the presence of
  such buffers that would support L1 access speeds (sections 2.1.5, 2.2.4,
  3.7.3, 7.2, 7.6.3), stating that prefetches go into L2.  However, my
  observations are consistent with streams being met at L1 speeds, although it
  could be due to other factors (e.g. accesses are met at L2 speeds, but
  multiple accesses are done simultaneously).
[^rabbit-hole]: Yes, this is a bit of a theme for me.
[^freight-train]: Perhaps a conveyor belt would be a better analogy, but that
  lacks the visual impact of the freight train.
[^treeprof-manip]: We manually juxtaposed the output of two `treeprof` runs.
[^split-ops]: We are including only the operations used to process the inputs
  in the main loop.  These are: read input value (1), read factor level (2),
  fetch target vector from result list (3), read offset for target vector (4),
  write value to target vector at offset (5), and increment offset (6).
[^working-set-size]: We use for working set size the portion of the data that
  cannot be accessed sequentially when the input is randomized.  This is the
  size of the result list of vectors which will be a little over 10MM elements
  each 8 bytes in size, and the offset tracking vector which will be ~1MM
  elements, each 4 bytes in size.  In reality there is more data involved, but
  because the rest of it are accessed sequentially, their impact on cache
  utilization is much lower.
[^model-anomalies]: The decreasing times per item for the smaller sets are
  likely a result of R function call overhead being amortized over an increasing
  number of entries, and the jump up for the largest working set size likely
  corresponds to that size defeating the normal latency-concealing optimizations
  used for sequential accesses.  One point of failure could be that some level
  of the [Translation Lookaside Buffer][29] that acts as cache for the
  translation of virtual to physical memory addresses is also exceeded and a
  full main memory table walk is required.
[^access-dominates]: There are a few other operations involved, such as
  allocating memory and incrementing offset counters, but these are a small
  portion of the overall time.
[^asymptotic-latency]: Keep in mind that the x-axis is log base 2, so each step
  corresponds to a doubling of the working set size.  After the first past a
  cache level transition, only half the working set can be accessed from the
  faster cache.  After two only a quarter, and so on.
[^access-anomaly]: One seeming anomaly is that there are only three transitions
  from faster to slower memory, yet there are four bumps in the plot.  This is
  because our working set contains two randomly accessed elements: the result
  list with the double precision vectors that correspond to each group, and an
  integer vector that tracks the offsets within each group that the next element
  will be written to.  The last bump corresponds to the point at which the
  offset integer vector outgrows L3 cache.  With ten elements pre group on
  average, we will need ~1 million elements in the offset vector to track each
  group with a 10MM working set size.  At 4 bytes an element, that makes for a
  ~4MB integer vector, which is the size of L3 cache.
[^ooo-interleave]: See this [University of Utah CS7810 presentation][32]
  for some discussion of the parallelism obtained from banks in DRAM memory, and
  the introduction to the [Wikipedia SDRAM page][33] which also mentions banks,
  interleaving, and pipelining.
[^proc-group]: Prior to this step the group vector is turned into integer group
  numbers that match the offset into the result list that the values of a group
  will be stored in.
[^split-internal]: The timing corresponds to the [`.Internal` call that
  invokes][29] the [C code][25] that does the allocation of the result vector
  and the copying, although the bulk of the time is spent copying .   The
  `.Internal` call does not show up in the R profile data used to build the
  `treeprof` output because `.Internal` is both "special" (i.e.
  `typeof(.Internal) == "special"`, see `?Rprof`) _and_ a primitive.  "special"
  functions are those that can access their arguments unevaluated.
  See [R Internals][23] for details.
[^loop-dep]: `split` tracks the offset at which the next value in a group will be
  written to, and increments that offset each time a value is written.  In other
  words, in order to know where in the result vector a value needs to go, the
  CPU needs to know how many times previously that group was written to, which
  is difficult to do without a level of insight into the code the CPU is
  unlikely to have.
[^game-size]: Eighteen is an arbitrary number of distinct elements picked to be
  large enough so that it is non trivial for humans to scan through them.
[^l1-data-vs-inst]: My system actually has 64KB of L1 cache, but it is split
  into 32KB of data cache (L1d), and 32KB of instruction cache (L1i).
[^time-in-cycles]: `$2^{-25.6}s per element / (1 / 1.2GHz) \approx 24 cycles per
  element$`.
[^sys-time]: It runs `system.time` eleven times and returns the median timing.
  It is [defined in the appendix](#sys.time).
[^cache-labels]: These are labeled on a best-efforts basis and may be incorrect.
[^scan-slots]: Alternatively, we could scan the tokens for a particular label.

[3]: https://stackoverflow.com/a/11227902/2725969
[21]: /2019/02/24/a-strategy-for-faster-group-statisitics/
[22]: https://github.com/brodieG/treeprof

<!-- 
Ulrich Drepper Paper, lots of goodies.

Table 2.2: DDR3 array/bus frequencies (933MHz for DDR3-1866)
Bottom pg 15: Latency numbers for Pentium M, including 240 cycles for main
memory.

Oddly doesn't really talk about interleaved access.
-->

[27]: https://people.freebsd.org/~lstewart/articles/cpumemory.pdf

<!--
Nima Honarmand Memory Prefetching, including discussion of stream buffers.
-->

[28]: https://compas.cs.stonybrook.edu/~nhonarmand/courses/sp16/cse502/slides/13-prefetch.pdf

[51]: https://en.wikipedia.org/wiki/Lernaean_Hydra
<!-- Original Hydra Link 
Larry Wentzel
https://www.flickr.com/photos/wentzelepsy/
Attribution-NonCommercial 2.0 Generic (CC BY-NC 2.0)
-->
[52]: https://www.flickr.com/photos/wentzelepsy/15431618539/in/photolist-pvD5yB-orMGkS-oD9tMr-6Hy4vZ-dHArXF-4MkiSq-fmXesZ-fnKybA-7PdCYt-nKGMUu-9DwyBF-4RGwKf-95vYq-4YHwip-69ckvt-FwgCQm-95vLz-6w9W6f-bVZLqM-95vWc-4YHsJc-4YJhhV-fnvhfn-9DzB8h-4YHxAB-9db7nP-fnvrrP-Ww91p9-8Yhwn3-4tLnpy-bsHuHt-4RGJSU-8nLdkA-4MkjF5-4tLnzC-5Zr3e9-fnKFf7-69gAij-5sR6jQ-9DwEzx-gfesaD-fnf39A-6w5JmX-69gz5o-5xYUVS-6MLZD9-5N2EcN-X463Df-G23wk-fmXqjH

<!-- 
Interesting review of microcode with details on actual execution
-->

[53]: https://pdziepak.github.io/2019/05/02/on-lists-cache-algorithms-and-microarchitecture/

<!--
The microarchitecture of Intel, AMD and VIA CPUs

Looks like a full on reference guide.  Has cache timings among other things, but
no memory access times?
-->
[56]: https://www.agner.org/optimize/microarchitecture.pdf

<!--
Westmere wiki page points out 32 entries of 2MB, which would match the cliff we
see at 64MB
-->

[57]: https://en.wikipedia.org/wiki/Westmere_(microarchitecture)

<!--
Intel optimization manual

Trying to use this to bridge the line-fill buff vs stream buffer.  Section 2.1.5
discusses this, although it isn't clear what the level of cache pollution is,
and how L1 and L2 interact since stuff seems to be prefetched to L2, not L1.
This also doesn't exactly match the Honarmand model since:

"The streamer and spatial prefetcher prefetch the data to the last level cache.
Typi-cally data is brought also to the L2 unless the L2 cache is heavily loaded
with missing demand requests" 

Figure 2-1 on page 42 does show the "Line Fill Buffers" to be between L2 and
L1d, and these are said to have 10 entries (cache line sized), and 20 for L2 (?).

Also, Table 2-9 on p58 has DTLB sizes for Sandy bridge (64 4KB entries, 32
2/4MB entries, 4 1 GB entries)
-->

[58]: https://www.intel.com/content/dam/doc/manual/64-ia-32-architectures-optimization-manual.pdf

<!--
Miss Caches, Victim Caches and Stream Buffers

Another course presentation discussing the Joupi paper, also suggests stream
buffers are there, real, and between L1 and L2
-->
[59]: https://cseweb.ucsd.edu/classes/fa11/cse240A-a/Slides1/04_Jouppi.pdf
