---
title: "Hydra Chronicles, Part I: Pixie Dust"
author: ~
date: '2019-05-17'
slug: pixie-dust
categories: [r]
tags: [optimization]
image: "/post/2019-05-15-pixiedust_files/user-images/cpu-die_westmere-6core_edited.jpg"
imagerect: "/post/2019-05-15-pixiedust_files/user-images/cpu-die_westmere-6core_small.jpg"
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: "How cache and other microarchitectural factors affect the
performance of R code, how the fast ordering contributed by data.table
mitigates potential problems, and opens up the possibility of faster
algorithms."
---

```{r echo=FALSE}
options(digits=3, crayon.enabled=TRUE)
suppressMessages(library(ggplot2))
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
```
```{r echo=FALSE, comment="", results='asis'}
old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)
```
```{r, echo=FALSE}
writeFansi <- function(x) {
  writeLines(
    paste0(
      "<pre></pre><pre><code>",
      paste0(fansi::sgr_to_html(x), collapse="\n"),
      "</code></pre>"
  ) )
}
```

More time that can be reasonably justified following the siren call of the
unexplained.

# Often the Road Least Travelled is so For Good Reasons

<!-- this needs to become a shortcode -->
<img
  id='front-img'
  src= "/post/2019-05-17-pixie-dust_files/user-imgs/hydra-white.png"
  class='post-inset-image'
/>

It wasn't supposed to be this way.  You shouldn't be looking at drawing of a
[Lernean Hydra][51] laughing its ass off.  This post was going to be short,
sweet, and useful.  Instead it turned into a many-headed monster that is smugly
sitting on my chest smothering all productivity out of me.  What follows here is
a re-telling of my flailing attempt at chopping off one its heads.

It all started on a lark to see if I could make base R competitive with the
undisputed champ `data.table` [at computing group statistics][21].  That thought
alone is scoff-worthy enough that I deserve little sympathy for what happened
next.  **Warning**: this is an optimization post in a blog about R, so please
set your expectations for riveting stories about adventures appropriately.

> TL;DR: ordering data can have a huge impact on how quickly it can be processed
> because of micro-architectural factors and because of better algorithms that
> become available.

Let's skip back to the care-free, hydra-less days when I should have known
better:

```{r echo=FALSE}
blogdown::shortcode("tweet", "1106231241488154626")
```

"More on this next week"... &#x1f923;&#x1f923;&#x1f923;.  Aaaah, what a fool.
That was two months ago.  The key observations back then were that:

* `data.table` contributed its radix sort to R in R3.3.0.
* As a result, `order` became 50x faster under at least some work loads.

I was also alluding that something fairly remarkable happens when you order data
before you process it.  Let's illustrate with 10MM items in ~1MM groups:

```{r warning=FALSE}
RNGversion("3.5.2"); set.seed(42)
n     <- 1e7
n.grp <- 1e6
grp   <- sample(n.grp, n, replace=TRUE)
x     <- runif(n)
```
```{r sys-time, echo=FALSE}
sys.time <- function(exp, reps=11) {
  res <- matrix(0, reps, 5)
  time.call <- quote(system.time({NULL}))
  time.call[[2]][[2]] <- substitute(exp)
  gc()
  for(i in seq_len(reps)) {
    res[i,] <- eval(time.call, parent.frame())
  }
  structure(res, class='proc_time2')
}
print.proc_time2 <- function(x, ...) {
  print(
    structure(
      x[order(x[,3]),][floor(nrow(x)/2),],
      names=c("user.self", "sys.self", "elapsed", "user.child", "sys.child"),
      class='proc_time'
) ) }
```

In my case I was splitting data to compute group statistics:

```{r eval=FALSE}
sys.time(gs <- split(x, grp))
```
```
   user  system elapsed
  5.643   0.056   5.704
```

`split` will split our vector `x` into a list of vectors as per `grp`, so the
following displays the first three split vectors:

```{r eval=FALSE}
str(head(gs, 3))
```
```
List of 3
 $ 1: num [1:11] 0.216 0.537 0.849 0.847 0.347 ...
 $ 2: num [1:7] 0.0724 0.1365 0.5364 0.0517 0.1887 ...
 $ 3: num [1:7] 0.842 0.323 0.325 0.49 0.342 ...
```

`sys.time` is a wrapper around `system.time`[^sys-time].   Compare to:

```{r eval=FALSE}
sys.time({
  o <- order(grp)
  xo <- x[o]
  go <- grp[o]
  gso <- split(xo, go)
})
```
```
   user  system elapsed
  1.973   0.060   2.054
```

Sorting values by group prior to running `split` makes it twice as fast,
**including the time required to sort the data**.  We just doubled the speed of
an important workhorse R function without touching the code.  And we get the
exact same results:

```{r eval=FALSE}
identical(gs, gso)
```
```
[1] TRUE
```

Mind blowing.  I remember peeking at the [most popular QA on Stackoverflow][3] a
few years ago and thinking "oh, that's not a problem I need to worry about as an
R user".  I'm tickled that I was wrong and I get to see this type of effect in
play first hand, although here there are more issues than just branch
prediction.

Two months ago I reached a fork in the road and I could have taken the "oh,
neat, let's move on and do something useful" one.  Instead I took the other one.
The one leading down the rabbit hole[^rabbit-hole].  And here I am, flat on my
back with a fat hydra sitting on my chest, licking its paws rather unconcerned
with my flailing efforts to cut off its remaining heads.

# Latency Is The Enemy

<figure class='post-inset-image'>
<div class='post-inset-image-frame'><img
  id='front-img'
  src='/post/2019-03-03-faster-group-stats-in-base-r_files/images/coal-train.jpg'
/></div>
<figcaption>Just one lump of coal please</figcaption>
</figure>

_Something_ is going on underneath the hood that dramatically affects the
performance of the random scenario. Over the decades CPU processing speed has
grown faster than memory latency has dropped.  Nowadays it may take ~200 CPU
cycles to have a CPU data request fulfilled from main memory.  Memory bandwidth
on the other hand has kept up.  The over-used analogy is that of a freight
train: it is slow to start moving and get to you (latency), but if you can have
it properly loaded it will bury you in goods once it arrives (bandwidth).

There are two classes of tricks memory systems use to mask the latency and
leverage the bandwidth.  The first one involves having some amount of low
latency memory known as cache.  This memory is very limited because it requires
a more expensive design _and_ it must be close to the CPU, typically on the
limited real estate of the CPU die:

<figure class='aligncenter' style='max-width: 500px;'>
<img
  id='cpu-die'
  src= "/post/2019-05-17-pixie-dust_files/user-imgs/cpu-die_westmere-6core_annotated2.jpeg"
/>
<figcaption>Half of a Westmere 6 Core Xeon Die</figcaption>
</figure>

We can see three levels of cache labeled L1, L2, and L3[^cache-labels], with the
first two inside the cores, and the last shared across all cores.  Each core
will have its own L1 and L2 cache, though we only label them within one core.
L1 cache is split between data cache (L1d) and instruction cache (L1i).

<span id=second-trick></span>The second latency concealing trick involves
interpreting instructions in such a way that the bandwidth "freight train" can
be fully loaded before it leaves the depot[^freight-train].  This includes
preloading memory areas the CPU is likely to need based on prior access
patterns, executing operations out-of-order so that memory requests for later
operations are initiated while earlier ones wait for theirs to be fulfilled,
guessing at which conditional branch will be executed before its condition is
known, and other optimizations.

In many cases the combination of these tricks is sufficient to conceal or at
least amortize the latency over many accesses, but there are pathological
operations that defeat them.  `split`ting the randomly ordered vector is an
example of such an operation.  [`treeprof`][22] gives us detailed timings for
the random and sorted `split` calls[^treeprof-manip]: <span
id=split-times></span>

```{r echo=FALSE}
string <- "times in milliseconds
                                         random  |      sorted
split --------------------------- : 7239 -    0  |  1573 -   0
    split.default --------------- : 7239 - \033[43m1997\033[m  |  1573 - \033[43m137\033[m
        as.factor --------------- : 5242 - 2471  |  1436 - 630
            sort ---------------- : 2771 -    0  |   806 -   0
                unique.default -- : 2705 - 2705  |   789 - 789
                sort.default ---- :   67 -    0  |    17 -   0
                    sort.int ---- :   67 -   27  |    17 -   7
                        order --- :   40 -   40  |     9 -   9"
writeLines(string)
```

There are many differences between the two runs, but let's focus on the
highlighted numbers to start.  These correspond to the internal C code that
scans through the input and processed group vectors[^proc-group] and copies each
input value into the result vector indicated by the group[^split-internal].  The
code will take the same exact number of steps irrespective of input order, yet
there is a ~15x speed-up between the sorted (137ms) and randomized versions
(1,997ms).

The following analogy may be helpful in understanding what is going on.  Imagine
we have been given a board full of tokens, each identified by one of eighteen
two letter combinations[^game-size].  We have also been given a box with
eighteen slots, each one labeled with one of those letter combinations.  Our job
is to transfer each token into the slot bearing its label:

```{r echo=FALSE, eval=FALSE}
strings <- sample(do.call(paste0, expand.grid(LETTERS, LETTERS)), 18)
strings.r <- strings.s <- matrix(sample(rep(strings, 8)), ncol=12)
strings.s[] <- sort(strings.s)
for(i in seq_len(nrow(strings.s))) cat(strings.s[i,], '\n')
for(i in seq_len(nrow(strings.r))) cat(strings.r[i,], '\n')
for(i in sample(unique(c(strings.s)))) cat(i, "[ ]\n")
```
```
              TOKENS                            SLOTS
              ------                            -----
CF DT EQ FD GE JO PF PL QS RM UA UE
CF DT EQ FD GE JO PF PL QS RM UA UE       PF->[  ]  DT->[  ]
CF DT EQ FD GE JO PF PL QS RM UA UE       QO->[  ]  KI->[  ]
CF DT EQ FD GE JO PF PL QS RM UA UE       RM->[  ]  GE->[  ]
CF DW EQ FM GE KI PF QO QS SQ UA UZ       SQ->[  ]  UZ->[  ]
CF DW EQ FM GE KI PF QO QS SQ UA UZ       UE->[  ]  QS->[  ]
CF DW EQ FM GE KI PF QO QS SQ UA UZ       FD->[  ]  FM->[  ]
CF DW EQ FM GE KI PF QO QS SQ UA UZ       PL->[  ]  CF->[  ]
DT DW FD FM JO KI PL QO RM SQ UE UZ       JO->[  ]  UA->[  ]
DT DW FD FM JO KI PL QO RM SQ UE UZ       DW->[  ]  EQ->[  ]
DT DW FD FM JO KI PL QO RM SQ UE UZ
DT DW FD FM JO KI PL QO RM SQ UE UZ
```

We pick up the first token and scan the slots until we find the one with the
"CF" label and drop the token in it.  The next token down is also labeled "CF".
This time, since we remember where we just dropped the last token we can repeat
the action for it and the subsequent six "CF" tokens.  Each time we hit a new
token label we will re-scan the slots.  In this analogy latency is the time it
takes to find a slot, and the bandwidth the rate at which we can transfer tokens
into the slot once we locate it.

Imagine instead if the board we had been given looked as follows:

```
              TOKENS                            SLOTS
              ------                            -----
KI DW CF EQ JO GE PF FM UE UA UE CF
SQ QS JO QS QS QO GE DT QS PL FD DT        PF->[  ]  DT->[  ]
CF RM QO SQ KI FD QS GE DW PF FD FD        QO->[  ]  KI->[  ]
UA SQ DW FM QO EQ SQ UE UE UZ SQ QO        RM->[  ]  GE->[  ]
RM UE FM JO GE FD KI PL UA UZ RM PL        SQ->[  ]  UZ->[  ]
QO PF JO PF GE CF UA UA DT QO GE DW        UE->[  ]  QS->[  ]
KI JO PL PL DW DT QO FD FM UZ DT RM        FD->[  ]  FM->[  ]
EQ UE SQ RM PL CF UZ EQ DT KI UZ FM        PL->[  ]  CF->[  ]
JO RM KI UZ QS UZ DT PL KI JO PL UA        JO->[  ]  UA->[  ]
FD FM QO EQ FM CF CF SQ DW RM QS FM        DW->[  ]  EQ->[  ]
DT CF UE PF KI DW FD GE UA EQ UE GE
UZ RM JO PF QS DW UA SQ PF PF EQ EQ
```

Whatever we do we'll be stuck re-scanning the slots with each
token[^scan-slots].  If our short term memory is good we might remember the
location for a few of the labels and transfer those without a scan, but unless
we can remember the location of every slot it will take us much longer than in
the sorted case.

Computers will also struggle with randomly order input for similar reasons.  The
`split` process is akin to our tokens and slots, except with ten million tokens
and ~one million slots.  Like us, computers have fast but limited short term
memory in the CPU cache, but the problem is too large for the cache so the high
latency main memory must be used.  There are additional complexities that defeat
the second class of optimizations (better packing of freight train) that we will
discuss in the next section.

# Modelling Cache Effects

My system has a 3 level cache hierarchy as in the [pictured die](#cpu-die).
Each level is larger but slower than the previous one[^mem-latency]:

<!--
Some question whether 250 cycle for main memory is right.  Most quote 100ns,
which maybe with the processors
-->

```{r fig.width=3, fig.height=4, out.width='45%', echo=FALSE, out.extra="style='display: inline;'"}
labels <- c('L1', 'L2', 'L3', 'Main')
 # access.times <- c(3, 15, 40, 250)
access.times <- c(4, 14, 40, 250)
sizes <- c(2^15, 2^18, 2^21, 2^33)

library(ggplot2)
ggplot(data.frame(labels=labels, cycles=access.times), aes(labels, cycles)) +
  geom_col() + xlab(NULL) +
  geom_label(aes(label=access.times), vjust=-.25) +
  ylab('Access Time (CPU Cycles)') +
  scale_y_continuous(expand=expand_scale(mult=c(0.05, 0.15)))
ggplot(data.frame(labels=labels, sizes=sizes/2^20), aes(labels, sizes)) +
  geom_col(color='grey35') +
  geom_col() +
  ylab('Memory Size (MB)') + xlab(NULL) +
  geom_label(
    aes(
      label=c('32KB', '256KB', '2MB', '8GB^^'),
      y=c(head(sizes, -1), 290)
    ), vjust=-.25
  ) +
  # expand doesn't work with ylim
  # scale_y_continuous(expand=expand_scale(mult=c(0.05, 0.10)))
  coord_cartesian(ylim=c(0, 300*1.1/1.05))
  # coord_cartesian(ylim=c(0,2^4))
```

L1 cache is quite fast, but at 32KB[^l1-data-vs-inst], like our short term
memory, it is completely insufficient to hold any meaningfully large working
set.

Given the cache structure of my system we can we can build a model of memory
access time by working set size.  For reference we show our current working set
size as a vertical dashed line[^working-set-size]:

```{r echo=FALSE}
library(ggplot2)
library(ggbeeswarm)
library(reshape2)

cache.sizes <- c(15, 18, 22, log2(1e7 * (8 + 4/10)))
cache.lab <- c('L1d\n32KB', 'L2\n256KB', 'L3\n4MB', '10MM\nItems')
cache.styles <- c(rep('solid', 3), 'dashed')
test.sizes <- c(9:25)
mem.sizes <- c(L1=15, L2=18, L3=22, Main=33)
set.sizes.1 <- setNames(2^test.sizes * 8, test.sizes)
set.sizes.2 <- setNames(2^test.sizes * 4 / 10, test.sizes)

cache_times <- function(set.sizes, mem.sizes, access.times) {
  hit.rates <- outer(
    set.sizes, 2^mem.sizes, function(x, y) 1 - pmax(0, x - y) / x
  )
  times <- matrix(numeric(), nrow=nrow(hit.rates), ncol=ncol(hit.rates))
  mult <- rep(1, nrow(hit.rates))
  for(i in seq_len(ncol(hit.rates))) {
    times[,i] <- hit.rates[,i] * access.times[i] * mult
    mult <- mult * (1 - hit.rates[,i])
  }
  times
}
times1 <- cache_times(set.sizes.1, mem.sizes, access.times)
times2 <- cache_times(set.sizes.2, mem.sizes, access.times)
times <- rowSums(times1) + rowSums(times2)

mdl.dat <- data.frame(
  x=log2(rep(set.sizes.1 + set.sizes.2, 2)),
  y=log2(c(times + 4 * 4, rep(6 * 4, length(times)))),
  variable=rep(c('random', 'sorted'), each=length(times)),
  shape='Single'
)
  # actually data, here so we can use same ymax
base <- ('../../static/data/group-stat-base-split-times%s.RDS')
res1 <- readRDS(sprintf(base, ''))
res2 <- readRDS(sprintf(base, '-2'))
res3 <- readRDS(sprintf(base, '-3'))

res.all <- c(res1, res2, res3)
proc_times <- function(res) {
  colnames <- unique(names(res))
  res.mx <- matrix(
    res, ncol=length(colnames), dimnames=list(NULL, colnames), byrow=TRUE
  )
  mx.lens <- vapply(res.mx[,1], nrow, 0)
  rep.cols <- !colnames %in% c('normal', 'sorted')
  rep.col.vals <- lapply(
    colnames[rep.cols],
    function(x) rep(unlist(res.mx[, x]), mx.lens)
  )
  resdf <- cbind(
    data.frame(
      random=unlist(lapply(res.mx[, 1], function(x) x[,3])),
      sorted=unlist(lapply(res.mx[, 2], function(x) x[,3]))
    ),
    setNames(rep.col.vals, colnames[rep.cols])
  )
  resdfm <- transform(
    melt(resdf, id.vars=colnames[rep.cols]),
    # time.per=ifelse(variable=='ratio', value, value/(n*times)),
    time.per=value/(n*times)/(1/1.2e9),
    shape='Single'
  )
  resdfm
}
resdfm <- proc_times(res.all)
ymax <- max(c(times, resdfm$time.per))
dat.label <- as.data.frame(
  c(
    values=lapply(log2(ymax), rep, each=length(cache.sizes)),
    list(size=unname(cache.sizes)),
    list(label=cache.lab), list(style=cache.styles)
) )

format_log_2 <- function(x) parse(text=paste0('2^', x))
format_mb <- function(x)
  ifelse(
    x >= 20,
    paste0(round(2^(x-20)), 'MB'),
    paste0(round(2^(x-10)), 'KB')
  )

format_cycles <- function(x) round(2^x)
clrs <- c(random='#cccc00', sorted='#33ee33', random2='#3333ee')
extra <- list(
  scale_y_continuous(label=format_cycles),
  scale_x_continuous(label=format_mb, expand=expand_scale(c(0.05, 0.10))),
  scale_shape_manual(values=c(Single=16, Median=18)),
  scale_color_manual(values=clrs),
  scale_fill_manual(values=clrs),
  ylab("CPU Cycles Per Item, log2"),
  xlab("Working Set Size, log2"),
  theme(legend.title=element_blank()) #legend.position='bottom',
)

ggplot(mdl.dat, aes(x, y)) +
  extra +
  geom_line(aes(color=variable)) +
  geom_text(
    data=dat.label,
    aes(
      x=size - .125, y=values, label=label, color=NULL, group=NULL,
      shape=NULL
    ),
    color='black', hjust=1
  ) +
  geom_vline(data=NULL, xintercept=cache.sizes, linetype=cache.styles) + NULL
  # coord_cartesian(ylim=c(4, 9))
```

`split.default` has six memory accesses[^split-ops].  When the working set fits
in L1 cache they are all done at L1 speeds, which explains why the chart starts
at 24 cycles per element in both cases[^access-dominates].  In the token/slots
analogy, a working set that fits in L1 is akin to a set of distinct labels small
enough that we can memorize the slot locations for each of them.

Of the six accesses four will always be sequential.  The other two are
sequential only for the sorted set.  We assume that all sequential accesses can
be met at L1 cache speeds without actually using L1 cache due to prefetching /
stream buffers[^streams].  This is one of those ["pack the freight train"
optimizations](#second-trick) where the memory system detects sequential
accesses and starts sending the likely-to-be-needed-data before it is actually
needed.  The first element is still subject to the full latency, but subsequent
elements will follow in quick succession.

Once we exceed L1 cache size with the random set, things slows down as an
increasing number of the non-sequential accesses must come from slower memory.
Each transition to a slower level of memory is manifests as an increase in
per-element time increase that asymptotically[^asymptotic-latency] approaches
the slower memory's incremental latency[^access-anomaly].

This simple model predicts the random inputs will be ~13 times slower to process
than the sorted ones for 10MM long inputs, which is close to the [ observed ~15
times in `split.default`](#split-times).  We tested different working set sizes:


```{r echo=FALSE, fig.height=5}
time_med <- function(x) {
  resmm <- tapply(x$time.per, x[c('variable', 'n')], median)
  resdfmm <- melt(resmm)
  names(resdfmm)[3] <- 'time.per'
  transform(
    resdfmm,
    # facet=ifelse(variable=='ratio', 'Time Ratios (random/sorted)', 'Time'),
    shape='Median'
  )
}
resdfmm <- time_med(resdfm)
 # ymax <- stack(tapply(log2(resdfm$time.per), resdfm$facet, max))
ymax <- max(log2(resdfm$time.per))

ggplot(
  resdfm,
  aes(
    x=log2(n * (8 + 4/10)), y=log2(time.per),
    color=variable, shape=shape, fill=variable
  )
  #aes(x=log2(n * (8 + 4/10)), y=log2(time.per), color=variable, shape=shape)
) +
  geom_quasirandom(position='dodge', alpha=0.20) +
  extra +
  geom_text(
    data=dat.label,
    aes(
      x=size - .125, y=values, label=label, color=NULL, group=NULL,
      shape=NULL, fill=NULL
    ),
    color='black', hjust=1
  ) +
  geom_point(data=resdfmm, size=3) +
  geom_line(data=mdl.dat, aes(x, y)) +
  geom_vline(data=NULL, xintercept=cache.sizes, linetype=cache.styles)
  # facet_wrap(~facet, scales='free_y')
```

Outside of a few anomalies[^model-anomalies] the model works remarkably well.
This implies that none of the tricks other than cache are having a substantive
effect on performance[^model-caveat].  A possible reason may be that the
splitting algorithm uses an offset vector to track where in each group vector to
write the next value.  This offset vector is updated at the end of each
iteration and is read from the next introducing a cross-iteration
dependency[^loop-dep], which likely hamstrings out-of-order execution.

# Degrees of Pathology

We now have a reasonable understanding for why `split` works much faster with
ordered than unordered input.  What this doesn't explain is why the combination
of ordering the inputs and splitting them is faster than splitting them
directly.  After all the ordering process itself involves random memory
accesses:

```{r eval=FALSE}
o <- order(grp)
go <- grp[o]            # random access
xo <- x[o]              # random access
split.default(xo, go)   # sequential
```

Even ignoring the `order` call, the indexing steps required to order `x` and
`grp` into `xo` and `go` employ random accesses.  I ran the timings for `x[o]`
as well as for a version with `x` and `o` pre-sorted:

```{r echo=FALSE, fig.height=5}
test.sizes <- 12:30 - 3
set.sizes <- setNames(2^test.sizes * 8, test.sizes)

times <- rowSums(cache_times(set.sizes, mem.sizes, access.times))

mdl.dat <- data.frame(
  x=log2(rep(set.sizes, 2)),
  y=log2(c(times + 2*4, rep(3 * 4, length(times)))),
  variable=rep(c('random', 'sorted'), each=length(times)),
  shape='Single'
)
resdfm <- proc_times(readRDS(sprintf(base, '-4')))
ymax <- max(resdfm$time.per, 2^mdl.dat$y)
cache.sizes.new <- c(head(cache.sizes, -1), log2(8*1e7))
dat.label <- as.data.frame(
  c(
    values=lapply(log2(ymax), rep, each=length(cache.sizes)),
    list(size=unname(cache.sizes.new)),
    list(label=cache.lab), list(style=cache.styles)
) )
resdfmm <- time_med(resdfm)

ggplot(
  resdfm,
  aes(
    x=log2(n * 8), y=log2(time.per),
    color=variable, shape=shape, fill=variable
  )
  #aes(x=log2(n * (8 + 4/10)), y=log2(time.per), color=variable, shape=shape)
) +
  geom_quasirandom(position='dodge', alpha=0.20) +
  extra +
  geom_text(
    data=dat.label,
    aes(
      x=size - .125, y=values, label=label, color=NULL, group=NULL,
      shape=NULL, fill=NULL
    ),
    color='black', hjust=1
  ) +
  geom_line(data=mdl.dat, aes(x, y)) +
  geom_point(data=resdfmm, size=3) +
  geom_vline(data=NULL, xintercept=cache.sizes.new, linetype=cache.styles)
```

There are many interesting things going on, but the most notable one is that the
random case substantially outperforms our naive cache-only model.  Most likely
this is happening because of out-of-order execution and parallelism in the
memory system. A pseudo code version of the [subset C code][31] might look like:
<span id=subset-pseudo></span>
```{r eval=FALSE}
res <- numeric(length(grp))
seqi <- seq_along(grp)
for(i in seqi) {
  val <- x[i]        # sequential - fetch x value to copy
  idx <- o[i]        # sequential - fetch write offset
  res[idx] <- val    # random - write x value at offset
}
```

The slow step is `res[idx] <- val` as `idx` can point to any location in the
result vector and will not be met from cache at large enough working set sizes.
But the nice thing about this loop is that each iteration is independent of all
others.  This allows the processor to begin execution of multiple loop
iterations before the first one completes.  Subsequent requests for writes at
random locations can be made as quickly as the fast sequential reads of offsets
from `o`.

As we saw previously, the latency of the first random access will still be the
full main memory latency, but we can amortize this cost by the lesser of how
many loop iterations we can put "in-flight" out-of-order , and the degree
of parallelism available in main memory[^mem-parallelism].

# Visualizing Out of Order Execution

Out-of-order instructions are kept "in-flight" in the reorder buffer
(ROB)[^rob], a memory structure that can store a number of both pending and
complete instructions.  The purpose of the buffer is to allow instructions to be
executed out-of-order, but have their results committed in logical program
order.

Let's pretend our system's ROB supports up to eight "instructions" "in-flight".
The first two iterations of the [previous `for` loop](#pseudo-random) are turned
into the following instructions:<span id=linear-ops></span>

```{r eval=FALSE}
## loop iteration 1
i1        <- seqi[1]               # PENDING
val1      <- x[i1]                 # PENDING
idx1      <- o[i1]                 # PENDING
res[idx1] <- val1                  # PENDING
## loop iteration 2
i2        <- seqi[2]               # PENDING
val2      <- x[i2]                 # PENDING
idx2      <- o[i2]                 # PENDING
res[idx2] <- val2                  # PENDING
```

Again we use some pseudo-R-code as a stand-in for what would actually be machine
micro-operations[^uops].  Notice how we renamed the `i`, `val`, and `idx`
variables by appending the loop number to each of them[^tomasulo].  This allows
the CPU to re-order them without instructions from later iterations overwriting
earlier ones.  For example, they could now be executed in the following order:

```{r eval=FALSE}
i1        <- seqi[1]               # PENDING
i2        <- seqi[2]               # PENDING

val1      <- x[i1]                 # PENDING
val2      <- x[i2]                 # PENDING

idx1      <- o[i1]                 # PENDING
idx2      <- o[i2]                 # PENDING

res[idx1] <- val1                  # PENDING
res[idx2] <- val2                  # PENDING
```

The processor should be free to chose any instruction to execute that does not
depend on others.  In this case both the `i1` and `i2` instructions are
eligible, so the processor could execute one, the other, or even both
simultaneously[^simulataneous-ops].  As a result shortly after the first main
memory access is complete this will be the state:

```{r eval=FALSE}
# i1      <- seqi[1]               # retired
i2        <- seqi[2]               # complete
i3        <- seqi[3]               # PENDING

val1      <- x[i1]                 # PENDING
val2      <- x[i2]                 # PENDING

idx1      <- o[i1]                 # PENDING
idx2      <- o[i2]                 # PENDING

res[idx1] <- val1                  # PENDING
res[idx2] <- val2                  # PENDING
```

`i1`, and `i2` are both computed.  Additionally, the `i1` instruction is retired,
meaning its result can be treated as final and it no longer occupies one of the
eight "in-flight" slots.  The `i2` instruction cannot be retired despite being
complete because in the logical flow of the program it comes after the `val1`,
`idx1`, and `res[idx1]` commands that are still pending.  The `i3` instruction
takes up the slot freed by the retirement of the `i1` instruction.

The same dynamic will play out for the `val*` and `idx*` steps, which will lead
to:

```{r eval=FALSE}
# i1      <- seqi[1]               # retired
# val1    <-   x[i1]               # retired
# idx1    <-   o[i1]               # retired

i2        <- seqi[2]               # complete
i3        <- seqi[3]               # complete

val2      <- x[i2]                 # complete
val3      <- x[i3]                 # complete

idx2      <- o[i2]                 # complete
idx3      <- o[i3]                 # complete

res[idx1] <- val1                  # PENDING
res[idx2] <- val2                  # PENDING
```

This is where the benefit of out-of-order execution becomes apparent.  All
memory accesses up to this were sequential, so executing out-of-order is
of marginal value as subsequent reads would be fast anyway.  But the
writes to `res` are random and so subject to the full memory latency.  Out of
order execution allows us to run them both concurrently taking advantage of the
memory system parallelism to satisfy them together in roughly the same amount of
time as it would take to run a single one[^mem-parallel-caveat].  [Without
out-of-order execution](#linear-ops) we would not be able to run the second
write to `res` until after the first one completed.  The effective bandwidth of
our system would continue to grow with the number of instructions we can keep
"in-flight" up until we saturate the parallelism available in main memory.

One way to confirm that out-of-order execution is indeed happening is to prevent
it with a cross-iteration loop dependency for an otherwise similar work load. We
can use "chasing indices" like this one:

<img
  src= "/post/2019-05-17-pixie-dust_files/user-imgs/index-chasing.png"
  class='aligncenter'
/>

Each vector element contains in it the (0-based) index of the next element to
visit.  The arrows illustrate the path that results from following the indices.
Unfortunately it isn't possible to engage in such index chasing efficiently in
pure R code, so instead we wrote a [C function to do it](#index-chasing).

We added the timings of that function on top of the prior runs, shown as the
blue `random2` data set:

```{r echo=FALSE}
resdfm2 <- proc_times(readRDS(sprintf(base, '-8')))
resdfm3 <- rbind(
  resdfm,
  transform(subset(resdfm2, variable=='random'), variable='random2')
)
ymax <- max(resdfm3$time.per, 2^mdl.dat$y)
dat.label <- as.data.frame(
  c(
    values=lapply(log2(ymax), rep, each=length(cache.sizes.new)),
    list(size=unname(cache.sizes.new)),
    list(label=cache.lab), list(style=cache.styles)
) )
resdfmm <- time_med(resdfm3)

ggplot(
  resdfm3,
  aes(
    x=log2(n * 8), y=log2(time.per),
    color=variable, shape=shape, fill=variable
  )
  #aes(x=log2(n * (8 + 4/10)), y=log2(time.per), color=variable, shape=shape)
) +
  geom_quasirandom(position='dodge', alpha=0.20) +
  extra +
  geom_text(
    data=dat.label,
    aes(
      x=size - .125, y=values, label=label, color=NULL, group=NULL,
      shape=NULL, fill=NULL
    ),
    color='black', hjust=1
  ) +
  geom_line(data=mdl.dat, aes(x, y)) +
  geom_point(data=subset(resdfmm, !is.na(time.per)), size=3) +
  geom_vline(data=NULL, xintercept=cache.sizes.new, linetype=cache.styles)
```

Sure enough, adding the sequential dependency as exists in `split.default`
brings timings much closer to the theoretical cache-only model, particularly
once the working set exceeds L3 cache size.  Random accesses are somewhat
pathological to modern memory systems, but those that depend on prior accesses
are downright killer.  Ordering our input vectors first isolates random accesses
to simple operations without sequential dependencies giving the memory
system the best chance to mitigate latency.

# Branch Prediction

One optimization we glossed over is branch prediction.  An out-of-order CPU
cannot put instructions "in-flight" past a branch (e.g. `if` statement) that
depends on incomplete previous instructions because it does not know which side
of the branch to put "in-flight".  Modern CPUs will make an educated guess and
keep putting instructions "in-flight" so we can maintain the benefits of
out-of-order execution in branched code.

Ironically the key feature of branch predicting CPUs is not the ability to
guess, but rather the ability to recover from an incorrect guess.  The reorder
buffer assists in this by marking all "in-flight" operations predicated on
a branch guess as speculative.  These may then be discarded if the guess is
discovered to be incorrect when the branch condition computation completes.
Mispredictions do have a cost (15-20 cycles on my system[^bp-penalty]), so CPU
designers invest a substantial amount of effort to improve branch prediction. 

Branch prediction likely only plays a minor role in our examples[^bp-minor];
however, for completeness sake I've contrived a new one that demonstrates its
effect.  In order to control the branching I wrote the example in C.  I also had
to compile it with all **optimizations turned off** (`-O0`) because compilers
are very aggressive at removing branches in code, precisely because they
interfere with out-of-order execution.  The function computes the cumulative sum
of a logical vector where FALSE is -1 and TRUE +1:

```{c eval=FALSE}
SEXP add_walk2(SEXP x) {
  R_xlen_t size = XLENGTH(x);
  int *x_ptr=INTEGER(x);
  int accum = 0;

  for(R_xlen_t i = 0; i < size; ++i) {
    if(*x_ptr > 0) {accum++;} else {accum--;}  // BRANCH
    ++x_ptr;
  }
  return ScalarInteger(accum);
}
```

We can then test this function with logical vectors with varying proportions of
TRUE values, both random and sorted:

```{r echo=FALSE, message=FALSE}
res2 <- readRDS(sprintf(base, '-12'))
resdfm <- proc_times(res2)
resdfm <- transform(resdfm, time.per=time.per/1e7)
fracs <- sort(unique(resdfm[['fraction']]))
min.time <- median(subset(resdfm, fraction == 0)[['time.per']])
mdldf <- rbind(
  data.frame(
    fraction=fracs, time.per=min.time + pmin(fracs, 1-fracs) * 15,
    variable='random'
  ),
  data.frame(fraction=fracs, time.per=min.time, variable='sorted')
)
ggplot(
  resdfm,
  aes(x=fraction, y=time.per, color=variable, fill=variable)
  #aes(x=log2(n * (8 + 4/10)), y=log2(time.per), color=variable, shape=shape)
) +
  geom_quasirandom(position='dodge', alpha=0.20) +
  geom_line(data=mdldf) +
  extra +
  scale_y_continuous(label=identity) +
  scale_x_continuous(label=identity) +
  xlab("Fraction TRUE") +
  ylab("CPU Cycles Per Item") +
  ggtitle("add_walk timings with 1e7 logical input")
```

The branch misprediction penalty models (lines) use 15 cycles penalty per
misprediction, which fits the observed timings well[^bp-penalty-variability].
In the randomly ordered case the branch predictor cannot do better than
predicting the higher frequency branch on average.  In the sorted case there is
essentially no misprediction penalty as predicting recently seen branch
directions is a good strategy.


```{r eval=FALSE}
o2 <- o
o2[sample(length(o), floor(length(o)/2))] <- length(o) + 1L
sys.time(x[o])
sys.time(x[o2])

x2 <- rep(NA_real_, length(x))
sys.time(x2[o] <- x)
sys.time(x2[o2] <- x)
```


# Loose Ends

The internal code in `split.default` had the largest proportional difference
between sorted and unsorted cases, but both `as.factor` and `unique.default`
together account for a larger part of the overall difference:

```{r echo=FALSE}
string <- "times in milliseconds
                                         random  |      sorted
split --------------------------- : 7239 -    0  |  1573 -   0
    split.default --------------- : 7239 - 1997  |  1573 - 137
        as.factor --------------- : 5242 - \033[43m2471\033[m    |  1436 - \033[43m630\033[m
            sort ---------------- : 2771 -    0  |   806 -   0
                unique.default -- : 2705 - \033[43m2705\033[m  |   789 - \033[43m789\033[m
                sort.default ---- :   67 -    0  |    17 -   0
                    sort.int ---- :   67 -   27  |    17 -   7
                        order --- :   40 -   40  |     9 -   9"
writeLines(string)
```

These are the steps that turn the groups into factor levels so they can be used
as offsets into the result vector for `split.default`.  Both of them rely on
[hash tables][2]: `unique` uses one to detect whether [a value is
duplicated][61], and [`as.factor` uses one][63] via [`match` to assign a unique
index][62] to each distinct value in its input.

Why is the sorting effect less pronounced with these?  Most likely because
hashing by its nature undoes some of the benefits of the sort.  In R, hash
tables are stored as vectors twice the size of the vector being
hashed[^hash-tables-in-r].  So in `unique.default(grp)`, we are generating a
hash table with **twenty million** elements, of which we will eventually use
~one million.  Worse, from a memory system perspective, the hashing function
will try to distribute the values evenly [and randomly][64] throughout the hash
vector.  Nothing like large memory allocations accessed randomly to make the
memory system happy.

Sorting the data still helps because for each group the same hash key will be
accessed repeatedly such that only the first access for each group is subjected
to the full main memory latency, other optimizations notwithstanding.




<!-- Notes on Hash Tables in R - For Integers

Haven't thought too hard about distinctions between long vs not long

They are sized to be the closest power of 2 that is larger than twice the
length of the input vector.

https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/unique.c#L355

Do I have this right?  This is insane if that's the case, as we can just create
a vector that size and use the vector values as offsets?  Ah, no, because the
vector value could be e.g. 2^31 - 1 in a vector of length 1000.

One of the mindblowing things is that the hash table is self overlapping.  This
allows the hash table to be a single integer vector instead of a list of cons
cells.  It does lead to the odd situation that if you hit a hash code for the
first time that spot may be occupied from a previous overflow.

Split

1. Factor vector read in sequence?  No, we don't need this again.
2. Input numeric vector
3. Output numeric vector

For non-long vectors:

1. Read factor level     (int)
2. Read offset in group  (int)xx  Random seek in lvls long vector
3. Read input            (dbl)
4. Read list element     (ptr)xx
5. Read group element    (dbl)x   In most cases should be from after read above
6. Write group elment    (dbl)    Random seek in lvls long vec of vecs
7. Increment offset      (int).
8. Write offset          (int)    Should still be in cache

Vs.

1. Read offset
2. Read input            (dbl)xx
3. Write to output       (dbl)

   source value sequentially - this need not be cached
4. Write value           (double)*



-->

# Better Algorithms

```{r eval=FALSE}
sys.time(u0 <- unique(grp))
```
```
   user  system elapsed
  1.197   0.039   1.244
```
```{r eval=FALSE}
sys.time({
  o <- order(grp)
  go <- grp[o]
})
```
```
   user  system elapsed
  0.439   0.002   0.443
```
```{r eval=FALSE}
sys.time(u1 <- unique(go))
```
```
   user  system elapsed
  0.349   0.039   0.389
```
```{r eval=FALSE}
sys.time({u2 <- go[c(TRUE, go[-1L] != go[length(go)])]})
```
```
   user  system elapsed
  0.129   0.011   0.141
```

# Conclusions

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id='feedback-cont'></div>

# Appendix

## Function Definitions

### sys.time

```{r sys-time, eval=FALSE}
```

### Index Chasing

Simulate a pointer chasing chain by generating indices that reference the next
index to go to.  We make the index a numeric rather than an integer just to
match working set sizes we used in other examples.

```{r eval=FALSE}
rand_path <- function(size) {
  if(!is.numeric(size) || length(size) != 1 || is.na(size) || size < 1)
    stop("bad input size")
  idx <- sample(as.integer(size - 1L)) + 1
  res <- numeric(size)
  res[c(1, head(idx, -1))] <- idx
  res[tail(idx, 1)] <- 1
  res - 1
}
```

We can then "walk" these indices with some C code.  We write effectively
meaningless but iteration dependent values to the result vector to try to avoid
any clever optimizations from the memory system.  This is not equivalent
to indexing a vector in random order, but the number of memory accesses should
be comparable and that is the limiting element.

```{r eval=FALSE}
walkrand <- inline::cfunction(
  sig=c(x='numeric', times='integer'),
  body="
  // make sure x only contains values in seq_along(x) - 1
  R_xlen_t len = XLENGTH(x);
  double total = 0;
  double *pointer = REAL(x);
  int jmax = asInteger(times);
  SEXP res;
  for(int j = 0; j < jmax; ++j) {
    res = PROTECT(allocVector(REALSXP, len));
    double *resvec = REAL(res);
    total = 0;
    R_xlen_t idx = 0;
    for(R_xlen_t i = 0; i < len; ++i) {
      idx = pointer[idx];
      total += idx;
      resvec[i] = total;
    }
    UNPROTECT(1);
  }
  return res;
")
```

## Benchmarking Code

### Split timings

```{r eval=FALSE}
times <- c(
   1e4, 1e4,  1e4,  5e3,   1e3,  1e3, 1e3,  1e2,  1e2,    50,   25,    25,
   10,    5,    3
)
reps <- c(
    11,  11,   11,   11,    11,   11,   5,    5,     5,    5,    5,     5,
     5,   5,    5
)
ns <- rev(ns)
times <- rev(times) * 2
reps <- rep(reps)

res <- vector('list', 0)
for(i in seq_along(ns)) {
  RNGversion("3.5.2"); set.seed(42)
  n <- ns[i]
  x <- runif(n)
  g <- sample(n/10, n, rep=TRUE)
  go <- sort(g)
  gfo <- as.factor(go)
  gf <- as.factor(g)

  writeLines(sprintf('------ %f -------', n))
  time.normal <-
    # sys.time(for(j in seq_len(times[i])) duplicated.default(g))
    sys.time(for(j in seq_len(times[i])) split.default(x, gf), reps=reps[i])
  time.sorted <-
    sys.time(for(j in seq_len(times[i])) split.default(x, gfo), reps=reps[i])
    # sys.time(for(j in seq_len(times[i])) duplicated.default(go))

  res <- append(
    res, list(
      normal=time.normal, sorted=time.sorted, n=n, times=times[i]
  ) )
  print(time.normal)
  print(time.sorted)
}

colnames <- unique(names(res))
resmx <- matrix(
  unname(res), ncol=length(colnames), dimnames=list(NULL, colnames),
  byrow=TRUE
)
```

### Random Walk

<!-- we used this one originally to confirm we were doing something similar to
indexing -->
```{r eval=FALSE, echo=FALSE}
scanrand <- inline::cfunction(
  sig=c(x='numeric', times='integer'),
  body="
  // make sure x only contains values of sample(length(x)) - 1L
  R_xlen_t len = XLENGTH(x);
  double *pointer = REAL(x);
  int jmax = asInteger(times);
  SEXP res;
  for(int j = 0; j < jmax; ++j) {
    res = PROTECT(allocVector(REALSXP, len));
    double *resvec = REAL(res);
    for(R_xlen_t i = 0; i < len; ++i) {
      resvec[i] = pointer[(int) pointer[i]];
    }
    UNPROTECT(1);
  }
  return res;
")
```
```{r eval=FALSE}
sizes <- 2^(10:27)
times <- pmax(2^24 / sizes, 1)
sizes.sub <- seq_along(sizes) # tail(seq_along(sizes), 3)
samps <- lapply(sizes[sizes.sub], rand_path)
y <- 11

res <- vector('list', 0)
for(i in seq_along(samps)) {
  idx <- sizes.sub[i]
  writeLines(sprintf('------ %f -------', sizes[idx]))
  time.normal <- sys.time(walkrand(samps[[i]], times[[idx]]), y)

  # subsequent processing code relied on having sorted vs unsorted data, so
  # that's why we write the same value twice
  res <- append(
    res, list(
      normal=time.normal, sorted=time.normal, n=sizes[[idx]], times=times[idx]
  ) )
  print(time.normal)
}
```

[^micro-ops]: The twelve operations we show here will likely correspond to
  substantially more micro-operations.
[^hash-tables-in-r]: Hash tables used to take up 4x the size of the
  hashed input, but in [2004 that was switched to 2x][65].  A tidbit I found
  fascinating is that the R hash tables are [open-addressed][67], meaning that
  collisions are resolved by walking along the hash vector until an open slot is
  found instead of appending an element to the end of a linked list or similar.
[^rob]: Assuming no instruction dependencies, CPUs that can engage in
  out-of-order execution can get ahead by as many micro-operations as will fit
  in the re-order buffer.  Re-order buffer sizes vary by architecture.  On my
  Skylake system the reorder buffer supports 224 micro-operations ([Agner
  Fog][57], p149).  Do note that while our pseudo-code loop appears to have
  three (four including the `for` index increment) instructions per iteration,
  in reality there will be far more micro-operations because the actual R code
  is [more][31] [complex][60] than the pseudo code would indicate.  The
  re-order buffer is a faster than cache memory location where pending and
  completed operations can wait to be retired in logical order.  Operations that
  complete ahead of others that logically precede them in the program will
  wait in the reorder buffer until those preceding operations complete.
[^mem-parallelism]: It is difficult to get information on exactly how many
  independently accessible  banks my system has, but it seems likely to be at
  least 16.  So long as the random accesses are from different banks they should
  all be addressable concurrently.  This [anandtech article][35] goes into some
  detail on memory banks.
[^model-caveat]: It is also possible the reality is more complicated and
  there are offsetting factors cancelling each other out.  I have no easy way of
  knowing that my inferences about what is happening are correct.  I'm relying
  exclusively on the simple model producing reasonably results, and common sense
  based on the information I have been able to gather.  Ideally I would be able
  to query performance counters to confirm that cache misses can indeed explain
  the observed timings, but those are hard to access on OS X, and even harder
  still for code that is run as part of R.  One element that is almost
  undoubtedly influencing performance is Translation Lookaside Buffer (TLB)
  misses, which I do not model at all.
[^mem-latency]: The latency figures are approximate.  In particular, the main
  memory latency may be incorrect.  For cache we use the numbers provided in
  [Agner Fog's "The microarchitecture of Intel, AMD and VIA CPUs"][57].  For L3
  we picked a seemingly reasonable number in the provided range.
[^streams]: [Nima Honarmand's "Memory Prefetching"][28] lecture strongly suggests
  data streams can be supported without polluting cache (p13,21-22), as does the
  [Paul Wicks "Miss Caches, Victim Caches and Stream Buffers" presentation][59]
  (p14-15), both of them based off of the Jouppi paper.  But the term "stream
  buffer" does not appear to be an Intel term, and the [Intel 64 and IA-32
  Optimization Reference Manual][58] is not super clear about the presence of
  such buffers that would support L1 access speeds (sections 2.1.5, 2.2.4,
  3.7.3, 7.2, 7.6.3), stating that prefetches go into L2.  However, my
  observations are consistent with streams being met at L1 speeds, although it
  could be due to other factors (e.g. accesses are met at L2 speeds, but
  multiple accesses are done simultaneously).
[^rabbit-hole]: Yes, this is a bit of a theme for me.
[^freight-train]: Perhaps a conveyor belt would be a better analogy, but that
  lacks the visual impact of the freight train.
[^treeprof-manip]: We manually juxtaposed the output of two `treeprof` runs.
[^split-ops]: We are including only the operations used to process the inputs
  in the main loop.  These are: read input value (1), read factor level (2),
  fetch target vector from result list (3), read offset for target vector (4),
  write value to target vector at offset (5), and increment offset (6).
[^working-set-size]: We use for working set size the portion of the data that
  cannot be accessed sequentially when the input is randomized.  This is the
  size of the result list of vectors which will be a little over 10MM elements
  each 8 bytes in size, and the offset tracking vector which will be ~1MM
  elements, each 4 bytes in size.  In reality there is more data involved, but
  because the rest of it are accessed sequentially, their impact on cache
  utilization is much lower.  Additionally, because the offset tracking vector
  is so much smaller than the main result list we assume that it will not
  substantively alter the eviction rate of the larger result set, and it will
  also not be evicted as any part of it is likely to have been accessed more
  recently than the result set.
[^access-dominates]: There are a few other operations involved, such as
  allocating memory and incrementing offset counters, but these are a small
  portion of the overall time.
[^asymptotic-latency]: Keep in mind that the x-axis is log base 2, so each step
  corresponds to a doubling of the working set size.  After the first step past
  a cache level transition, only half the working set can be accessed from the
  faster cache.  After two only a quarter, and so on.
[^model-anomalies]: Two notable Anomalies are (1) decreasing times per item for
  the smaller sets that are possibly a result of R function call overhead being
  amortized over an increasing number of entries, and (2) a jump up at largest
  working set sizes for sequential access that could correspond to the
  [Translation Lookaside Buffer][29] being exhausted.  This is a cache for
  translation of virtual to physical memory addresses.  The [Intel 64 and IA-32
  Optimization Reference Manual][58] notes in table 2-9 that there are 32 2/4MB
  DTLB entries, which would suggest a TLB walk after 64 or 128MB, around the
  point we see a timing anomaly.
[^access-anomaly]: There are only three transitions from faster to slower
  memory, yet there are four bumps in the plot.  This is because our working set
  contains two randomly accessed elements: the result list with the double
  precision vectors that correspond to each group, and an integer vector that
  tracks the offsets within each group that the next element will be written to.
  The last bump corresponds to the point at which the offset integer vector
  outgrows L3 cache.  With ten elements per group on average, we will need ~1
  million elements in the offset vector to track each group with a 10MM working
  set size.  At 4 bytes an element, that makes for a ~4MB integer vector, which
  is the size of L3 cache.
[^ooo-interleave]: See this [University of Utah CS7810 presentation][32]
  for some discussion of the parallelism obtained from banks in DRAM memory, and
  the introduction to the [Wikipedia SDRAM page][33] which also mentions banks,
  interleaving, and pipelining.
[^proc-group]: Prior to this step the group vector is turned into integer group
  numbers that match the offset into the result list that the values of a group
  will be stored in.
[^split-internal]: The timing corresponds to the [`.Internal` call that
  invokes][30] the [C code][25] that does the allocation of the result vector
  and the copying, although the bulk of the time is spent copying .   The
  `.Internal` call does not show up in the R profile data used to build the
  `treeprof` output because `.Internal` is both "special" (i.e.
  `typeof(.Internal) == "special"`, see `?Rprof`) _and_ a primitive.  "special"
  functions are those that can access their arguments unevaluated.
  See [R Internals][23] for details.
[^loop-dep]: `split` tracks the offset at which the next value in a group will be
  written to, and increments that offset each time a value is written.  In other
  words, in order to know where in the result vector a value needs to go, the
  CPU needs to know how many times previously that group was written to, which
  is difficult to do without a level of insight into the code the CPU is
  unlikely to have.
[^game-size]: Eighteen is an arbitrary number of distinct elements picked to be
  large enough so that it is non trivial for humans to scan through them.
[^l1-data-vs-inst]: My system actually has 64KB of L1 cache, but it is split
  into 32KB of data cache (L1d), and 32KB of instruction cache (L1i).
[^time-in-cycles]: `$2^{-25.6}s per element / (1 / 1.2GHz) \approx 24 cycles per
  element$`.
[^sys-time]: It runs `system.time` eleven times and returns the median timing.
  It is [defined in the appendix](#sys.time).
[^cache-labels]: These are labeled on a best-efforts basis and may be incorrect.
[^scan-slots]: Alternatively, we could scan the tokens for a particular label.

[2]: https://en.wikipedia.org/wiki/Hash_table
[3]: https://stackoverflow.com/a/11227902/2725969
[21]: /2019/02/24/a-strategy-for-faster-group-statisitics/
[22]: https://github.com/brodieG/treeprof
[23]: https://cran.r-project.org/doc/manuals/r-release/R-ints.html#Special-internals
[25]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/split-incl.c#L8

<!--
Ulrich Drepper Paper, lots of goodies.

Table 2.2: DDR3 array/bus frequencies (933MHz for DDR3-1866)
Bottom pg 15: Latency numbers for Pentium M, including 240 cycles for main
memory.

Oddly doesn't really talk about interleaved access.
-->

[27]: https://people.freebsd.org/~lstewart/articles/cpumemory.pdf

<!--
Nima Honarmand Memory Prefetching, including discussion of stream buffers.
-->

[28]: https://compas.cs.stonybrook.edu/~nhonarmand/courses/sp16/cse502/slides/13-prefetch.pdf

[29]: https://en.wikipedia.org/wiki/Translation_lookaside_buffer

[30]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/library/base/R/split.R#L31

<!-- subset C code -->

[31]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/subset.c#L72

<!-- Parallelism from Banks -->

[32]: https://pubweb.eng.utah.edu/~cs7810/pres/11-7810-12.pdf
[33]: https://en.wikipedia.org/wiki/Synchronous_dynamic_random-access_memory#Command_interactions

<!--
Good layperson layout of various factors affecting RAM performance,
including discussion of banks and ability to use them to saturate bandwidth

There is ambiguity about how many banks there are, and if they are confined to
one integrated circuit (IC) or not.  It sounds like not?  I.e bank 1 in IC 1 is
the same bank and bank 2 in IC2?  Same number of banks as ICs, so hard to
dissambiguate.

> It's important to understand that each page of memory is segmented evenly
> across Bank n of each IC for the associated rank.

> We can now see why the DDR3 core has a 8n-prefetch (where n refers to the
> number of banks per rank) as every read access to the memory requires a
> minimum of 64 bits (8 bytes) of data to be transferred. This is because each
> bank, of which there are eight for DDR3, fetches no less than 8 bits (1 byte)
> of data per read request - the equivalent of one column's worth of data.

-->

[35]: https://www.anandtech.com/show/3851/everything-you-always-wanted-to-know-about-sdram-memory-but-were-afraid-to-ask/2

[51]: https://en.wikipedia.org/wiki/Lernaean_Hydra
<!-- Original Hydra Link
Larry Wentzel
https://www.flickr.com/photos/wentzelepsy/
Attribution-NonCommercial 2.0 Generic (CC BY-NC 2.0)
-->
[52]: https://www.flickr.com/photos/wentzelepsy/15431618539/in/photolist-pvD5yB-orMGkS-oD9tMr-6Hy4vZ-dHArXF-4MkiSq-fmXesZ-fnKybA-7PdCYt-nKGMUu-9DwyBF-4RGwKf-95vYq-4YHwip-69ckvt-FwgCQm-95vLz-6w9W6f-bVZLqM-95vWc-4YHsJc-4YJhhV-fnvhfn-9DzB8h-4YHxAB-9db7nP-fnvrrP-Ww91p9-8Yhwn3-4tLnpy-bsHuHt-4RGJSU-8nLdkA-4MkjF5-4tLnzC-5Zr3e9-fnKFf7-69gAij-5sR6jQ-9DwEzx-gfesaD-fnf39A-6w5JmX-69gz5o-5xYUVS-6MLZD9-5N2EcN-X463Df-G23wk-fmXqjH

<!--
Interesting review of microcode with details on actual execution
-->

[53]: https://pdziepak.github.io/2019/05/02/on-lists-cache-algorithms-and-microarchitecture/

<!--
The microarchitecture of Intel, AMD and VIA CPUs

Looks like a full on reference guide.  Has cache timings among other things, but
no memory access times?
-->
[56]: https://www.agner.org/optimize/microarchitecture.pdf

<!--
Westmere wiki page points out 32 entries of 2MB, which would match the cliff we
see at 64MB
-->

[57]: https://en.wikipedia.org/wiki/Westmere_(microarchitecture)

<!--
Intel optimization manual

Trying to use this to bridge the line-fill buff vs stream buffer.  Section 2.1.5
discusses this, although it isn't clear what the level of cache pollution is,
and how L1 and L2 interact since stuff seems to be prefetched to L2, not L1.
This also doesn't exactly match the Honarmand model since:

"The streamer and spatial prefetcher prefetch the data to the last level cache.
Typi-cally data is brought also to the L2 unless the L2 cache is heavily loaded
with missing demand requests"

Figure 2-1 on page 42 does show the "Line Fill Buffers" to be between L2 and
L1d, and these are said to have 10 entries (cache line sized), and 20 for L2 (?).

Also, Table 2-9 on p58 has DTLB sizes for Sandy bridge (64 4KB entries, 32
2/4MB entries, 4 1 GB entries)
-->

[58]: https://www.intel.com/content/dam/doc/manual/64-ia-32-architectures-optimization-manual.pdf

<!--
Miss Caches, Victim Caches and Stream Buffers

Another course presentation discussing the Joupi paper, also suggests stream
buffers are there, real, and between L1 and L2
-->
[59]: https://cseweb.ucsd.edu/classes/fa11/cse240A-a/Slides1/04_Jouppi.pdf

<!-- subset C code, see 67 -->

[60]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/subset.c#L126
[66]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/subset.c#L74

<!--
Where duplicated and match initialize hash tables
-->

[61]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/unique.c#L545
[62]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/unique.c#L979
[63]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/library/base/R/factor.R#L80
[64]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/unique.c#L64
[65]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/unique.c#L336
[67]: https://en.wikipedia.org/wiki/Open_addressing

<!-- Intel Intro to X64 Assembly -->
[68]: https://software.intel.com/sites/default/files/m/d/4/1/d/8/Introduction_to_x64_Assembly.pdf

<!--
Dan Luu Pseudo-Transcript on branch prediction talk

Some useful stuff on pipelining.
Goes in depth into branch prediction techniques and effectiveness.
-->

[69]: https://danluu.com/branch-prediction/

<!-- List 10-20 cycles for branch misprediction -->

[70]: https://en.wikipedia.org/wiki/Branch_misprediction

<!--
John McCalpin who seems to now what he's talking about discusses concurrency
limits.

This is a very interesting thread.  Gets to the meat of how reorder buffer and
memory loads interact.

A few interesting points:

* On main mem latency:
    * 2.6GHZ has 85ns memory latency, which is ~220 cycles
    * For a single-socket system the latency may be as low as ~60 ns, so at
      3.0 GHz the latency would be about 180 cycles.
* L1d misses supported by line fill buffers, of which there are 10 on Sandy
  bridge, suspects same for skylake.
* If no miss there are way more buffers available (72 load, 42 store)
* To get more than 10 memory requests you need to go to L2 hardware prefetchers.
-->

[71]: https://software.intel.com/en-us/forums/software-tuning-performance-optimization-platform-monitoring/topic/595220

<!--
Anandtech articles showing haswell microarchitectural details
Looked at it trying to better understand relation between reservation station
and reorder buffer-->

[72]: https://www.anandtech.com/show/6355/intels-haswell-architecture/8
[73]: https://www.anandtech.com/show/9582/intel-skylake-mobile-desktop-launch-architecture-analysis/5

[74]: https://en.wikipedia.org/wiki/Re-order_buffer


