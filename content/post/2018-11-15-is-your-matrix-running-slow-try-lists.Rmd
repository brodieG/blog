---
title: Is Your Matrix Running Slow? Try Lists.
author: ~
date: '2018-11-15'
slug: is-your-matrix-running-slow-try-lists
draft: true
categories: [optimization]
tags: [rstats]
---

```{r echo=FALSE}
options(digits=3)
library(microbenchmark)
library(treeprof)
knitr::opts_chunk$set(
  comment = ""
)
```

# Matrices are Always Faster Than Data Frames, Right?

Generally speaking matrices are amongst the R data structures that are
fastest to operate on.  After all, they are contiguous chunks of memory with
almost no overhead.  Unfortunately, they do come with a major limitation: any
subset operation requires a copy of the data subset.  Consider:

```{r}
set.seed(1023)
n <- 1e6
x1 <- runif(n)
y1 <- runif(n)
x2 <- runif(n)
y2 <- runif(n)

M <- cbind(x1, y1, x2, y2)
D <- data.frame(x1, y1, x2, y2)
microbenchmark(M[,1:2], D[,1:2], times=25)
```

The data frame subset is much faster because R treats data frame columns as
independent objects, so referencing an entire column does not require a copy if
the data is not modified.

# Awesome, I'll Just Use Data Frames! Right?

If you have been in R circles for any length of time you will inevitably have
heard something I will loosely paraphrase as:

> blah blah blah data frames are EVIL blah blah

Whoever said that to you was not completely lying.  Let's see what happens if
we select rows and columns:

```{r}
idx <- sample(seq_len(nrow(M)), n/2)
microbenchmark(M[idx,1:2], D[idx,1:2], times=25)
```

So we cannot just switch to data frames blindly.  But what is going on?
We can easily check how time is used by our expression with
[treeprof](https://github.com/brodieG/treeprof):

```{r eval=FALSE}
treeprof(D[idx,1:2], target.time=1)
```
```{r echo=FALSE}
print(treeprof::treeprof(D[idx,1:2], target.time=1))
```

So `anyDuplicated` is taking a bunch of time, and while not obvious from the
profile data alone, this is because data frames do not like having duplicated
row names.  So let's introduce some duplicated row names to see what happens:

```{r eval=FALSE}
treeprof(D[c(idx,idx[1]), 1:2], target.time=1)
```
```{r echo=FALSE}
print(treeprof::treeprof(D[c(idx,idx[1]), 1:2], target.time=1))
```

Ouch!  The duplicate row name triggered the creation of *character* row names,
at great expense.  In return, you get:

```{r}
tail(rownames(D[c(idx,idx[1]), 1:2]))
```

Not. A. Good. Deal.

# Lists to the Rescue

With a little extra work you can do most of what you would normally do with a
matrix or data frame with a list.  It is not something I would want to do on a
regular basis, but it is an option to consider if you are looking to squeeze out
some extra performance without compiled code or external dependencies.

For example, to select "rows" in a list of vectors, we use `lapply` to apply `[`
to each component vector:

```{r}
L <- as.list(D)
str(L)
sub.L <- lapply(L[1:2], '[', idx)
str(sub.L)

identical(sub.L, as.list(D[idx,1:2]))
```

This results in the same elements selected as with data frame or matrix
subsetting, except the result remains a list.

The expression `lapply(L[1:2], '[', idx)` is equivalent to:

```{r}
cols <- 1:2
sub.L <- vector("list", length(cols))
for(i in cols) sub.L[[i]] <- L[[i]][idx]
```

Despite the explicit looping over columns this list based subsetting is
as fast or faster than matrix subsetting:

```{r}
microbenchmark(lapply(L[1:2], '[', idx), M[idx,1:2], D[idx,1:2], times=25)
```

And if we only index columns then the list is even faster than the data frame:

```{r}
microbenchmark(L[1:2], M[,1:2], D[,1:2], times=25)
```

# Patterns in List Based Analysis

## An Example: Euclidean Distances

Working with lists takes some extra work, but because R is a mostly functional
language with good list facilities, it can rewarding to find the list
equivalents of common matrix operations.  We do this here for the Euclidean
distance between two variables.

To compute the Euclidean distance between the 2D points in (x1,y1) and (x2,y2)
with matrices we use the square root of the sum of squares:

```{r echo=FALSE}
a <- c('x1','y1')
b <- c('x2','y2')
```
<a name='dist-calc'></a>
```{r dist-calc, eval=FALSE}
a <- c('x1','y1')
b <- c('x2','y2')
sqrt(rowSums((M[,a] - M[,b])^2))
```

This can be broken down into component pieces so we can compare each step of the
matrix calculation to the list equivalent.  First, the difference in the x and y
values:

```{r}
microbenchmark(times=25,
  Matrix= diff.M <- M[,a] - M[,b],
  List=   diff.L <- Map('-', L[a], L[b])
)
```

Here the list approach benefits from subsetting full columns.
Unfortunately, we need to explicitly cycle through the columns with `Map`, a
close cousin of `mapply`.  We use `Map` instead of `mapply` because `Map` always returns a list.  `Map` calls its first argument with each value in the
second and third arguments in turn.  It is equivalent to:

```{r eval=FALSE}
fun <- `-`
cols <- seq_along(a)
diff.L <- vector('list', length(cols))
for(i in cols) diff.L[[i]] <- fun(L[a][[i]], L[b][[i]])
```

To take the squares of each column we do so explicitly with `lapply`:

```{r}
microbenchmark(times=25,
  Matrix= diffsq.M <- diff.M ^ 2,
  List=   diffsq.L <- lapply(diff.L, '^', 2)
)
```

Unsurprisingly there is no speed benefit here, but there is also no penalty.
For the final step we can use `Reduce`:

```{r}
microbenchmark(times=25,
  Matrix= dist.M <- sqrt(rowSums(diffsq.M)),
  List=   dist.L <- sqrt(Reduce('+', diffsq.L))
)
```

`Reduce` collapses the elements of the input list by repeatedly applying a
binary function to turn two list elements into one.  To be completely honest I'm
rather surprised it is _faster_ than `rowSums`.  The speed advantage or
`Reduce` seems to persist up to about eight columns.  `Reduce` is equivalent to:

```{r eval=FALSE}
res <- numeric(length(diffsq.L[[1]]))
for(el in diffsq.L) res <- res + el
```

And to show that the calculations are the same:

```{r}
all.equal(dist.L, dist.M)
```

In exchange for the hassle of all the explicit looping through columns with
`Map`, `lapply`, and `Reduce` we get a noticeable performance gains:

```{r}
microbenchmark(times=25,
  Matrix= sqrt(rowSums((M[,a] - M[,b])^2)),
  List=   sqrt(Reduce('+', lapply(Map('-', L[a], L[b]), '^', 2)))
)
```

A ~2x speed-up from such an innocuous looking operation is
nothing to sneeze at.  Each step is at least as fast with lists as it is with
matrices, and some are faster.  This should be true so long as your data is tall
and skinny, i.e. with a high ratio of observations to variables, and you need to
subset for columns.

## List to Matrix and Back

R is awesome because it adopts some of the better features of functional
languages and list based languages.  For example, the following two statements
are equivalent:

```{r}
ML1 <- cbind(x1=L[[1]], y1=L[[2]], x2=L[[3]], y2=L[[4]])
ML2 <- do.call(cbind, L)
```

Proof:

```{r}
identical(ML1, ML2)
identical(ML1, M)
```

The blurring of calls, lists, data, and functions allows for very powerful
manipulations of list based data.  In this case, `do.call(cbind, LIST)` will
transform any list containing equal length vectors into a matrix, irrespective
of how many columns it has.To confirm `cbind` did what we claim:

To go in the other direction:

```{r}
LM <- lapply(seq_len(ncol(M)), function(x) M[,x])
identical(L, setNames(LM, colnames(M)))
```

You _could_ use `split` but it is much slower.

## More on `do.call`

List based analysis works particularly well with `do.call` and functions that
use `...` parameters:

```{r}
microbenchmark(times=25,
  do.call(pmin, L),
  pmin(M[,1],M[,2],M[,3],M[,4])
)
```

The expression is both easier to type, and faster!

There are a couple of "gotchas" with `do.call`, but they only crop up if you try
to pass quoted language as part of the argument list (e.g. `quote(a + b)`).  If
you are just using normal R data objects then the one drawback that I am
aware of is that the call stack can get messy if you ever invoke `traceback()`
or `sys.calls()` from a function called by `do.call`.

Remember that data frames are lists, so you can do things like:

```{r eval=FALSE}
do.call(pmin, iris[-5])
```

## List Matrices?

Let's revisit our earlier [distance calculation example](#dist-calc):

```{r dist-calc, eval=FALSE}
```

One problem with our matrix structure is that it does not allow us to flexibly
specify which dimensions or which points to select.  This is not too bad in the
example because there are only two variables and two dimensions, but with more
of either typing out the names or constructing them with string manipulation
would quickly become tedious.

Instead, we can use an array structure:

```{r}
A <- array(M, dim=c(nrow(M), ncol(M)/2, 2), dimnames=list(NULL, c('x','y')))
A[1:2,,]
```

The third dimension tracks which variable the x-y coordinates belong to.  The
distance calculation becomes:

```{r}
dist.A <- sqrt(rowSums((A[,,1] - A[,,2])^2))
all.equal(dist.A, dist.M)
```

But how do we benefit from this structural simplification with our list-based
methods?  Well, it turns out that lists can be matrices and arrays too:

```{r}
LM <- matrix(L, nrow=2, dimnames=list(c('x','y')))
LM
str(LM)
```
A chimera!  If you've never seen this it may seem a bit strange, but it allows
us to create array-like structures with lists:

```{r}
head(LM[['x', 1]])
head(A[,'x', 1])
```

We can use the same abstraction as with arrays, and keep performance advantage:

```{r}
dist.A <-  sqrt(rowSums(                  (A[,,1] - A[,,2])   ^   2))
dist.LM <- sqrt(Reduce('+', lapply(Map('-', LM[,1], LM[,2]), '^', 2)))
all.equal(dist.LM, dist.A)

microbenchmark(times=10,
  Array=sqrt(rowSums((A[,,1] - A[,,2])^2)),
  ListMatrix=sqrt(Reduce('+', lapply(Map('-', LM[,1], LM[,2]), '^', 2)))
)
```

# A Case Study

One of my side projects required me to implement [barycentric coordinate][1]
conversions.  You can think of barycentric coordinates of a point on the surface
of a triangle as the weights needed on each vertex such that the triangle
balances on that point.  We show here three different points (red crosses) with
the barycentric coordinates associated with each vertex.  The size of the vertex
helps visualize the "weights":


```{r bary, echo=FALSE}
## Conversion to Barycentric Coordinates; List-Matrix Version

bary_L <- function(p, v) {
  det <-  (v[[2,'y']]-v[[3,'y']])*(v[[1,'x']]-v[[3,'x']]) +
          (v[[3,'x']]-v[[2,'x']])*(v[[1,'y']]-v[[3,'y']])

  l1 <- (
          (v[[2,'y']]-v[[3,'y']])*(  p[['x']]-v[[3,'x']]) +
          (v[[3,'x']]-v[[2,'x']])*(  p[['y']]-v[[3,'y']])
        ) / det
  l2 <- (
          (v[[3,'y']]-v[[1,'y']])*(  p[['x']]-v[[3,'x']]) +
          (v[[1,'x']]-v[[3,'x']])*(  p[['y']]-v[[3,'y']])
        ) / det
  l3 <- 1 - l1 - l2
  list(l1, l2, l3)
}
## Conversion to Barycentric Coordinates; Array Version

bary_A <- function(p, v) {
  det <- (v[,2,'y']-v[,3,'y'])*(v[,1,'x']-v[,3,'x']) +
         (v[,3,'x']-v[,2,'x'])*(v[,1,'y']-v[,3,'y'])

  l1 <- (
          (v[,2,'y']-v[,3,'y']) * (p[,'x']-v[,3,'x']) +
          (v[,3,'x']-v[,2,'x']) * (p[,'y']-v[,3,'y'])
        ) / det
  l2 <- (
          (v[,3,'y']-v[,1,'y']) * (p[,'x']-v[,3,'x']) +
          (v[,1,'x']-v[,3,'x']) * (p[,'y']-v[,3,'y'])
        ) / det
  l3 <- 1 - l1 - l2
  cbind(l1, l2, l3)
}
```
```{r data-generation, echo=FALSE}
## Define basic triangle vertex position and colors

height <- sin(pi/3)
v.off <- (1 - height) / 2
vinit <- cbind(
  x=list(0, .5, 1), y=as.list(c(0, height, 0) + v.off),
  r=list(1, 1, 0), g=list(0,1,1), b=list(1,0,1)
)
## Sample points within the triangles

p1 <- list(x=.5, y=tan(pi/6) * .5 + v.off)
p2 <- list(x=.5, y=sin(pi/3) * .8 + v.off)
p3 <- list(x=cos(pi/6)*sin(pi/3), y=sin(pi/3) * .5 + v.off)

## Generate n x n sample points within triangle; here we also expand the
## vertex matrix so there is one row per point, even though all points are
## inside the same triangle.  In our actual use case, there are many
## triangles with a handful of points within each triangle.

n <- 400
rng.x <- range(unlist(vinit[,'x']))
rng.y <- range(unlist(vinit[,'y']))
points.raw <- expand.grid(
  x=seq(rng.x[1], rng.x[2], length.out=n),
  y=seq(rng.y[1], rng.y[2], length.out=n),
  KEEP.OUT.ATTRS=FALSE
)
vp <- lapply(vinit, '[', rep(1, nrow(points.raw)))
dim(vp) <- dim(vinit)
dimnames(vp) <- dimnames(vinit)

## we're going to drop the oob points for the sake of clarity, one
## nice perk of barycentric coordinates is that negative values indicate
## you are out of the triangle.

bc.raw <- bary_L(points.raw, vp)
inbounds <- Reduce('&', lapply(bc.raw, '>=', 0))

## Make a list-matrix version of the data

point.L <- lapply(points.raw, '[', inbounds)
vertex.L <- lapply(vp, '[', inbounds)
dim(vertex.L) <- dim(vp)
dimnames(vertex.L) <- list(V=sprintf("v%d", 1:3), Data=colnames(vp))

## Generate an array version of the same data

point.A <- do.call(cbind, point.L)
vertex.A <- array(
  unlist(vertex.L), c(sum(inbounds), nrow(vertex.L), ncol(vertex.L)),
  dimnames=c(list(NULL), dimnames(vertex.L))
)
```
```{r plot-utils, echo=FALSE}
plot_par <- function() {
  par(bg='#EEEEEE')
  par(xpd=TRUE)
  par(mai=c(.25,.125,.25,.125))
  par(mfrow = c(1, 3))
}
labeler <- function(x) sprintf("%.02f", unlist(x))
plot_bary <- function(p, v) {
  plot.new()
  bc <- bary_L(p, v)
  polygon(unlist(v[,'x']), unlist(v[,'y']))
  points(p, col='red', pch=4, cex=3)
  points(v[,'x'], v[,'y'], cex=unlist(bc) * 8, pch=19)
  text(v[,'x'], v[,'y'], labels=labeler(bc), pos=c(1,3,1), offset=1)
}
plot_bary2 <- function(p, v) {
  plot.new()
  bc <- bary_L(p, v)
  polygon(unlist(v[,'x']), unlist(v[,'y']))
  clrs <- lapply(apply(v[,c('r','g','b')], 2, Map, f='*', bc), Reduce, f='+')
  points(p, col='white', bg=do.call(rgb, clrs), pch=22, cex=3)
  points(
    v[,'x'], v[,'y'], cex=unlist(bc) * 8, pch=19,
    col=rgb(apply(v[,c('r','g','b')], 2, unlist))
  )
  text(v[,'x'], v[,'y'], labels=labeler(bc), pos=c(1,3,1), offset=1.2)
}
plot_shaded <- function(p, col, v=vinit) {
  par(bg='#EEEEEE')
  par(xpd=TRUE)
  par(mai=c(.25,.25,.25,.25))
  plot.new()

  points(p, pch=15, col=col, cex=.1)
  polygon(v[,c('x', 'y')])
  points(
    vinit[,c('x', 'y')], pch=21, cex=5,
    bg=rgb(apply(v[, c('r','g','b')], 2, unlist)), col='white'
  )
}
```
```{r echo=FALSE, fig.width=7, fig.height=2.5}
plot_par()
plot_bary(p1, vinit)
plot_bary(p2, vinit)
plot_bary(p3, vinit)
```

This is useful if we want to interpolate values from the vertices to points in
the triangle.  Here we interpolate a color for a point in a triangle from the
colors of the vertices of that triangle:

```{r echo=FALSE, fig.width=7, fig.height=2.5}
plot_par()
plot_bary2(p1, vinit)
plot_bary2(p2, vinit)
plot_bary2(p3, vinit)
```


The formula for converting cartesian coordinates [to Barycentric][1] is:

$$\lambda_1 = \frac{(y_2 - y_3)(x - x_3) + (x_3 - x_2)(y - y_3)}{(y_2 - y_3)(x_1 - x_3) + (x_3 - x_2)(y_1 - y_3)}\\
\lambda_2 = \frac{(y_3 - y_1)(x - x_3) + (x_1 - x_3)(y - y_3)}{(y_2 - y_3)(x_1 - x_3) + (x_3 - x_2)(y_1 - y_3)}\\
\lambda_3 = 1 - \lambda_1 - \lambda_2$$

You can see that whatever the data structure is, we will be using a lot of
column subsetting for this calculation.  We implement `bary_A` and `bary_L` to
convert cartesian coordinates to barycentric using array and list-matrix data
structures respectively.  Since both of these are a pretty naïve translations of
the above formulas, we relegate them to the [code
appendix](#barycentric-conversions).  We will use them to shade every point of
our triangle with the weighted color of the vertices.

First, the data:

```{r}
str(point.L)  # every point we want to compute color
vertex.L      # list-matrix!
```

The `vertex.L` list-matrix has the x, y coordinates and the red, green, and blue
color channel vales of the triangle associated with each point.  In this
particular case it happens to be the same triangle for every point, so we could
have used a single instance of the triangle.  However, the generalized is to
have many triangles with a small and varying number of points per triangle, and
to handle that efficiently in R we need to match each point to its
own triangle.

And the shading function:

```{r}
v_shade_L <- function(p, v) {
  ## 1) compute barycentric coords
  bc <- bary_L(p, v)
  ## 2) for each point, weight vertex colors by bary coords
  clr.raw <- apply(v[, c('r','g','b')], 2, Map, f="*", bc)
  ## 3) for each point-colorchannel, sum the weighted values
  lapply(clr.raw, Reduce, f="+")
}
```

Step 2) is the most complicated.  To understand what's going on we need to look
at the two data inputs to `apply`, which are:

```{r echo=FALSE}
v <- vertex.L
bc <- bary_L(point.L, vertex.L)
clr.raw <- apply(v[, c('r','g','b')], 2, Map, f="*", bc)
```
```{r}
v[, c('r','g','b')]
str(bc)
```
In:
```{r eval=FALSE}
  clr.raw <- apply(v[, c('r','g','b')], 2, Map, f="*", bc)
```
We ask `apply` to call `Map` for each column of `v[, c('r','g','b')]`.  The
additional arguments `f='*'` and `bc` are passed on to Map such that, for the
first column, the `Map` call is:

```{r eval=FALSE}
Map('*', v[,'r'], bc)
```

`v[,'r']` is a list with the red color channel values for each of the three
vertices for each of the points in our data:

```{r}
str(v[,'r'])
```

`bc` is a list of the barycentric coordinates of each point which we will use as
the vertex weights when averaging the vertex colors to point colors:

```{r}
str(bc)
```

The `Map` call is just multiplying each color value by its weight vertex weight.

`apply` will repeat this over the three color channels giving us the barycentric
coordinate-weighted color channel values for each point:

```{r}
str(clr.raw)
```

Step 3) collapse the weighted values into r,g,b triples for each point:

```{r}
  str(lapply(clr.raw, Reduce, f="+"))
```

And with the colors we can render our fully shaded triangle (see appendix for
`plot_shaded`):

```{r interp-list, fig.width=5, fig.height=5}
color.L <- v_shade_L(point.L, vertex.L)
plot_shaded(point.L, do.call(rgb, color.L))
```

The corresponding function in array format looks very similar to the list one:

```{r}
v_shade_A <- function(p, v) {
  ## 1) compute barycentric coords
  bc <- bary_A(p, v)
  ## 2) for each point, weight vertex colors by bary coords
  v.clrs <- v[,,c('r','g','b')]
  clr.raw <- array(bc, dim=dim(v.clrs)) * v.clrs
  ## 3) for each point-colorchannel, sum the weighted values
  rowSums(aperm(clr.raw, c(1,3,2)), dim=2)
}
str(point.A)
str(vertex.A)
color.A <- v_shade_A(point.A, vertex.A)
```

For a detailed description of what it is doing see the appendix.  The take-away
for now is that every function in the array implementation is fast in the sense
that all the real work is done in internal C code, and both functions produce
the same result:

```{r}
all.equal(do.call(cbind, color.L), color.A, check.attributes=FALSE)
```
Yet, the list based shading is substantially faster:

```{r}
microbenchmark(times=25,
  v_shade_L(point.L, vertex.L),
  v_shade_A(point.A, vertex.A)
)
```
```{r, eval=FALSE}
treeprof(v_shade_L(point.L, vertex.L))
treeprof(v_shade_A(point.A, vertex.A))
```
```{r, echo=FALSE}
print(treeprof(v_shade_L(point.L, vertex.L)))
print(treeprof(v_shade_A(point.A, vertex.A)))
```

# Conclusions



# Appendix

## Barycentric Conversions

```{r bary, eval=FALSE}
```

## Data

```{r data-generation, eval=FALSE}
```


```{r, echo=FALSE, eval=FALSE}
## Matrix multiplication
L_dotprod <- function(x, y) mapply(function(u, v) sum(L[[u]] * L[[v]]), x, y)
microbenchmark(times=5,
  prod_L <- outer(seq_along(L), seq_along(L), L_dotprod),
  prod_M <- t(M) %*% M
)
all.equal(prod_L, prod_M, check.attributes=FALSE)
```


[1]: https://en.wikipedia.org/wiki/Barycentric_coordinate_system#Barycentric_coordinates_on_triangles
