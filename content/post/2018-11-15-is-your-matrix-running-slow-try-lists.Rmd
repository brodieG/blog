---
title: Is Your Matrix Running Slow? Try Lists.
author: ~
date: '2018-11-15'
slug: is-your-matrix-running-slow-try-lists
draft: true
categories: [optimization]
tags: [rstats]
---

```{r echo=FALSE}
options(digits=3)
library(microbenchmark)
library(treeprof)
knitr::opts_chunk$set(
  comment = ""
)
```
```{r col-eq, echo=FALSE}
col_eq <- function(x, y) {
  if(is.matrix(x)) x <- lapply(seq_len(ncol(x)), function(z) x[, z])
  if(is.matrix(y)) y <- lapply(seq_len(ncol(y)), function(z) y[, z])
  # stopifnot(is.list(x), is.list(y))
  all.equal(x, y, check.attributes=FALSE)
}
```
```{r bary-l, echo=FALSE}
## Conversion to Barycentric Coordinates; List-Matrix Version

bary_L <- function(p, v) {
  det <-  (v[[2,'y']]-v[[3,'y']])*(v[[1,'x']]-v[[3,'x']]) +
          (v[[3,'x']]-v[[2,'x']])*(v[[1,'y']]-v[[3,'y']])

  l1 <- (
          (v[[2,'y']]-v[[3,'y']])*(  p[['x']]-v[[3,'x']]) +
          (v[[3,'x']]-v[[2,'x']])*(  p[['y']]-v[[3,'y']])
        ) / det
  l2 <- (
          (v[[3,'y']]-v[[1,'y']])*(  p[['x']]-v[[3,'x']]) +
          (v[[1,'x']]-v[[3,'x']])*(  p[['y']]-v[[3,'y']])
        ) / det
  l3 <- 1 - l1 - l2
  list(l1, l2, l3)
}
```
```{r bary-a, echo=FALSE}
## Conversion to Barycentric Coordinates; Array Version

bary_A <- function(p, v) {
  det <- (v[,2,'y']-v[,3,'y'])*(v[,1,'x']-v[,3,'x']) +
         (v[,3,'x']-v[,2,'x'])*(v[,1,'y']-v[,3,'y'])

  l1 <- (
          (v[,2,'y']-v[,3,'y']) * (p[,'x']-v[,3,'x']) +
          (v[,3,'x']-v[,2,'x']) * (p[,'y']-v[,3,'y'])
        ) / det
  l2 <- (
          (v[,3,'y']-v[,1,'y']) * (p[,'x']-v[,3,'x']) +
          (v[,1,'x']-v[,3,'x']) * (p[,'y']-v[,3,'y'])
        ) / det
  l3 <- 1 - l1 - l2
  cbind(l1, l2, l3)
}
```
```{r data-generation, echo=FALSE}
## Define basic triangle vertex position and colors

height <- sin(pi/3)
v.off <- (1 - height) / 2
vinit <- cbind(
  x=list(0, .5, 1), y=as.list(c(0, height, 0) + v.off),
  r=list(1, 1, 0), g=list(0,1,1), b=list(1,0,1)
)
## Sample points within the triangles

p1 <- list(x=.5, y=tan(pi/6) * .5 + v.off)
p2 <- list(x=.5, y=sin(pi/3) * .8 + v.off)
p3 <- list(x=cos(pi/6)*sin(pi/3), y=sin(pi/3) * .5 + v.off)

## Generate n x n sample points within triangle; here we also expand the
## vertex matrix so there is one row per point, even though all points are
## inside the same triangle.  In our actual use case, there are many
## triangles with a handful of points within each triangle.

n <- 400
rng.x <- range(unlist(vinit[,'x']))
rng.y <- range(unlist(vinit[,'y']))
points.raw <- expand.grid(
  x=seq(rng.x[1], rng.x[2], length.out=n),
  y=seq(rng.y[1], rng.y[2], length.out=n),
  KEEP.OUT.ATTRS=FALSE
)
vp <- lapply(vinit, '[', rep(1, nrow(points.raw)))
dim(vp) <- dim(vinit)
dimnames(vp) <- dimnames(vinit)

## we're going to drop the oob points for the sake of clarity, one
## nice perk of barycentric coordinates is that negative values indicate
## you are out of the triangle.

bc.raw <- bary_L(points.raw, vp)
inbounds <- Reduce('&', lapply(bc.raw, '>=', 0))

## Make a list-matrix version of the data

point.L <- lapply(points.raw, '[', inbounds)
vertex.L <- lapply(vp, '[', inbounds)
dim(vertex.L) <- dim(vp)
dimnames(vertex.L) <- list(V=sprintf("v%d", 1:3), Data=colnames(vp))

## Generate an array version of the same data

point.A <- do.call(cbind, point.L)
vertex.A <- array(
  unlist(vertex.L), c(sum(inbounds), nrow(vertex.L), ncol(vertex.L)),
  dimnames=c(list(NULL), dimnames(vertex.L))
)
```
```{r plot-utils, echo=FALSE}
plot_par <- function() {
  par(bg='#EEEEEE')
  par(xpd=TRUE)
  par(mai=c(.25,.125,.25,.125))
  par(mfrow = c(1, 3))
}
labeler <- function(x) sprintf("%.02f", unlist(x))
plot_bary <- function(p, v) {
  plot.new()
  bc <- bary_L(p, v)
  polygon(unlist(v[,'x']), unlist(v[,'y']))
  points(p, col='red', pch=4, cex=3)
  points(v[,'x'], v[,'y'], cex=unlist(bc) * 8, pch=19)
  text(v[,'x'], v[,'y'], labels=labeler(bc), pos=c(1,3,1), offset=1.3)
}
plot_bary2 <- function(p, v) {
  plot.new()
  bc <- bary_L(p, v)
  polygon(unlist(v[,'x']), unlist(v[,'y']))
  clrs <- lapply(apply(v[,c('r','g','b')], 2, Map, f='*', bc), Reduce, f='+')
  points(p, col='white', bg=do.call(rgb, clrs), pch=22, cex=3)
  points(
    v[,'x'], v[,'y'], cex=unlist(bc) * 8, pch=19,
    col=rgb(apply(v[,c('r','g','b')], 2, unlist))
  )
  text(v[,'x'], v[,'y'], labels=labeler(bc), pos=c(1,3,1), offset=1.3)
}
plot_shaded <- function(p, col, v=vinit) {
  par(bg='#EEEEEE')
  par(xpd=TRUE)
  par(mai=c(.25,.25,.25,.25))
  plot.new()

  points(p, pch=15, col=col, cex=.2)
  polygon(v[,c('x', 'y')])
  points(
    vinit[,c('x', 'y')], pch=21, cex=5,
    bg=rgb(apply(v[, c('r','g','b')], 2, unlist)), col='white'
  )
}
```
```{r v-shade-l, echo=FALSE}
v_shade_L <- function(p, v) {
  ## 1) compute barycentric coords
  bc <- bary_L(p, v)
  ## 2) for each point, weight vertex colors by bary coords
  clr.raw <- apply(v[, c('r','g','b')], 2, Map, f="*", bc)
  ## 3) for each point-colorchannel, sum the weighted values
  lapply(clr.raw, Reduce, f="+")
}
```
<div style='display: none;'>
```{r interp-list, echo=FALSE}
color.L <- v_shade_L(point.L, vertex.L)
plot_shaded(point.L, do.call(rgb, color.L))
```
</div>

# TL;DR

<img
src='/post/2018-11-15-is-your-matrix-running-slow-try-lists_files/figure-html/interp-list-1.png'
style='float: left; margin-right: 15px; margin-bottom: 5px;' width='200'/> I was
minding my own business bothering no one when I [fell into the rabbit
hole](https://www.brodieg.com/2018/10/23/do-not-shade-r/).  Now I'm just digging
a warren of tunnels through topics completely unrelated to the projects I'm
supposed to be working on, and only tangentially related to those that
distracted me in the first place.  As part of this undirected flailing I
discovered that shading triangle faces from vertex colors is done faster with
list data structures than with matrices and arrays.

Wait, what?  Simple primitive operations like addition and multiplication can
run faster on lists than on matrices?  The short answer is yes.  The long answer
is full of story-ruining caveats and boring benchmarks, and it is waiting for
you in all its gory glory.

# Matrices Are Slow?

## Column Subsetting

Generally speaking matrices are amongst the R data structures that are
fastest to operate on.  After all, they are contiguous chunks of memory with
almost no overhead.  Unfortunately, they do come with a major limitation: any
subset operation requires a copy of the data subset.  Consider:

```{r}
set.seed(1023)
n <- 1e5

M <- cbind(runif(n), runif(n), runif(n), runif(n))
D <- as.data.frame(M)
microbenchmark(M[,1:2], D[,1:2], times=25)
```

The data frame subset is much faster because R treats data frame columns as
independent objects, so referencing an entire column does not require a copy if
the data is not modified.

But data frames have big performance problems of their own.  Let's see what
happens if we select columns **and** rows:

```{r}
idx <- sample(seq_len(nrow(M)), n/2)
microbenchmark(M[idx,1:2], D[idx,1:2], times=25)
```

And its even worse if we have duplicate row indices:

```{r}
idx2 <- c(idx, idx[1])
microbenchmark(M[idx2,1:2], D[idx2,1:2], times=25)
```

This is all happening because data frames have row names, and as a result any
subset operation requires checking for duplicate row names, and creating new
unique character row names if there are duplicates.

## Lists to the Rescue

With a little extra work you can do most of what you would normally do with a
matrix or data frame with a list.  It is not something I would want to do on a
regular basis, but it is an option to consider if you are looking to squeeze out
some extra performance without compiled code or external dependencies.

For example, to select "rows" in a list of vectors, we use `lapply` to apply `[`
to each component vector:

```{r}
L <- as.list(D)
str(L)
sub.L <- lapply(L[1:2], '[', idx)
str(sub.L)

col_eq(sub.L, D[idx,1:2])  # col_eq compares values in columns only
```

This results in the same elements selected as with data frame or matrix
subsetting, except the result remains a list.  You can see how `col_eq` compares
objects in the [appendix](#col-eq).

Despite the explicit looping over columns this list based subsetting is
as fast or faster than matrix subsetting:

```{r}
microbenchmark(lapply(L[1:2], '[', idx), M[idx,1:2], D[idx,1:2], times=25)
```

And if we only index columns then the list is even faster than the data frame:

```{r}
microbenchmark(L[1:2], M[,1:2], D[,1:2], times=25)
```

# Patterns in List Based Analysis

## Row Subsetting

Select rows indexed by `idx` (we saw this earlier):

```{r}
microbenchmark(times=25,
  Matrix= M[idx,],
  List=   lapply(L, '[', idx)
)
```

The `lapply` expression is equivalent to:

```{r eval=FALSE}
result <- vector("list", length(L))
for(i in seq_along(L)) result[[i]] <- L[[i]][idx]
```

We use `lapply` to apply a function to each element of our list.  In our case,
each element is a vector, so we apply the function to each vector, one at a
time.

## Column Operations

Suppose we wanted to multiply all the columns in our data by two:

```{r}
microbenchmark(times=25,
  Matrix=  M * 2,
  List=    lapply(L, '*', 2)
)
```

The `lapply` call follows the same principle as with row subsetting.  We can use
any binary function this way.  For unary functions we just omit the additional
argument:

```{r eval=FALSE}
lapply(L, sqrt)
```

We can also extend this to column aggregating functions:

```{r}
microbenchmark(times=25,
  Matrix_sum=  colSums(M),
  List_sum=    lapply(L, sum),
  Matrix_min=  apply(M, 2, min),
  List_min=    lapply(L, min)
)
```

In the matrix case we have the special `colSums` function that does all the
calculations in internal C code.  Despite that it is no faster than the list
approach.  And if there is no special column function and we need to rely on
`apply` then the list approach is much faster.

If we want to use different scalars for each column things get a bit more
complicated, particularly for the matrix approach.  With the list we can use
`Map`:

```{r}
vec <- 2:5
microbenchmark(times=25,
  Matrix_1= t(t(M) * vec),
  Matrix_2= sapply(seq_along(vec), function(x) M[,x] * vec[x]),
  Matrix_3= M %*% diag(vec),
  List=     Map('*', L, vec)
)
col_eq(M %*% diag(vec), Map('*', L, vec))
```

Even our cleverest matrix implementation is slower than the list approach, with
the additional infirmities that it cannot be extended to other operators, and it
works incorrectly if the data contains infinite values.

`Map` is a close cousin of `mapply`. It calls its first argument, in
this case the function `*`, with one element each from the subsequent arguments,
looping through the argument elements.  It is equivalent to:

```{r eval=FALSE}
f <- `*`
result <- vector('list', length(cols))
for(i in seq_along(L)) result[[i]] <- f(L[[i]], vec[[i]])
```

We use `Map` instead of `mapply` because it is guaranteed to always return a
list, whereas `mapply` will simplify the result to a matrix.

`Map` also allows us to operate pairwise on columns.  Another way to compute the
square of our data might be:

```{r}
microbenchmark(times=25,
  Matrix= M * M,
  List=   Map('*', L, L)
)
col_eq(M * M, Map('*', L, L))
```

## Row-wise Operations

A classic row-wise operation is to sum values by row.  This is easily done with
`rowSums` for matrices.  For lists, we can use `Reduce` to collapse every column
into one:

```{r}
message('rowwise')
microbenchmark(times=25,
  Matrix= rowSums(M),
  List=   Reduce('+', L)
)
col_eq(rowSums(M), Reduce('+', L))
```

Much to my surprise `Reduce` is substantially faster, despite explicitly looping
through the columns.  From informal testing `Reduce` has an edge up to eight
columns or so.

`Reduce` collapses the elements of the input list by repeatedly applying a
binary function to turn two list elements into one.  `Reduce` is equivalent to:

```{r eval=FALSE}
result <- numeric(length(L[[1]]))
for(l in L) result <- result + l
```

An additional advantage of the `Reduce` approach is you can use any binary
function.

## List to Matrix and Back

R is awesome because it adopts some of the better features of functional
languages and list based languages.  For example, the following two statements
are equivalent:

```{r}
ML1 <- cbind(x1=L[[1]], y1=L[[2]], x2=L[[3]], y2=L[[4]])
ML2 <- do.call(cbind, L)
```

Proof:

```{r}
identical(ML1, ML2)
identical(ML1, M)
```

The blurring of calls, lists, data, and functions allows for very powerful
manipulations of list based data.  In this case, `do.call(cbind, LIST)` will
transform any list containing equal length vectors into a matrix, irrespective
of how many columns it has.To confirm `cbind` did what we claim:

To go in the other direction:

```{r}
LM <- lapply(seq_len(ncol(M)), function(x) M[,x])
identical(L, setNames(LM, colnames(M)))
```

You _could_ use `split` but it is much slower.

## More on `do.call`

List based analysis works particularly well with `do.call` and functions that
use `...` parameters:

```{r}
microbenchmark(times=25,
  List=   do.call(pmin, L),
  Matrix= pmin(M[,1],M[,2],M[,3],M[,4])
)
```

The expression is both easier to type, and faster!

There are a couple of "gotchas" with `do.call`, but they only crop up if you try
to pass quoted language as part of the argument list (e.g. `quote(a + b)`).  If
you are just using normal R data objects then the one drawback that I am
aware of is that the call stack can get messy if you ever invoke `traceback()`
or `sys.calls()` from a function called by `do.call`.

Remember that data frames are lists, so you can do things like:

```{r eval=FALSE}
do.call(pmin, iris[-5])
```

## High Dimensional Data

Matrices naturally extend to arrays in high dimensional data.  Imagine we wish
to track triangle vertex coordinates in 3D space.  We can easily set up an array
structure to do this:

```{r}
V <- runif(n * 3 * 3)
tri.A <- array(
  V, c(n, 3, 3),
  dimnames=list(NULL, vertex=paste0('v', 1:3), dim=c('x','y','z'))
)
tri.A[1:2,,]
```

How do we do this with lists?  Well, it turns out that you can make
list-matrices and list-arrays.  First we need to manipulate our data little as
we created for use with an array:

```{r}
MV <- matrix(V, nrow=n)
tri.L <- lapply(seq_len(9), function(x) MV[,x])
```

And now ... boom!

```{r}
dim(tri.L) <- tail(dim(tri.A), -1)
dimnames(tri.L) <- tail(dimnames(tri.A), -1)
tri.L
class(tri.L)
typeof(tri.L)
```

A chimera! `tri.L` is a list-matrix.  While this may seem strange if you haven't
come across one previously, the list matrix and 3D array are essentially
equivalent:

```{r}
head(tri.A[,'v1','x'])
head(tri.L[['v1','x']])
```

An example operation might be to find the mean value of each coordinate for each
vertex:

```{r}
colMeans(tri.A)
apply(tri.L, 1:2, do.call, what=mean)
```

We'll be using these in the next section.

# A Case Study

One of my side projects required me to implement [barycentric coordinate][1]
conversions.  You can think of barycentric coordinates of a point on the surface
of a triangle as the weights needed on each vertex such that the triangle
balances on that point.  We show here three different points (red crosses) with
the barycentric coordinates associated with each vertex.  The size of the vertex
helps visualize the "weights":

```{r echo=FALSE, fig.width=7, fig.height=2.5, fig.align='center'}
plot_par()
plot_bary(p1, vinit)
plot_bary(p2, vinit)
plot_bary(p3, vinit)
```

This is useful if we want to interpolate values from the vertices to points in
the triangle.  Here we interpolate a color for a point in a triangle from the
colors of the vertices of that triangle:

```{r echo=FALSE, fig.width=7, fig.height=2.5, fig.align='center'}
plot_par()
plot_bary2(p1, vinit)
plot_bary2(p2, vinit)
plot_bary2(p3, vinit)
```


The formula for converting cartesian coordinates [to Barycentric][1] is:

$$\lambda_1 = \frac{(y_2 - y_3)(x - x_3) + (x_3 - x_2)(y - y_3)}{(y_2 - y_3)(x_1 - x_3) + (x_3 - x_2)(y_1 - y_3)}\\
\lambda_2 = \frac{(y_3 - y_1)(x - x_3) + (x_1 - x_3)(y - y_3)}{(y_2 - y_3)(x_1 - x_3) + (x_3 - x_2)(y_1 - y_3)}\\
\lambda_3 = 1 - \lambda_1 - \lambda_2$$

You can see that whatever the data structure is, we will be using a lot of
column subsetting for this calculation.  We implement `bary_A` and `bary_L` to
convert cartesian coordinates to barycentric using array and list-matrix data
structures respectively.  Since both of these are a pretty naïve translations of
the above formulas, we relegate them to the [code
appendix](#barycentric-conversions).  We will use them to shade every point of
our triangle with the weighted color of the vertices.

First, the data:

```{r}
str(point.L)  # every point we want to compute color
vertex.L      # list-matrix!
```

The `vertex.L` list-matrix has the x, y coordinates and the red, green, and blue
color channel vales of the triangle associated with each point.  In this
particular case it happens to be the same triangle for every point, so we could
have used a single instance of the triangle.  However, the generalized is to
have many triangles with a small and varying number of points per triangle, and
to handle that efficiently in R we need to match each point to its
own triangle.

And the shading function:

```{r v-shade-l}
```

Step 2) is the most complicated.  To understand what's going on we need to look
at the two data inputs to `apply`, which are:

```{r echo=FALSE}
v <- vertex.L
bc <- bary_L(point.L, vertex.L)
clr.raw <- apply(v[, c('r','g','b')], 2, Map, f="*", bc)
```
```{r}
v[, c('r','g','b')]
str(bc)
```
In:
```{r eval=FALSE}
clr.raw <- apply(v[, c('r','g','b')], 2, Map, f="*", bc)
```
We ask `apply` to call `Map` for each column of `v[, c('r','g','b')]`.  The
additional arguments `f='*'` and `bc` are passed on to Map such that, for the
first column, the `Map` call is:

```{r eval=FALSE}
Map('*', v[,'r'], bc)
```

`v[,'r']` is a list with the red color channel values for each of the three
vertices for each of the points in our data:

```{r}
str(v[,'r'])
```

`bc` is a list of the barycentric coordinates of each point which we will use as
the vertex weights when averaging the vertex colors to point colors:

```{r}
str(bc)
```

The `Map` call is just multiplying each color value by its weight vertex weight.

`apply` will repeat this over the three color channels giving us the barycentric
coordinate-weighted color channel values for each point:

```{r}
str(clr.raw)
```

Step 3) collapse the weighted values into r,g,b triples for each point:

```{r}
str(lapply(clr.raw, Reduce, f="+"))
```

And with the colors we can render our fully shaded triangle (see appendix for
`plot_shaded`):

```{r interp-list, fig.width=5, fig.height=5, fig.align='center'}
```

The corresponding function in array format looks very similar to the list one:

```{r bary-matrix}
v_shade_A <- function(p, v) {
  ## 1) compute barycentric coords
  bc <- bary_A(p, v)
  ## 2) for each point, weight vertex colors by bary coords
  v.clrs <- v[,,c('r','g','b')]
  clr.raw <- array(bc, dim=dim(v.clrs)) * v.clrs
  ## 3) for each point-colorchannel, sum the weighted values
  rowSums(aperm(clr.raw, c(1,3,2)), dim=2)
}
```

For a detailed description of what it is doing [see the
appendix](array-shading).  The take-away for now is that every function in the
array implementation is fast in the sense that all the real work is done in
internal C code.

The data are also similar, although they are in matrix and array format instead
of list and matrix-list:

```{r}
str(point.A)
str(vertex.A)
```

Both shading implementations produce the same result:

```{r}
color.A <- v_shade_A(point.A, vertex.A)
all.equal(do.call(cbind, color.L), color.A, check.attributes=FALSE)
```

Yet, the list based shading is substantially faster:

```{r}
microbenchmark(times=25,
  v_shade_L(point.L, vertex.L),
  v_shade_A(point.A, vertex.A)
)
```

We can get a better sense of where the speed differences are coming from by
looking at the profile data using [`treeprof`][2]:

```{r, eval=FALSE}
treeprof(v_shade_L(point.L, vertex.L))
```
```{r, echo=FALSE}
print(treeprof(v_shade_L(point.L, vertex.L), target.time=1))
```
```{r, eval=FALSE}
treeprof(v_shade_A(point.A, vertex.A))
```
```{r, echo=FALSE}
print(treeprof(v_shade_A(point.A, vertex.A), target.time=1))
```

Most of the time difference is in the [`bary_L`](#bary_l) vs [`bary_A`](#bary_a)
functions that compute the barycentric coordinates.  You can also tell that
because almost all the time in the `bary_*` functions is "self" time (the second
column in the profile data) that this time is all spent doing primitive /
internal operations.

Step 2) takes about the same amount of time for both the list-matrix and array
methods (the `apply` call for `v_shade_L` and the `array` call for `v_shade_A`,
but step 3) is also much faster for the list approach because with the
array approach we need to permute the dimensions so we can use `rowSums`.

# Conclusions

If your data is tall and skinny, list data structures may allow you to extract
substantial additional performance out of base R code.  This will come at the
cost of some convolution in the code, but with a little care it can be justified
for code written once and used many times.

# Appendix

## Column Equality

<a name='col_eq'></a>
```{r col-eq, eval=FALSE}
```

## Barycentric Conversions

<a name='bary_l'></a>
```{r bary-l, eval=FALSE}
```
<a name='bary_a'></a>
```{r bary-a, eval=FALSE}
```

## Data

```{r data-generation, eval=FALSE}
```

## Matrix Barycentric Calculation


[1]: https://en.wikipedia.org/wiki/Barycentric_coordinate_system#Barycentric_coordinates_on_triangles
[2]: https://github.com/brodieG/treeprof
