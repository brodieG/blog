---
title: Is Your Matrix Running Slow? Try Lists.
author: ~
date: '2018-11-15'
slug: is-your-matrix-running-slow-try-lists
draft: true
categories: [optimization]
tags: [rstats]
---

```{r echo=FALSE}
options(digits=3)
library(microbenchmark)
knitr::opts_chunk$set(
  comment = ""
)
```

# Matrices are Always Faster Than Data Frames, Right?

Generally speaking matrices are amongst the R data structures that are
fastest to operate on.  After all, they are contiguous chunks of memory with
almost no overhead.  Unfortunately, they do come with a major limitation: any
subset operation requires a copy of the data subset.  Consider:

```{r}
set.seed(1023)
n <- 1e5
x1 <- runif(n)
y1 <- runif(n)
x2 <- runif(n)
y2 <- runif(n)

M <- cbind(x1, y1, x2, y2)
D <- data.frame(x1, y1, x2, y2)
microbenchmark(M[,1:2], D[,1:2], times=25)
```

The data frame subset is much faster because R treats data frame columns as
independent objects, so referencing an entire column does not require a copy if
the data is not modified.

# Awesome, I'll Just Use Data Frames! Right?

If you have been in R circles for any length of time you will inevitably have
heard something I will loosely paraphrase as:

> blah blah blah data frames are EVIL blah blah

Whoever said that to you was not completely lying.  Let's see what happens if
we select rows and columns:

```{r}
idx <- sample(seq_len(nrow(M)), n/2)
microbenchmark(M[idx,1:2], D[idx,1:2], times=25)
```

So we cannot just switch to data frames blindly.  But what is going on?
We can easily check how time is used by our expression with
[treeprof](https://github.com/brodieG/treeprof):

```{r eval=FALSE}
treeprof::treeprof(D[idx,1:2], target.time=1)
```
```{r echo=FALSE}
print(treeprof::treeprof(D[idx,1:2], target.time=1))
```

So `anyDuplicated` is taking a bunch of time, and while not obvious from the
profile data alone, this is because data frames do not like having duplicated
row names.  So let's introduce some duplicated row names to see what happens:

```{r eval=FALSE}
treeprof::treeprof(D[c(idx,idx[1]), 1:2], target.time=1)
```
```{r echo=FALSE}
print(treeprof::treeprof(D[c(idx,idx[1]), 1:2], target.time=1))
```

Ouch!  The duplicate row name triggered the creation of *character* row names,
at great expense.  In return, you get:

```{r}
tail(rownames(D[c(idx,idx[1]), 1:2]))
```

Not. A. Good. Deal.

# Lists to the Rescue

With a little extra work you can do most of what you would normally do with a
matrix or data frame with a list.  It is not something I would want to do on a
regular basis, but it is an option to consider if you are looking to squeeze out
some extra performance without compiled code or external dependencies.

For example, to select "rows" in a list of vectors, we use `lapply` to apply `[`
to each component vector:

```{r}
L <- as.list(D)
str(L)
sub.L <- lapply(L[1:2], '[', idx)
str(sub.L)

identical(sub.L, as.list(D[idx,1:2]))
```

This results in the same elements selected as with data frame or matrix
subsetting, except the result remains a list.

The expression `lapply(L[1:2], '[', idx)` is equivalent to:

```{r}
cols <- 1:2
sub.L <- vector("list", length(cols))
for(i in cols) sub.L[[i]] <- L[[i]][idx]
```

Despite the explicit looping over columns this list based subsetting is
as fast or faster than matrix subsetting:

```{r}
microbenchmark(lapply(L[1:2], '[', idx), M[idx,1:2], D[idx,1:2], times=25)
```

And if we only index columns then the list is even faster than the data frame:

```{r}
microbenchmark(L[1:2], M[,1:2], D[,1:2], times=25)
```

# Patterns in List Based Analysis

## An Example: Euclidean Distances

Working with lists takes some extra work, but because R is a mostly functional
language with good list facilities, it can rewarding to find the list
equivalents of common matrix operations.  We do this here for the Euclidean
distance between two variables.

To compute the Euclidean distance between the 2D points in (x1,y1) and (x2,y2)
with matrices we use the square root of the sum of squares:

```{r echo=FALSE}
a <- c('x1','y1')
b <- c('x2','y2')
```
<a name='dist-calc'></a>
```{r dist-calc, eval=FALSE}
a <- c('x1','y1')
b <- c('x2','y2')
sqrt(rowSums((M[,a] - M[,b])^2))
```

This can be broken down into component pieces so we can compare each step of the
matrix calculation to the list equivalent.  First, the difference in the x and y
values:

```{r}
microbenchmark(times=25,
  Matrix= diff.M <- M[,a] - M[,b],
  List=   diff.L <- Map('-', L[a], L[b])
)
```

Here the list approach benefits from subsetting full columns.
Unfortunately, we need to explicitly cycle through the columns with `Map`, a
close cousin of `mapply`.  We use `Map` instead of `mapply` because `Map` always returns a list.  `Map` calls its first argument with each value in the
second and third arguments in turn.  It is equivalent to:

```{r eval=FALSE}
fun <- `-`
cols <- seq_along(a)
diff.L <- vector('list', length(cols))
for(i in cols) diff.L[[i]] <- fun(L[a][[i]], L[b][[i]])
```

To take the squares of each column we do so explicitly with `lapply`:

```{r}
microbenchmark(times=25,
  Matrix= diffsq.M <- diff.M ^ 2,
  List=   diffsq.L <- lapply(diff.L, '^', 2)
)
```

Unsurprisingly there is no speed benefit here, but there is also no penalty.
For the final step we can use `Reduce`:

```{r}
microbenchmark(times=25,
  Matrix= dist.M <- sqrt(rowSums(diffsq.M)),
  List=   dist.L <- sqrt(Reduce('+', diffsq.L))
)
```

`Reduce` collapses the elements of the input list by repeatedly applying a
binary function to turn two list elements into one.  To be completely honest I'm
rather surprised it is _faster_ than `rowSums`.  This is partly because we only
have two columns we are summing over, but still.  `Reduce` is equivalent to:

```{r eval=FALSE}
res <- numeric(length(diffsq.L[[1]]))
for(el in diffsq.L) res <- res + el
```

And to show that the calculations are the same:

```{r}
all.equal(dist.L, dist.M)
```

In exchange for the hassle of all the explicit looping through columns with
`Map`, `lapply`, and `Reduce` we get a noticeable performance gains:

```{r}
microbenchmark(times=25,
  Matrix= sqrt(rowSums((M[,a] - M[,b])^2)),
  List=   sqrt(Reduce('+', lapply(Map('-', L[a], L[b]), '^', 2)))
)
```

A ~2x speed-up from such an innocuous looking operation is
nothing to sneeze at.  Each step is at least as fast with lists as it is with
matrices, and some are faster.  This should be true so long as your data is tall
and skinny, i.e. with a high ratio of observations to variables, and you need to
subset for columns.

## List to Matrix and Back

R is awesome because it adopts some of the better features of functional
languages and list based languages.  For example, the following two statements
are equivalent:

```{r}
ML1 <- cbind(x1=L[[1]], y1=L[[2]], x2=L[[3]], y2=L[[4]])
ML2 <- do.call(cbind, L)
```

Proof:

```{r}
identical(ML1, ML2)
identical(ML1, M)
```

The blurring of calls, lists, data, and functions allows for very powerful
manipulations of list based data.  In this case, `do.call(cbind, LIST)` will
transform any list containing equal length vectors into a matrix, irrespective
of how many columns it has.To confirm `cbind` did what we claim:

To go in the other direction:

```{r}
LM <- lapply(seq_len(ncol(M)), function(x) M[,x])
identical(L, setNames(LM, colnames(M)))
```

You _could_ use `split` but it is much slower.

## More on `do.call`

List based analysis works particularly well with `do.call` and functions that
use `...` parameters:

```{r}
microbenchmark(times=25,
  do.call(pmin, L),
  pmin(M[,1],M[,2],M[,3],M[,4])
)
```

The expression is both easier to type, and faster!

There are a couple of "gotchas" with `do.call`, but they only crop up if you try
to pass quoted language as part of the argument list (e.g. `quote(a + b)`).  If
you are just using normal R data objects then the one drawback that I am
aware of is that the call stack can get messy if you ever invoke `traceback()`
or `sys.calls()` from a function called by `do.call`.

Remember that data frames are lists, so you can do things like:

```{r eval=FALSE}
do.call(pmin, iris[-5])
```

## List Matrices?

Let's revisit our earlier [distance calculation example](#dist-calc):

```{r dist-calc, eval=FALSE}
```

One problem with our matrix structure is that it does not allow us to flexibly
specify which dimensions or which points to select.  This is not too bad in the
example because there are only two variables and two dimensions, but with more
of either typing out the names or constructing them with string manipulation
would quickly become tedious.

Instead, we can use an array structure:

```{r}
A <- array(M, dim=c(nrow(M), ncol(M)/2, 2), dimnames=list(NULL, c('x','y')))
A[1:2,,]
```

The third dimension tracks which variable the x-y coordinates belong to.  The
distance calculation becomes:

```{r}
dist.A <- sqrt(rowSums((A[,,1] - A[,,2])^2))
all.equal(dist.A, dist.M)
```

But how do we benefit from this structural simplification with our list-based
methods?  Well, it turns out that lists can be matrices and arrays too:

```{r}
LM <- matrix(L, nrow=2, dimnames=list(c('x','y')))
LM
str(LM)
```
A chimera!  If you've never seen this it may seem a bit strange, but it allows
us to create array-like structures with lists:

```{r}
head(LM[['x', 1]])
head(A[,'x', 1])
```

We can use the same abstraction as with arrays, and keep performance advantage:

```{r}
dist.LM <- sqrt(Reduce('+', lapply(Map('-', LM[,1], LM[,2]), '^', 2)))
all.equal(dist.LM, dist.A)

microbenchmark(times=10,
  sqrt(rowSums((A[,,1] - A[,,2])^2)),
  sqrt(Reduce('+', lapply(Map('-', LM[,1], LM[,2]), '^', 2)))
)
```

# A Case Study

One of my side projects required me to implement [barycentric coordinate][1]
conversions to interpolate values from triangle vertices into the triangles
themselves.  So in the following diagram I wish to compute the barycentric
coordinates of each gray point relative to its enclosing triangle:

```{r echo=FALSE, eval=FALSE}
bary_m <- function(M) {
  det <- (M[,'y2']-M[,'y3'])*(M[,'x1']-M[,'x3']) +
         (M[,'x3']-M[,'x2'])*(M[,'y1']-M[,'y3'])

  l1 <- (
          (M[,'y2']-M[,'y3']) * (M[,'x']-M[,'x3']) +
          (M[,'x3']-M[,'x2']) * (M[,'y']-M[,'y3'])
        ) / det
  l2 <- (
          (M[,'y3']-M[,'y1']) * (M[,'x']-M[,'x3']) +
          (M[,'x1']-M[,'x3']) * (M[,'y']-M[,'y3'])
        ) / det
  l3 <- 1 - l1 - l2
  cbind(l1, l2, l3)
}
bary_l <- function(L) {
  det <- (L[['y2']]-L[['y3']])*(L[['x1']]-L[['x3']]) +
         (L[['x3']]-L[['x2']])*(L[['y1']]-L[['y3']])

  l1 <- (
          (L[['y2']]-L[['y3']]) * (L[['x']]-L[['x3']]) +
          (L[['x3']]-L[['x2']]) * (L[['y']]-L[['y3']])
        ) / det
  l2 <- (
          (L[['y3']]-L[['y1']]) * (L[['x']]-L[['x3']]) +
          (L[['x1']]-L[['x3']]) * (L[['y']]-L[['y3']])
        ) / det
  l3 <- 1 - l1 - l2
  list(l1, l2, l3)
}
size <- 2
vert <- sin(60/180*pi)

x <- c(0, .5, 1)
y <- c(0, vert, 0)

x.rep <- 2
y.rep <- 4
rep <- x.rep * y.rep
x.off <- 1.2
y.off <- 1

xy.off <- as.matrix(
  expand.grid(x=(seq_len(x.rep) - 1) * x.off, y=(seq_len(y.rep) - 1) * y.off)
)[rep(seq_len(rep), each=3),]
tri.up <- cbind(
  cbind(rep(x, rep), rep(y, rep)) + xy.off, id=rep(seq_len(rep), each=3)
)
tri.down <- tri.up
tri.down[,'y'] <- -tri.down[,'y']
tri.down[,'y'] <- tri.down[,'y'] - min(tri.down[,'y'])
tri.down[,'x'] <- tri.down[,'x'] + x.off / 2
tri.down[,'id'] <- tri.down[,'id'] + max(tri.down[,'id'])

meshdf <- as.data.frame(rbind(tri.up, tri.down))
meshdf <- transform(meshdf,
  x.mid=ave(x, id, FUN=function(z) mean(range(z))),
  y.mid=ave(y, id, FUN=function(z) mean(range(z)))
)
meshdf <- with(meshdf, meshdf[order(y.mid + x.mid / (rep * 2)),])
meshdf$id <- rep(seq_len(rep * 2), each=3)
meshdf$idv <- seq_len(nrow(meshdf))

colstep <- 256 / (size * 2)^2
meshdf <- transform(
  meshdf,
  r=c(1,0,0) + ((id * colstep) - 1) * c(0, 1, 0) / 255,
  g=c(0,1,0) + ((id * colstep) - 1) * c(0, 0, 1) / 255,
  b=c(0,0,1) + ((id * colstep) - 1) * c(1, 0, 0) / 255
)
meshdf <- transform(meshdf, color=rgb(r, g, b))

n <- 30
mrg <- .025
rng.x <- diff(range(x))
rng.y <- diff(range(y))
points <- expand.grid(
  x=seq(min(x) + mrg * rng.x, max(x) - mrg * rng.x, length.out=n),
  y=seq(min(y) + mrg * rng.y, max(y) - mrg * rng.y, length.out=n),
  KEEP.OUT.ATTRS=FALSE
)
points.all <- do.call(
  rbind,
  lapply(
    sort(unique(meshdf$id)),
    function(z) {
      transform(points,
        id=z, x=x + (z-1) %% y.rep * x.off / 2, y=y + (z-1) %/% y.rep * y.off
) } ) )
ggplot() +
  geom_point(data=points.all, aes(x, y, color=factor(id)), size=.2) +
  geom_polygon(data=meshdf, aes(x, y, group=id), alpha=0, color='black')


mesh1 <- meshdf[seq(1, len=rep*2, by=3), c('x','y','r','g','b')]
mesh2 <- meshdf[seq(2, len=rep*2, by=3), c('x','y','r','g','b')]
mesh3 <- meshdf[seq(3, len=rep*2, by=3), c('x','y','r','g','b')]

l.dat <- matrix(
  c(as.list(mesh1), as.list(mesh2), as.list(mesh3)),
  ncol=ncol(mesh1), dimnames=list(NULL, names(mesh1)), byrow=TRUE
)
l.dat.all <- lapply(l.dat, '[', points.all$id)
dim(l.dat.all) <- dim(l.dat)
dimnames(l.dat.all) <- dimnames(l.dat)

l.coord <- c(points.all[c('x', 'y')], l.dat.all[, c('x', 'y')])
names(l.coord) <- c('x', 'y', 'x1', 'x2', 'x3', 'y1', 'y2', 'y3')
bl <- bary_l(l.coord)

inbounds <- Reduce('&', lapply(bl, '>=', 0))
blg <- lapply(bl, "^", 1/1.4)
clrs <- lapply(
  apply(l.dat.all[, c('r','g','b')], 2, Map, f="*", blg),
  Reduce, f="+"
)
clrs <- lapply(clrs, pmin, 1)
res <- cbind(
  points.all[inbounds,],
  color=do.call(rgb, lapply(clrs, '[', inbounds))
)
ggplot(meshdf) +
  geom_point(data=res, aes(x, y, color=I(color)), size=1.5, shape=15) +
  geom_polygon(aes(x, y, group=id), alpha=0, color='black') +
  geom_point(aes(x, y, color=I(color)), size=3)
  # geom_label(aes(x.mid, y.mid, label=id))

microbenchmark(bl <- bary_l(l.coord), bm <- bary_m(mx.coord))

all.equal(do.call(cbind, bl), bm, check.attributes=FALSE)
# generate points inside

poly.x <- rep(NA_real_, nrow(meshdf) / 3 * 4)
poly.x[seq_len(nrow(meshdf)) +
  cumsum(((seq_len(nrow(meshdf)) - 1) / 3))



```


```{r, echo=FALSE, eval=FALSE}
## Matrix multiplication
L_dotprod <- function(x, y) mapply(function(u, v) sum(L[[u]] * L[[v]]), x, y)
microbenchmark(times=5,
  prod_L <- outer(seq_along(L), seq_along(L), L_dotprod),
  prod_M <- t(M) %*% M
)
all.equal(prod_L, prod_M, check.attributes=FALSE)
```


[1]: https://en.wikipedia.org/wiki/Barycentric_coordinate_system#Barycentric_coordinates_on_triangles
