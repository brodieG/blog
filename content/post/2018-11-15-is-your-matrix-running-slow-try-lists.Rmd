---
title: Is Your Matrix Running Slow? Try Lists.
author: ~
date: '2018-11-15'
slug: is-your-matrix-running-slow-try-lists
draft: true
categories: [optimization]
tags: [rstats]
---

```{r echo=FALSE}
options(digits=3)
library(microbenchmark)
knitr::opts_chunk$set(
  comment = ""
)
```

# Matrices are Always Faster Than Data Frames, Right?

Generally speaking matrices are amongst the R data structures that are
fastest to operate on.  After all, they are contiguous chunks of memory with
almost no overhead.  Unfortunately, they do come with a major limitation: any
subset operation requires a copy of the data subset.  Consider:

```{r}
set.seed(1023)
x1 <- runif(1e6)
y1 <- runif(1e6)
x2 <- runif(1e6)
y2 <- runif(1e6)

M <- cbind(x1, y1, x2, y2)
D <- data.frame(x1, y1, x2, y2)
microbenchmark(M[,1:2], D[,1:2], times=25)
```

The data frame subset is much faster because R treats data frame columns as
independent objects, so referencing an entire column does not require a copy if
the data is not modified.

# Awesome, I'll Just Use Data Frames! Right?

If you have been in R circles for any length of time you will inevitably have
heard something I will loosely paraphrase as:

> blah blah blah data frames are EVIL blah blah

However said that to you was not completely lying.  Let us see what happens if
we select rows and columns:

```{r}
idx <- sample(seq_len(nrow(M)), 5e4)
microbenchmark(M[idx,1:2], D[idx,1:2], times=25)
```

Ouch!  So we cannot just switch to data frames blindly.  But what is going on?
We can easily check how time is used by our expression with
[treeprof](https://github.com/brodieG/treeprof):

```{r eval=FALSE}
treeprof::treeprof(D[idx,1:2], target.time=1)
```
```{r echo=FALSE}
print(treeprof::treeprof(D[idx,1:2], target.time=1))
```

So `anyDuplicated` is taking a bunch of time, and while not obvious from the
profile data alone, this is because data frames do not like having duplicated
row names.  So let's introduce some duplicated row names to see what happens:

```{r eval=FALSE}
treeprof::treeprof(D[c(idx,idx[1]), 1:2], target.time=1)
```
```{r echo=FALSE}
print(treeprof::treeprof(D[c(idx,idx[1]), 1:2], target.time=1))
```

Data frame creates new character row names that now have to which adds
additional overhead.  This makes the data frame operation several times slower
than the corresponding matrix operation.  In exchange you get:

```{r}
tail(rownames(D[c(idx,idx[1]), 1:2]))
```

Not. a. good. deal.

# Lists to the Rescue

With a little extra work you can do most of what you would normally do with a
matrix or data frame with a list.  It is not something I would want to do on a
regular basis, but for a package where performance it could be a
better alternative than compiled code or adding a dependency.

```{r}
L <- as.list(D)
identical(
  lapply(L[1:2], '[', idx),   ## apply `[` to each "column"
  as.list(D[idx,1:2])
)
```

In this case we subset each column individually, but as shown above the result
is the same.  Now the list subsetting performance is comparable to matrix
subsetting:

```{r}
microbenchmark(M[idx,1:2], D[idx,1:2], lapply(L[1:2], '[', idx), times=25)
```

And if we only index columns then the list is even faster than the data frame:

```{r}
microbenchmark(M[,1:2], D[,1:2], L[1:2], times=25)
```

# Patterns in List Based Analysis

## An Example: Euclidean Distances

As hinted at by the `lapply(L[1:2], '[', idx)` construct earlier, unlocking the
speed benefits takes a little work.  Since R is a (mostly) functional
programming language with good list facilities, it can be fun and rewarding to
find the list equivalents of common matrix operations.

We will illustrate how to compute the Euclidean distance between the 2D points 
in (x1,y1) and (x2,y2) by breaking down the matrix based calculation into steps
and showing the corresponding list based steps:

<a name='dist-calc'></a>
```{r dist-calc, eval=FALSE}
sqrt(rowSums((M[,c('x1','y1')] - M[,c('x2','y2')])^2))
```

To start, we use `Map` pairwise differences between the x and y columns:

```{r}
microbenchmark(times=25,
  diff_M <- M[,c('x1','y1')] - M[,c('x2','y2')],
  diff_L <- Map('-', L[c('x1','y1')], L[c('x2','y2')])
)
```

Unlike with matrices, we need to explicitly cycle through the columns with
`Map`, but in exchange we get fast list subsetting.  `Map` sequentially calls
the binary function `'-'` (minus operator) with one element from each of the
input lists `L[c('x1','y1')]` and `L[c('x2','y2')]`.  This is semantically
equivalent to the following expression:

```{r eval=FALSE}
list(L[['x1']] - L[['x2']], L[['y1']] - L[['y2']])
```

The advantage is that we can easily extend the `Map` call to more columns if
needed.

Since `diff_L` is a list taking the squares of each difference requires `lapply`:

```{r}
microbenchmark(times=25,
  diffsq_M <- diff_M ^ 2
  diffsq_L <- lapply(diff_L, '^', 2),
)
```

In this case we do not get a speed advantage since there is no column
subsetting, but the two approaches are neck and neck with each other.

For the final step we can use `Reduce`:

```{r}
microbenchmark(times=25,
  dist_M <- sqrt(rowSums(diffsq_M)),
  dist_L <- sqrt(Reduce('+', diffsq_L))
)
```

And to show that the calculations are actually equivalent:

```{r}
all.equal(dist_L, dist_M)
```

`Reduce` collapses the elements of the input list by repeatedly applying a binary
function to turn two list elements into one.  In this case we only had two
columns so we could just have used `diffsq_l[[1]] + diffsq_l[[2]]`, but as with
`rowSums`, `Reduce` will automatically scale with more columns.  It is
semantically equivalent to:

```{r}
res <- numeric(length(diffsq_L[[1]]))
for(el in diffsq_L) res <- res + el
```

I am unsure why `rowSums` is slower than `Reduce` in this case.  I would have
expected the two to be comparable.

The conclusion is that the list based calculation is faster overall, with each
step at least as fast as the matrix operation.  This should generally be true so
long as you data is tall and skinny, i.e. with a high ratio of observations to
variables, and you need to subset for columns.

## List to Matrix and Back

R is awesome because it adopts some of the better features of functional
languages and list based languages.  For example, the following two statements
are equivalent:

```{r}
ML1 <- cbind(x1=L[[1]], y1=L[[2]], x2=L[[3]], y2=L[[4]])
ML2 <- do.call(cbind, L)
identical(ML1, ML2)
```

`do.call` calls a function (`cbind`) with a list for arguments.  The blurring of
calls, lists, data, and functions allows for very powerful manipulations of
list based data.  In this case, `do.call(cbind, LIST)` will transform any
list containing equal length vectors into a matrix, irrespective of how many
columns it has.  To confirm `cbind` did what we claim:

```{r}
identical(M, ML2)
```

To go in the other direction:

```{r}
LM <- lapply(seq_len(ncol(M)), function(x) M[,x])
identical(L, setNames(LM, colnames(M)))
```

You _could_ use `split` but it is much slower.

## More on `do.call`

List based analysis works particularly well with `do.call` and functions that
use `...` parameters:

```{r}
microbenchmark(times=25,
  do.call(pmin, L),
  pmin(M[,1L],M[,2L],M[,3L],M[,4L])
)
```

The expression is both easier to type, and faster!

There are a couple of "gotchas" with `do.call`, but they only crop up if you try
to pass quoted language as part of the argument list (e.g. `quote(a + b)`).  If
you are just using normal R data objects then the one drawback that I am
aware of is that the call stack can get messy if you ever invoke `traceback()`
or `sys.calls()` from a function called by `do.call`.

## Other Patterns

Operate with a different scalar for each column:

```{r}
microbenchmark(times=10,
  adds_L <- Map('+', L, 1:4),
  adds_M <- t(t(M) + 1:4)
)
all.equal(do.call(cbind, adds_L), adds_M)
```

Matrix multiplication:

```{r}
L_dotprod <- function(x, y) mapply(function(u, v) sum(L[[u]] * L[[v]]), x, y)
microbenchmark(times=5,
  prod_L <- outer(seq_along(L), seq_along(L), L_dotprod),
  prod_M <- t(M) %*% M
)
all.equal(prod_L, prod_M, check.attributes=FALSE)
```

So maybe we should not just willy nilly switch everything to lists.  To be
honest I was surprised the list approach got so close, although partly that is
because we need to transpose `M` for the multiplication.  If we had a
pre-transposed matrix the difference would be more marked.

Given this particular trick is unlikely to be useful, we will not spend any time
explaining it.

## List Matrices?

Let's revisit our earlier [distance calculation example](#dist-calc):

```{r dist-calc, eval=FALSE}
```

One problem with our matrix structure is that it does not allow us to flexibly
specify which dimensions or which points to select.  This is not too bad in the
example because there are only two variables and two dimensions, but with more
of either typing out the names or constructing them with string manipulation
would quickly become tedious.  Instead, we can use an array structure:

```{r}
A <- array(M, dim=c(nrow(M), ncol(M)/2, 2), dimnames=list(NULL, c('x','y')))
A[1:2,,]
```

The third dimension tracks which variable the x-y coordinates belong to.  The
distance calculation becomes:

```{r}
dist_A <- sqrt(rowSums((A[,,1] - A[,,2])^2))
all.equal(dist_A, dist_M)
```

But how do we benefit from this structural simplification with our list-based
methods?  Well, it turns out that lists can be matrices and arrays too:

```{r}
LA <- matrix(L, nrow=2, dimnames=list(c('x','y')))
LA
```

A chimera!  If you've never seen this it may seem a bit strange, but it is very
useful:

```{r}
LA[1,], LA[2,]

```






# A Case Study

One of my side projects required me to implement [barycentric coordinate][1]
conversions to interpolate values from triangle vertices into the triangles
themselves.  So in the following diagram I wish to compute the barycentric
coordinates of each gray point relative to its enclosing triangle:

```{r echo=FALSE, eval=FALSE}
size <- 2
vert <- sin(60/180*pi)

x <- c(0, .5, 1)
y <- c(0, vert, 0)

x.rep <- 2
y.rep <- 4
rep <- x.rep * y.rep
x.off <- 1.2
y.off <- 1

xy.off <- as.matrix(
  expand.grid(x=(seq_len(x.rep) - 1) * x.off, y=(seq_len(y.rep) - 1) * y.off)
)[rep(seq_len(rep), each=3),]
tri.up <- cbind(
  cbind(rep(x, rep), rep(y, rep)) + xy.off, id=rep(seq_len(rep), each=3)
)
tri.down <- tri.up
tri.down[,'y'] <- -tri.down[,'y']
tri.down[,'y'] <- tri.down[,'y'] - min(tri.down[,'y'])
tri.down[,'x'] <- tri.down[,'x'] + x.off / 2
tri.down[,'id'] <- tri.down[,'id'] + max(tri.down[,'id'])

meshdf <- as.data.frame(rbind(tri.up, tri.down))
meshdf <- transform(meshdf,
  x.mid=ave(x, id, FUN=function(z) mean(range(z))),
  y.mid=ave(y, id, FUN=function(z) mean(range(z)))
)
meshdf <- with(meshdf, meshdf[order(y.mid + x.mid / (rep * 2)),])
meshdf$id <- rep(seq_len(rep * 2), each=3)
meshdf$idv <- seq_len(nrow(meshdf))

colstep <- 256 / (size * 2)^2
meshdf <- transform(
  meshdf,
  r=((id * colstep) - 1) * c(1, 0, 0) / 255,
  g=((id * colstep) - 1) * c(0, 1, 0) / 255,
  b=((id * colstep) - 1) * c(0, 0, 1) / 255
)
meshdf <- transform(meshdf, color=rgb(r, g, b))

n <- 30
mrg <- .025
rng.x <- diff(range(x))
rng.y <- diff(range(y))
points <- expand.grid(
  x=seq(min(x) + mrg * rng.x, max(x) - mrg * rng.x, length.out=n),
  y=seq(min(y) + mrg * rng.y, max(y) - mrg * rng.y, length.out=n)
)
points.all <- do.call(
  rbind,
  lapply(
    sort(unique(meshdf$id)),
    function(z) {
      transform(points,
        id=z, x=x + (z-1) %% y.rep * x.off / 2, y=y + (z-1) %/% y.rep * y.off
) } ) )


mesh1 <- meshdf[seq(1, len=rep*2, by=3), c('x','y','r','g','b')]
mesh2 <- meshdf[seq(2, len=rep*2, by=3), c('x','y','r','g','b')]
mesh3 <- meshdf[seq(3, len=rep*2, by=3), c('x','y','r','g','b')]

l.dat <- matrix(
  c(as.list(mesh1), as.list(mesh2), as.list(mesh3)),
  ncol=ncol(mesh1), dimnames=list(NULL, names(mesh1)), byrow=TRUE
)
l.dat.all <- lapply(l.dat, '[', points.all$id)
dim(l.dat.all) <- dim(l.dat)
dimnames(l.dat.all) <- dimnames(l.dat)

l.coord <- c(points.all[c('x', 'y')], l.dat.all[, c('x', 'y')])
names(l.coord) <- paste0(c('x', 'y'), rep(c("",1:3), each=2))
bl <- bary_l(list.coord)
inbounds <- Reduce('&', lapply(bl, '>=', 0))
blg <- lapply(bl, "^", 1/2.2)
clrs <- lapply(
  apply(l.dat.all[, c('r','g','b')], 2, Map, f="*", blg),
  Reduce, f="+"
)
res <- cbind(
  points.all[inbounds,], color=do.call(rgb, lapply(clrs, '[', inbounds))
)
ggplot(meshdf) +
  geom_point(data=res, aes(x, y, color=I(color)), size=1.5, shape=15) +
  geom_polygon(aes(x, y, group=id), alpha=0, color='black') +
  geom_point(aes(x, y, color=I(color)), size=3)
  # geom_label(aes(x.mid, y.mid, label=id))

microbenchmark(bl <- bary_l(list.coord), bm <- bary_m(mx.coord))

all.equal(do.call(cbind, bl), bm, check.attributes=FALSE)
# generate points inside

bary_m <- function(M) {
  det <- (M[,'y2']-M[,'y3'])*(M[,'x1']-M[,'x3']) +
         (M[,'x3']-M[,'x2'])*(M[,'y1']-M[,'y3'])

  l1 <- (
          (M[,'y2']-M[,'y3']) * (M[,'x']-M[,'x3']) +
          (M[,'x3']-M[,'x2']) * (M[,'y']-M[,'y3'])
        ) / det
  l2 <- (
          (M[,'y3']-M[,'y1']) * (M[,'x']-M[,'x3']) +
          (M[,'x1']-M[,'x3']) * (M[,'y']-M[,'y3'])
        ) / det
  l3 <- 1 - l1 - l2
  cbind(l1, l2, l3)
}
bary_l <- function(L) {
  det <- (L[['y2']]-L[['y3']])*(L[['x1']]-L[['x3']]) +
         (L[['x3']]-L[['x2']])*(L[['y1']]-L[['y3']])

  l1 <- (
          (L[['y2']]-L[['y3']]) * (L[['x']]-L[['x3']]) +
          (L[['x3']]-L[['x2']]) * (L[['y']]-L[['y3']])
        ) / det
  l2 <- (
          (L[['y3']]-L[['y1']]) * (L[['x']]-L[['x3']]) +
          (L[['x1']]-L[['x3']]) * (L[['y']]-L[['y3']])
        ) / det
  l3 <- 1 - l1 - l2
  list(l1, l2, l3)
}



```




[1]: https://en.wikipedia.org/wiki/Barycentric_coordinate_system#Barycentric_coordinates_on_triangles
