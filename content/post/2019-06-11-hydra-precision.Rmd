---
title: "Hydra Chronicles, Part III: Limits of Precision"
author: ~
date: '2019-06-18'
slug: hydra-precision
categories: [r]
tags: [optimization]
image: /post/2019-06-11-hydra-precision_files/user-imgs/quakes.png
imagerect: /post/2019-06-11-hydra-precision_files/user-imgs/iris.png
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: "How we ran face first "
---
```{r echo=FALSE}
old.opt <- options(digits=3)
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
suppressMessages(library(ggplot2))
```
```{r echo=FALSE, comment="", results='asis'}
old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)
```

```{r echo=FALSE}
.group_sum_int <- function(x, last.in.group) {
  xgc <- cumsum(x)[last.in.group]
  diff(c(0, xgc))
}
group_slope <- function(x, y, grp) {
  ## order inputs by group
  o <- order(grp)
  go <- grp[o]
  xo <- x[o]
  yo <- y[o]

  ## group sizes and group indices
  grle <- rle(go)
  gn <- grle[['lengths']]
  gnc <- cumsum(gn)              # Last index in each group
  gi <- rep(seq_along(gn), gn)   # Group recycle indices

  ## compute mean(x) and mean(y), and recycle them
  ## to each element of `x` and `y`:
  sx <- .group_sum_int(xo, gnc)
  ux <- (sx/gn)[gi]
  sy <- .group_sum_int(yo, gnc)
  uy <- (sy/gn)[gi]

  ## (x - mean(x)) and (y - mean(y))
  x_ux <- xo - ux
  y_uy <- yo - uy

  ## Slopes!
  x_ux.y_uy <- .group_sum_int(x_ux * y_uy, gnc)
  x_ux2 <- .group_sum_int(x_ux ^ 2, gnc)
  setNames(x_ux.y_uy / x_ux2, grle[['values']])
}
```

# Recap

<img
  id='front-img' 
  src='/post/2019-06-11-hydra-precision_files/user-imgs/quakes.png'
  class='post-inset-image'
/>

Last week we [slugged it out][100] with the reigning group-stats heavy weight
champ `data.table`.  The challenge was to compute the slope of a bivariate
regression fit line over 10MM entries and ~1MM groups.  The formula is:

$$\frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^{2}}$$

While this may seem like a silly task, we do it because computing group
statistics is both useful and a [weakness in R][102].  We implemented
[`group_slope`][101] based on [John Mount's][104] [`cumsum` idea][105], and for
a brief moment we thought we scored an improbable win over
`data.table`[^reformulation].  Unfortunately our results did not withstand
scrutiny.

Here is the data we used:

```{r make-data, warning=FALSE}
RNGversion("3.5.2"); set.seed(42)
n     <- 1e7
n.grp <- 1e6
grp   <- sample(n.grp, n, replace=TRUE)
noise <- rep(c(.001, -.001), n/2)  # more on this later
x     <- runif(n) + noise
y     <- runif(n) + noise          # we'll use this later
```

The slow and steady base R approach is to define the statistic as a function,
`split` the data, and apply the function with `vapply` or similar.

```{r eval=FALSE}
slope <- function(x, y) {
  x_ux <- x - mean.default(x)
  y_uy <- y - mean.default(y)
  sum(x_ux * y_uy) / sum(x_ux ^ 2)
}
id <- seq_along(grp)
id.split <- split(id, grp)
slope.ply <- vapply(id.split, function(id) slope(x[id], y[id]), 0)
```

Our [wild child version][101] is [4-6x][108] faster, and should produce the
same results.  It doesn't quite:

```{r eval=FALSE}
slope.gs <- group_slope(x, y, grp)  # our new method
all.equal(slope.gs, slope.ply)
```
```
[1] "Mean relative difference: 0.0001161377"
```

With a generous tolerance we find equality:

```{r eval=FALSE}
all.equal(slope.gs, slope.ply, tolerance=2e-3)
```
```
[1] TRUE
```

> **Disclaimer**: I have no special knowledge of floating point precision
> issues.  Everything I know is from reading the [wiki page][12], random web
> research, and experimentation.  If your billion dollar Mars lander burns up on
> entry because you relied on information in this post, you only have yourself
> to blame.

## Oh the Horror!

As we've alluded to previously `group_slope` is running into precision issues,
but we'd like to figure out exactly what's going wrong.  Let's find the worst
offending result, which for convenience we'll call `B`.  Its index in the group
list is then `B.gi`:<span id=prec-error></span>

```{r inputs-ordered, echo=FALSE}
o <- order(grp)
go <- grp[o]
xo <- x[o]
yo <- y[o]
grle <- rle(go)
gn <- grle[['lengths']]
gnc <- cumsum(gn)
gi <- rep(seq_along(gn), gn)
```

```{r echo=FALSE}
 # hard coding the above value so that rest of stuff that we actually
 # run can use it
B.gi <- 616793L
B.g <- 616826L
```
```{r eval=FALSE}
B.gi <- which.max(abs(slope.gs / slope.ply))
slope.gs[B.gi]
```
```
 616826
-3014.2
```
```{r eval=FALSE}
slope.ply[B.gi]
```
```
   616826
-2977.281
```

<span id=obs-err></span>That's a ~1.2% error, which is for a computational error
is downright ghastly, and on an entirely different plane of existence than the
comparatively quaint[^unequal].

```{r}
1 - 0.7 == 0.3
```

Let's look at the values in our problem group `616826`.  It turns out there are
only two of them:

```{r eval=FALSE}
x[grp == 616826]
```
```
[1] 0.4229786 0.4229543
```
```{r eval=FALSE}
y[grp == 616826]
```
```
[1] 0.7637899 0.8360645
```

The `x` values are kinda close to each other, but well within the resolution
of the [double precision format][12], which are what R "numeric" values are
stored as.  It gets a bit worse though because the next step is `$\sum(x -
\bar{x})^2$`:

```{r echo=FALSE}
x.g <- x[grp == 616826]
(B.s <- sum((x.g - mean(x.g))^2))
```
```{r eval=FALSE}
(B.s <- sum((x[grp == 616826] - mean(x[grp == 616826]))^2))
```
```
[1] 0.0000000002946472
```

Yeah, the numbers are starting to get small but still nothing crazy.  In order
to see why things go pear shaped we need to look back at the algorithm we use in
`group_slope`.  Remember that we're doing all of this in vectorized code, so we
don't have the luxury of using things like `sum(x[grp == 616826])` where we
specify groups explicitly to sum.  Instead, we use `cumsum` on group ordered
data.  Here is a visual recap of the key steps (these are steps 4 and 5 of the
[previously described algorithm][110]):

```{r cumsum-review, echo=FALSE, warning=FALSE}
library(ggbg)
RNGversion("3.5.2"); set.seed(42)
n1 <- 7
x1 <- seq_len(n1)
y1 <- runif(n1);
colors <- c('#3333ee', '#33ee33', '#eeee33')
g1 <- sample(1:3, n1, replace=TRUE)
steps <- c(
  '1 - Start', '2 - Sort By Group', '3 - Cumulative Sum',
  '4 - Last Value in Group', '5 - Take Differences', '6 - Group Sums!'
)
steps <- factor(steps, levels=steps)
df1 <- data.frame(
  x1, y1, g1=as.character(g1), step=steps[[1]], stringsAsFactors=FALSE
)
df2 <- df1[order(g1),]
df2[['x1']] <- x1
df2[['step']] <- steps[[2]]
df3 <- df2
df3 <- transform(
  df3, yc=cumsum(y1), step=steps[[3]],
  last=c(head(g1, -1) != tail(g1, -1), TRUE)
)
df4 <- transform(df3, step=steps[[4]])
df5 <- transform(
  subset(df4, last), x1=3:5, y1=c(yc[[1]], diff(yc)), step=steps[[5]]
)
df6 <- transform(df5, step=steps[[6]])

plot.extra <- list(
  facet_wrap(~step, ncol=3),
  ylab(NULL), xlab(NULL),
  theme(
    axis.text.x=element_blank(), axis.text.y=element_blank(),
    panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
    axis.ticks=element_blank()
  ),
  scale_fill_manual(values=setNames(colors, 1:3), guide=FALSE)
)
dfa <- data.frame(
  x1=1:7, y1=0, label=c('A.1', 'A.n', 'B.1', 'B.n', 'C.1', 'C.2', 'C.n'),
  step=steps[4]
)
dfb <- data.frame(
  x1=3:5, y1=0, label=c('A.s', 'B.s', 'C.s'), step=steps[[5]]
)
ggplot(mapping=aes(x=x1, y=y1, fill=g1)) +
  geom_col(
    data=df4, mapping=aes(y=yc, alpha=I(ifelse(last, 1, .15))),
    width=0.9
  ) +
  geom_col(data=df5, width=0.9, position='waterfall') +
  geom_text(data=dfa, aes(fill=NULL, label=label), vjust=1.2) +
  geom_text(data=dfb, aes(fill=NULL, label=label), vjust=1.2) +
  plot.extra
```

Our algorithm uses `cumsum` and some clever indexing to compute the values of
`A.n` and `B.n`, respectively the total cumulative sum of our statistic up to
the last value prior to our group and up to the last value in our group:

```
                    A.n                     B.n
462824.4016201458289288 462824.4016201461199671
```

In our algorithm we take the difference of these two numbers to get the group
sum.  But when values are so close relative to their magnitudes, doing so is the
computing equivalent of walking down the stairs into a dark basement with
ominous background music. To leave no doubt about the carnage that is about to
unfold (viewer discretion advised):<span id=carnage></span>

```{r echo=FALSE}
ux <- .group_sum_int(xo, gnc) / gn
x_ux2c <- cumsum((xo - ux[gi])^2)
i <- which(go == 616826L)
A.n <- x_ux2c[i[1L] - 1L]
B.n <- x_ux2c[i[length(i)]]
B.s.new <- B.n - A.n
```
```{r eval=FALSE}
B.s.new <- B.n - A.n       # recompute B.s as per our algorithm
rbind(A.n, B.n, B.s)
```
<pre></pre>
```{r echo=FALSE}
val <- "                                       [,1]
A.n      \033[42m462824.40162014\033[43m582\033[m89287984371185302734
B.n      \033[42m462824.40162014\033[43m611\033[m99671030044555664062
B.s.new       0.000000000\033[43m29\033[m10383045673370361328"
writeLines(val)
```

We highlight the approximate double precision digits in green.  [R internally
uses a representation][13] that allows [18-19 digits of precision][14] for
`cumsum`, and we show that additional precision in yellow[^extended-2].
The colors here have no semantic relation to those in the illustrative
plots.<span id=mu-prec></span>  The only precision left is in the extended
precision range, but that is not actually available outside of `cumsum`.

So how much precision does `B.s` actually have?  The naive decimal view suggests
quite a bit with all those trailing digits.  The color highlighting tells us
instead there probably is very little as `B.s` does not have any digits in the
"green".  Finally, the [observed error of ~1.2%](#obs-err) suggests there must
be some precision.

For the actual answer, we need to look at the binary representation of the
numbers.  Double precision floating points are typically encoded as per the
[IEEE-754 standard][12].  Here is a partial binary representation of our numbers
under that encoding[^ieee-754-approx]:

```{r echo=FALSE}
val <- "+18     +10        0        -10       -20       -30 -34  <- Exponent (2^n)
 |       |         |         |         |         |   |
\033[34m+\033[39;42m11100001111111010000110011011010000100100111110111111\033[m: A.n
\033[34m+\033[39;42m11100001111111010000110011011010000100100111111000100\033[m: B.n
\033[34m+\033[m00000000000000000000000000000000000000000000000000\033[42m101\033[m: B.s.new"
writeLines(val)
```

The ones and zeroes correspond to the significand of our number in binary form.
This is the part that governs the precision of the number.  It is allocated 52
and it is allocated 52 of the 64 bits available.  The base two exponent shown in
decimal captures (most) of the magnitude, and is given 11 bits. The last
bit is used to encode the sign, shown as the leading `+`.

In the significand, each "one" dictates that the corresponding power of two is
part of the value.  So for `A.n`, we can approximately compute the value as:

```{r}
2^18 + 2^17 + 2^16 + 2^11 + 2^10 + 2^9 + 2^8 + 2^7 + 2^6 + 2^5 + 2^3
```

We only used the first 15 bits of the significand for illustrative purposes,
and that allows us to reproduce the integral part of `A.n`.  This is not enough
to differentiate between `A.n` and `B.n`.  Even with all 52 bits of
significand[^fifty-three] we can barely tell the two numbers apart.  As a
result, when we take the difference between those two numbers, we are left with
just three bits of precision as most of the precision just cancels
out[^three-bits].

But, what about all those digits of seeming precision we see in the decimal
representation of `B.s.new`?  Well check this out:

```{r echo=FALSE}
options(sci.pen=100, digits=22)
```
```{r}
2^-32 + 2^-34
```
```{r}
B.s.new
```

The entire precision of `B.s` is encapsulated in `$2^{-32} + 2^{-34}$`.
All those digits are just an artifact of the conversion of a limited precision
binary number into decimal representation.  Subtraction of two nearby
floating point numbers is a recipe for disaster, and unfortunately for us our
`cumsum` and `diff` algorithm is likely to cause this.

# How Bad Is It, Really?

Given the binary representation of our numbers, we should be able to estimate
what degree of error we could observe.  The last significant bit in our starting
numbers corresponds to `$2^{-34}$`, so at the limit both `A.n` and `B.n` could
be off by as much as `$\pm2^{-35}$`.  It follows that `B.s`, which is the
difference of `A.n` and `B.n`, could have up to twice the error:

$$2 \times \pm2^{-35} = \pm2^{-34}$$

With a baseline value of `B.s` of `$2^{-32} + 2^{-34}$`, the relative error
becomes[^rel-err]:

$$\pm\frac{2^{-34}}{2^{-32} + 2^{-34}} = \pm20\%$$

If the theoretical precision is only `$\pm20\%$`, how did we end up with only a
`$~1.2\%$` error?  Sheer luck.  Let's look again at the `A.n` and `B.n` values,
but this time comparing the underlying 80 bit representations used by `cumsum`.
The [80 bit floating point][14] representation adds 10 bits to the significand.
We'll discuss shortly how we got these 80 bit representations, but first let's
look at `A.n`:<span id='a-err'></span>

```{r a-n-err, echo=FALSE}
val <- "+18     +10        0        -10       -20       -30       -40
 |       |         |         |         |         |   *     |   *
\033[34m+\033[42;39m11100001111111010000110011011010000100100111110111111\033[m00000000000: 64 bit
\033[34m+\033[42;39m11100001111111010000110011011010000100100111110111110\033[43m1110100010\033[m1: 80 bit
\033[34m+\033[39m00000000000000000000000000000000000000000000000000000000\033[43m1011101\033[m1: A.n Err
 "
writeLines(val)
```

And `B.n`:

```{r echo=FALSE}
val <- "+18     +10        0        -10       -20       -30       -40
 |       |         |         |         |         |   *     |   *
\033[34m+\033[39;42m11100001111111010000110011011010000100100111111000100\033[m00000000000: 64 bit
\033[34m+\033[39;42m11100001111111010000110011011010000100100111111000011\033[43m1111100010\033[m0: 80 bit
\033[34m+\033[39m0000000000000000000000000000000000000000000000000000000000\033[43m11110\033[m0: B.n Err"
writeLines(val)
```

The rounding errors `A.n.err` and `B.n.err`  between the 80 and 64 bit
representations are small compared to the difference between the `A.n` and
`B.n`.  Additionally, the rounding errors are in the same direction.  As a
result the total error caused by the rounding from 80 bits to 64 bits, in
"binary" representation, should be `$(0.00111100 - 1.01111011) \times 2^{-38}$`,
or a hair under `$2^{-38}$`:

```{r echo=FALSE}
options(old.opt) # switch back to normal display
options(digits=7)
```
```{r}
A.n.err.bits <- c(1,0,1,1,1,0,1,1)
B.n.err.bits <- c(0,0,1,1,1,1,0,0)
exps <- 2^-(38:45)
(err.est <- sum(B.n.err.bits * exps) - sum(A.n.err.bits * exps))
```

Here we estimated the error in our calculation from the binary representation,
and basically got it spot on when compared to the actual error:<span
id=prec-loss-err></span>

```{r}
(err.obs <-  B.s.new - B.s)
```

If we had been unlucky maybe the true values of `A.n` and `B.n` would have been
as follows:

```{r echo=FALSE}
val <- "+18     +10        0        -10       -20       -30       -40
 |       |         |         |         |         |   *     |   *
\033[34m+\033[39;42m11100001111111010000110011011010000100100111110111111\033[m00000000000: 64 bit
\033[34m+\033[39;42m11100001111111010000110011011010000100100111110111110\033[43m1000000000\033[m1: 80 bit
\033[34m+\033[m00000000000000000000000000000000000000000000000000000\033[43m0111111111\033[m1: A.n Err"
writeLines(val)
```

And:

```{r echo=FALSE}
val <- "+18     +10        0        -10       -20       -30       -40
 |       |         |         |         |         |   *     |   *
\033[34m+\033[39;42m11100001111111010000110011011010000100100111111000100\033[m00000000000: 64 bit
\033[34m+\033[39;42m11100001111111010000110011011010000100100111111000100\033[43m1000000000\033[m0: 80 bit
\033[31m-\033[39m00000000000000000000000000000000000000000000000000000\033[43m1000000000\033[m0: B.n Err"
writeLines(val)
```

These have the same 64 bit representations as the original values, but the error
relative to the "true" 80 bit representations is quite different:

```{r}
A.n.err.bits <- c(0,1,1,1,1,1,1,1,1,1,1)
B.n.err.bits <- -c(1,0,0,0,0,0,0,0,0,0,0)  # note this is negative
exps <- 2^-(35:45)

(err.est.max <- sum(B.n.err.bits * exps) - sum(A.n.err.bits * exps))
err.est.max / B.s
```
```
err.obs / B.s
```

In this case we get roughly the maximum possible error, which is ~16 times
larger than the observed one.

## A New Hope

```{r eval=FALSE, echo=FALSE}
options(old.opt)
```

Before we give up it is worth pointing out that precision loss in some cases is
a feature.  For example, [Drew Schmidt's `float`][17] package implements single
precision numerics for R as an explicit trade-off of precision for memory and
speed.  In deep learning, [reduced precision is all the rage][20].  In a sense,
we are trading off precision for speed in our `cumsum` group sums approach,
though not explicitly.

Still, it would be nice if we could make a version of this method that
doesn't suffer from this precision infirmity.  And it turns out we can!  The
main source of precision loss is due to the accumulated sum eventually growing
much larger than any given individual group in size.  This became particularly
bad for [cumulative sum of `$(x - \bar{x})^2$`](#carnage) due to the squaring
that exacerbates relative magnitude differences.

If you are still wondering how we came up with the 80 bit representations of the
[cumulative sums earlier](#eighty-bit), wonder now more.  For example, for
`A.n`, we make a copy of `$(x - \bar{x})^2$` vector, and append the negative
of `A.n` at the position following `A.n` position:

```{r bin_rep, echo=FALSE}
# A bit sloppy

val_to_bit <- function(x, start, end) {
  sig <- integer(start - end + 1L)
  exp <- floor(log2(x))
  exps <- start:end
  for(i in seq_along(exps)) if(sig[i] <- x >= 2^exps[i]) x <- x - 2^exps[i]
  sig
}
.bin_rep_int <- function(x) {
  x <- as.numeric(x)
  if(is.na(x)) {
    # we're just taking the 731 NA_real_ value, irrespecitve of what type of NA
    # we're dealing with
    as.integer(c(0, rep(1, 11), rep(0, 41), c(1,1,1,1,0,1,0,0,0,1,0)))
  } else {
    positive <- x > 0
    x <- abs(x)
    exp <- floor(log2(x))
    significand <- val_to_bit(x, exp, exp - 52L)
    exponent <- val_to_bit(exp + 1023L, 10L, 0L)
    c(as.integer(!positive), exponent, significand[-1L])
  }
}
bin_rep <- function(x) {
  if(!all(is.finite(x)))
    stop("scalar finite non-na inputs only")
  structure(vapply(x, .bin_rep_int, integer(64)), class='bin_rep')
}
print.bin_rep <- function(x, ...) {
  apply(x, 2,
    function(y) {
      sign <- if(!y[1]) '+' else '-'
      writeLines(
        sprintf(
          "%s1.%s * 2^%d", sign,
          paste0(c(y[-(1:12)]), collapse=""),
          sum(2^(10:0) * y[2:12]) - 1023L
  ) ) } )
  invisible(NULL)
}
plot.bin_rep <- function(x, ...) {
  res <- array(x, c(dim(x), 3))
  res[1,,3] <- 0
  res[2:11,,c(1,3)] <- 0
  res[12:64,,c(1,2)] <- 0
  res[] <- pmax(res, 0.2)
  # for(i in 10:1) res[i + 1,,] <- pmax(res[i + 1,,], i/20)
  # for(i in 1:52) res[i + 12,,] <- pmax(res[i + 12,,], i/104)

  plot(as.raster(unclass(aperm(res, c(2,1,3)))))
}
plot.new()
dat <- quakes
lim <- 320
old.par <- par(mfrow=c(1,ncol(dat)), mar=c(.5, .25, 2, .25))
for(i in names(dat)) {
  plot(bin_rep(head(dat[[i]], lim)))
  title(i)
}
```
```{r}
x_ux2.A.n <- x_ux2[1:(A.ni + 1)]
x_ux2.A.n[A.ni + 1] <- -A.n
```

We now have the original `$(x - \bar{x})^2$` values up to the element just past
group `A`, and at that position, the **cumulative** sum of all the prior values,
`A.n`.  In theory, if we compute the cumulative sum again the final value should
be zero.  But it's not:

```{r}
A.n.err <- tail(cumsum(x_ux2.A.n), 1)
bin_rep(A.n.err)
```

`bin_rep`, defined in the appendix, extracts the binary representation of
doubles.  `A.n.err` is not zero because the cumulative sum was computed with 80
bit precision.  By adding `-A.n` in 64 bit precision to the end of the vector we
are left with the difference between the 80 bit and 64 bit versions of the
cumulative sum.  In this case you might recognize that `A.n.err` as the error we
showed when we compared the [80 and 64 bit versions of `A.n`](#a-err):

```{r a-n-err, echo=FALSE}
```

We got the 80 bit version of `A.n` by subtracting the error to the 64 bit
version[^by-hand].

More generally, we can compute the 80->64 bit conversion errors for every group
by adding computing the group sums as we did previously, and then adding their
negatives back to the input vector for a second pass cumulative sum.  Visually:

```{r error-correct, echo=FALSE}
steps <- c(
  '2a - Sort By Group', '3a - Insert Neg Group Sum', '4a - Cumulative Sum',
  '5a - Last Val Is Error'
)
df2a <- transform(df2, step=factor(steps[1], levels=steps))
df3 <- rbind(
  transform(df2a, x1=x1+cumsum(c(0, diff(as.integer(g1))))),
  with(
    df2,
    data.frame(
      x1=c(3L,6L,10L),
      y1=-rowsum(y1,g1)+(runif(3)-.5)*.05,
      g1=as.character(1:3),
      step=step[1]
) ) )
df3 <- df3[order(df3[['x1']]),]
df3[['step']] <- steps[2]
df4 <- transform(df3, y1=cumsum(y1), step=steps[3])
df5 <-
  transform(df4, last=c(head(g1, -1) != tail(g1, -1), TRUE), step=steps[4])
df5a <- transform(subset(df5, last))

ggplot(mapping=aes(x=x1, y=y1, fill=g1)) +
  geom_col(data=df2) +
  geom_col(data=df3) +
  geom_col(data=df4) +
  geom_col(data=df5, mapping=aes(alpha=I(ifelse(last, 1, .15)))) +
  geom_tile(
    data=df5a, mapping=aes(fill=NA, color=g1), alpha=1,
    height=.6, width=1.2, size=0.5
  ) +
  scale_colour_manual(
    values=setNames(c(colors[1:2], 'yellow'), 1:3), guide=FALSE
  ) +
  plot.extra
```

If the first pass group sum was fully precise, the values inside the
highlighting boxes in the last panel should be zero. The small values we see
inside the boxes represent the errors of each group computation[^for-effect].
Steps that we are not showing above are collecting these values, taking their
differences, and adding them back to the first pass values.

While the second pass is still subject to precision issues, these are greatly
reduced because the cumulative sum is reset to near zero after each group, so we
will never be subtracting two large but near numbers from each other.

`.group_sum_int2` below embodies the approach.  One slight modification is that
we subtract the first pass group value from the last value in the group instead
of appending it to the group:

```{r}
.group_sum_int2 <- function(x, last.in.group){
  x.grp <- .group_sum_int(x, last.in.group)     # imprecise pass
  x[last.in.group] <- x[last.in.group] - x.grp  # subtract from each group
  x.grp + .group_sum_int(x, last.in.group)      # compute errors and add
}
```
```{r eval=FALSE}
sys.time(slope.gs2 <- group_slope2(x, y, grp, group_sum=.group_sum_int2))
```
```
   user  system elapsed
  2.477   0.747   3.278
```
```{r eval=FALSE}
all.equal(c(slope.ply), slope.gs2, check.attributes=FALSE)
```
```
[1] TRUE
```
```{r eval=FALSE}
quantile(slope.ply - slope.gs2, na.rm=TRUE)
```
```
       0%       25%       50%       75%      100%
-7.11e-15 -2.78e-17  0.00e+00  2.78e-17  3.55e-15
```

This does not match the original calculations exactly, but there is essentially
no error left.  Compare to the single pass calculation:

```{r eval=FALSE}
quantile(slope.ply - slope.gs, na.rm=TRUE)
```
```
       0%       25%       50%       75%      100%
-1.35e-02 -3.65e-12 -3.51e-16  3.60e-12  3.69e+01
```

The new error is _sixteen_ orders of magnitude smaller than the original.

Two-pass precision improvement methods have been around for long time.  For
example R's own `mean` uses a [variation on this method][18].

## I Swear, It's a Feature

We noted earlier that precision loss can be a feature.  We can make it so in
this case by providing some controls for it.  We saw earlier that it is possible
to [estimate precision loss](#prec-loss-err), so we can use this decide whether
we want to trigger the second precision correcting pass.  [`.group_sum_int3` in
the appendix](#adjustable-precision-group-sums) does exactly this by providing a
`p.bits` parameter.  This parameter specifies the minimum number of bits of
precision in the resulting group sums.  If as a result of the first pass we
determine the worst case loss could take us below `p.bits`, we run the second
precision improvement pass:

```{r group_sum_int3, eval=FALSE, echo=FALSE}
.group_sum_int3 <- function(x, last.in.group, p.bits=53, info=FALSE) {
  xgc <- cumsum(x)[last.in.group]
  gmax <- floor(log2(max(abs(range(xgc)))))
  gs <- diff(c(0, xgc))
  gsabs <- abs(gs)
  gmin <- floor(log2(min(gsabs[gsabs > 0])))
  precision <- 53 + (gmin - gmax)
  if(precision < p.bits) {
    x[last.in.group] <- x[last.in.group] - gs
    gs <- gs + .group_sum_int(x, last.in.group)
  }
  if(info) # info about precision and second pass
    structure(gs, precision=precision, precision.mitigation=precision < p.bits)
  else gs
}
```
```{r eval=FALSE}
microbenchmark::microbenchmark(times=10,
  .group_sum_int(x_ux2, gnc),            # original single pass
  .group_sum_int2(x_ux2, gnc),           # two pass, always
  .group_sum_int3(x_ux2, gnc, p.bits=2), # check, single pass
  .group_sum_int3(x_ux2, gnc, p.bits=3)  # check, two pass
)
```
```
                                    expr   min    lq mean median  uq max neval
              .group_sum_int(x_ux2, gnc)  44.0  94.5  101     97 115 153    10
             .group_sum_int2(x_ux2, gnc) 196.2 258.2  277    271 294 365    10
 .group_sum_int3(x_ux2, gnc, p.bits = 2)  79.7 119.4  119    126 128 133    10
 .group_sum_int3(x_ux2, gnc, p.bits = 3) 294.5 308.3  324    312 349 371    10
```

In this example the worst case precision is 2 bits, so with `p.bits=2` we run in
one pass, whereas with `p.bits=3` two passes are required.  There is some
overhead in checking precision, but it is small enough relative to the cost of
the second pass that it is probably warranted in many cases.

Let's try our slope calculation, this time again demanding at least 16 bits of
precision.  For our calculation 16 bits of precision implies the error will be
at most `$\pm2^{-16} \approx 1.53 \times 10^{-5}$`.

```{r eval=FALSE}
.group_sum_int_16 <- function(x, last.in.group)
  .group_sum_int3(x, last.in.group, p.bits=16)

sys.time(slope.gs3 <- group_slope2(x, y, grp, group_sum=.group_sum_int_16))
```
```
   user  system elapsed
  2.095   0.778   2.874
```
<!-- fastest:
   user  system elapsed
  2.164   0.682   2.856
-->
```
[1] TRUE
```
```{r eval=FALSE}
quantile((slope.ply - slope.gs3) / slope.ply, na.rm=TRUE)
```
```
       0%       25%       50%       75%      100%
-2.91e-08 -2.51e-14  0.00e+00  2.51e-14  4.73e-09
```

Indeed the errors are no larger than that.  The precision check occurs only at
the group sum step, so if other steps steps accumulate errors it is possible for
the final precision of a more complex computation to end up below the specified
levels.

<!-- ed note: fastest recorded, again with wifi off, everything shut down
   user  system elapsed
  2.234   0.598   2.842
-->

Updated reference times for `data.table`.  This is after we killed all
extraneous processes, including firefox.




Note on vector size[^vec-size].


# Conclusions

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id='feedback-cont'></div>

# References

<!--
What every programmer should know

https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html

A discussion with many easily followed examples is in Appendix G “Computational
Precision and Floating Point Arithmetic”, pages 753–771 of Statistical Analysis
and Data Display: An Intermediate Course with Examples in R, Richard M.
Heiberger and Burt Holland (Springer 2015, second edition).

https://link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf
-->

# Appendix

[^unequal]: One quirk of IEEE-754 floating point numbers is that some decimals
  such as 0.3 and 0.7 cannot be represented exactly in the encoding.  Some
  calculations such as this one expose that quirk.
[^slope-mod]: We use `mean.default` because believe it or not the S3 dispatch
  around `mean` adds substantial overhead for this task.
[^reformulation]: The win was for a literal implementation of the slope formula,
  under a [reformulated version][106] as shown by [Michael Chirico][107]
  `data.table` is faster.
[^extended-2]: The extended precision digits are never actually directly
  accessible in R as they are truncated away before return from C code.  We
  exercise poetic license here for illustrative purposes.  Note the green and
  yellow here have no semantic relationship to the colors in the algorithm
  visualization plots.
[^precision-digits]: Double precision is roughly `log10(2^53)`, or 15.95
  in decimal, but precision can really only be measured in binary "digits", so
  you cannot directly compute how much precision is lost by comparing two
  decimal numbers.  This particular subtraction loses us 21 bits of precision,
  or about 6.32 digits of decimal precision.
[^mag-diff]: Adding numbers with low precision but very different magnitudes
  can create a bit pattern in the double representation that suggests high
  precision, but that is just an artifact of the computation.  The actual
  precision is no higher than the precision of the highest magnitude number.
[^mean-pathological]: Incidentally, this implies that you will get less
  precise answers if your vector is [sorted by value][19] instead of even just
  randomly.
[^mag-prec]: There is still the general order of magnitude left, and since this
  is a base 2 order of magnitude you should still be at +-50% accuracy (i.e.
  value is between x and 2x).
[^prec-mismatch]: Precision mismatch can arise when adding or subtracting
  numbers different magnitudes, different precisions, or some combination of the
  two.  Only the most significant digits of the larger magnitude number that are
  either larger than the largest digit of the other number or overlap with a
  "precise" digit of the smaller number survive.  As always all these
  determinations should be made on binary digits, although it should hold
  roughly true for decimal digits.
[^for-effect]: We exaggerated the errors for expository purposes.
[^full-prec-assumption]: This is assuming the entire precision of
  `x_ux2.cum.pre` and `x_ux2.cum` is real.  As you saw previously these involve
  numbers that are not [fully precise](#mu-prec), which may cause concern.
  However, what we actually care about is that the cumulative sum of these
  numbers is numerically precise, not semantically precise.
[^plus-minus]: 3 bits allow us to represent `$2^3 == 8$` values, so naively
  error spans `$1/8$`, or `$\pm1/16 == \pm6.25%$`.
[^fifty-three]: If you counted closely you would see we show 53 bits in the
  significand, not 52.  This is because in reality "doubles" only store the last
  52 bits of the significand.  The first unstored bit is always assumed to be 1,
  so 52 bits are encoded explicitly, and the additional 53rd implicitly.  For
  clarity we show that first bit explicitly.  There is an exception to the first
  bit assumption: when the exponent is all zeroes the first implicit bit will be
  assumed to be zero as well.  This corresponds to ["subnormal" numbers][111].
[^three-bits]: Internally the `B.s` will be stored with all 53 bits of
  precision, but the trailing 50 bits will be all zeroes and carry no real
  precision.
[^rel-err]: The relative error value will fluctuate depending on the value we
  are measuring it to.  Our reference value is in binary `1.01` (exp:
  `$2^{-32}`), but it could also have been `1.11` or `0.01`, so for 3 bit values
  the relative error can be as low as ~14% or as high as 100%.
[^by-hand]: since R doesn't support 80 bit doubles at the user level, we had to
  do the binary addition by hand...
[^ieee-754-approx]: We show the entire significand, including the implicit
  leading bit, and we show the de-biased exponent explicitly.  See the [IEEE-754
  wikipedia page for details][12].

[12]: https://en.wikipedia.org/wiki/Double-precision_floating-point_format
[13]: https://github.com/wch/r-source/blob/R-3-5-branch/src/main/cum.c#L30
[14]: https://en.wikipedia.org/wiki/Extended_precision#Working_range
[15]: https://github.com/romainfrancois
[16]: https://github.com/ThinkR-open/seven31
[17]: https://github.com/wrathematics/float
[18]: https://github.com/wch/r-source/blob/R-3-3-branch/src/main/summary.c#L434
[19]: https://twitter.com/BrodieGaslam/status/1113783101262585856
[20]: https://www.ibm.com/blogs/research/2018/12/8-bit-precision-training/

[100]: /2019/06/10/base-vs-data-table/
[101]: /2019/06/10/base-vs-data-table/#so-you-think-you-can-group-stat
[102]: /2019/02/24/a-strategy-for-faster-group-statisitics/#group-statistics-in-r
[104]: https://github.com/JohnMount
[105]: https://github.com/WinVector/FastBaseR/blob/f4d4236/R/cumsum.R#L105
[106]: /2019/06/10/base-vs-data-table/#reformulated-slope
[107]: https://twitter.com/michael_chirico
[108]: /2019/06/10/base-vs-data-table/#all-timings
[109]: /2019/06/10/base-vs-data-table/#group-meta-data
[110]: /2019/06/10/base-vs-data-table/#algo-visual
[111]: https://en.wikipedia.org/wiki/Denormal_number
