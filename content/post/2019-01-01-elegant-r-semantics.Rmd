---
title: Elegant R Semantics
author: ~
date: '2019-01-01'
contenttype: article
slug: elegant-r-semantics
draft: true
weight: 1
image: /front-img/default.png
imagemrgvt: "0%"
imagemrghz: "0%"
description: R on the surface looks like a C-style language, but it really is
  not.  I explore how the R semantics allow for wonderfully satisfying code.
categories: [r]
tags: []
---

```{r echo=FALSE}
options(digits=3)
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
```

# R Semantics are Wonderful

R catches a lot of flak because its idiosyncrasies can be confusing to the
uninitiated.  Some are genuine historic infelicities that would probably be
fixed were they not relied on by existing code.  Most of them are a
manifestation of the powerful philosophy underlying the language:

* Vector based data structures.
* Everything is data.
* Freedom.

Related John Chamber's "Everything that exists is an object, Everything that
happens is a function call" [user2014][].

On the surface R looks like a traditional C style language, but the first two
principles above make it something entirely different.  Freedom means there is a
bewildering number of ways you can accomplish any given task, many of which are
computationally inefficient.  All of this can make for a trying experience for
first-time users wanting to do some quick analysis.

A way to cope with this is to embrace frameworks such as the [Tidyverse][1] that
mask much of the apparent complexity of the language behind a veneer of
consistency.  This allows beginners to produce "useful" analysis faster[^2].
For many this will be the right trade-off, but it is a trade-off.  The primary
issue is that the Tidyverse runs in R, so it is inevitable that sooner or later
Tidyverse users will have to deal with R semantics.  This means learning two
sets of possibly conflicting semantics.

Another approach is to invest time and effort to understand the foundational
concepts behind R semantics.  This too is a trade-off as a basic understanding
of R semantics alone is insufficient to do "useful" analysis.  You will need to
build on those.  But the reward is to recognize the seeming bewildering
idiosyncrasies of R as the elegant interactions of the foundational concepts of
the language[^wtf].  The beauty of it is that the foundational concepts are
simple.

This blog post looks to highlight some of the interesting ways one can extend
the basic R principles into useful higher level calculations.  For a detailed
review of the principles please read the criminally under-rated [R Language
Definition][2].

# Vector Based Data Structures

The basic data structure in R is the vector:

```{r}
vec <- 1:12
vec
```

`vec` is stored in memory as twelve 32-bit[^3] contiguous sections of memory.

One of the key principles of R is that vectors can be interpreted as
complex data structures by attaching meta-data to them, without changing the
underlying vector.  This is a simple and elegant alternative to defining custom
C-level data structures.

We can turn our vector into an matrix by adding a 'dim' attribute[^4]:

```{r}
mx <- vec
attr(mx, 'dim') <- c(3, 4)
mx
```

Even though the underlying data is unchanged, its semantics change. R
comes with several built-in special attributes like 'dim' that induce
data-structure specific semantics.  Additionally the 'class' attribute can be
used in combination with generic functions to attach any interpretation
imaginable[^5] to data.

The mere addition of the 'dim' attribute allows magic like matrix multiplication
on what are really one dimensional vectors.  At the same time, R can re-use all
its internal C vector code for matrices for cases where the semantics are the
same as in simple arithmetic operations:

```{r}
vec + 1
c(mx + 1)   # `c` drops the dim attribute
```

And anyone aware of the underlying vector structure of matrices (and recycling
semantics) realizes that you can add vectors column-wise to matrices:

```{r}
mx + (1:3) * 100
```

Or row-wise with transposition or repetition:

```{r}
t(t(mx) + (1:4) * 100)
mx + rep(1:4 * 100, each=nrow(mx))
```

The "dual" nature of matrices and arrays[^6] provides many opportunities for
creative manipulation.

We have been looking primary at numeric vectors, but there are several others.
There are logical vectors, character vectors, and also list vectors:

```{r}
l <- list(1, 1:3, c('hello', 'world'), TRUE)
str(l)
```

But how can lists be vectors if vectors are supposed to be a sequence of
contiguous, equal-size, equal-type elements?  It is because the list proper is a
vector of pointers that link to the actual contents.  The pointers themselves
are equal size and equal type, even though the "contents" are not.  If this
seems odd, realize that character vectors are the same.  They are vectors of
pointers to strings which themselves are often different sizes.  This is
apparent if we inspect the inner structure of the list:

```{r eval=FALSE}
.Internal(inspect(l))
```
```{r echo=FALSE}
l.insp <- capture.output(.Internal(inspect(l)))
writeLines(l.insp)
```

`VECSXP`, `REALSXP`, `INTSXP`, and `STRXSP`, and `LGLSXP`  are the internal R
names of respectively list, numeric, integer, and character vectors.  The
numbers preceded by "@" represent memory locations.  So the actual contents of
the list `l` are the "@" values highlighted here:

```{r echo=FALSE, results='asis'}
l.insp <- sub("^  (@[0-9a-f]+) ", "  \033[32;7m\\1\033[m ", l.insp)
writeLines(
  paste0(
    "<pre><code>", paste0(fansi::sgr_to_html(l.insp), collapse="\n"),
    "</code></pre>"
) )
```

Notice how
our character vector, the `STRSXP` element, has child elements just like the
list `VECSXP`[^vecsxp] element.  These child elements are the `CHARSXP` objects,
which are not directly in R.

Since lists are vectors, we can also add dim attributes to them and get
meaningful results:

```{r}
set.seed(42)
l <- replicate(12, runif(10), simplify=FALSE)
dim(l) <- c(3, 4)
l
l[[3, 2]]
```

While list-matrices are a bit odd they do have uses.  The important point for
now is that we started with the simple concept of vectors and attributes, and
ended up with strange but useful list-matrices.  R allowed us to combine simple
concepts to create useful complexity.

Data frames are another complex structure derived from simpler ones by the
addition of meta data, in this case lists interpreted as tabular structures by
the addition of the "class" and "row.names" attributes.

```{r}
df <- data.frame(a=1:6, b=letters[1:6])
df
class(df)
str(unclass(df))
```

It used to be a source of consternation to me when I first picked
up R that `length` did not return the number of rows in the data frame:

```{r}
length(df)
nrow(df)
```

Since the date frame "container" is a list of the columns, it is natural from
R's perspective that `length` should return the number of columns.

Everything is not perfect though.  R's treatment of different data structures
is not seamless.  In order for functions to behave as expected they need to be
designed with the data structures they will operate on in mind.  The `length`
function does not do this [^length-rows]: it simply returns the length of the
underlying vector whether for atomic vectors, matrices, or list vectors.  The
`head` function on the other hand is aware of the different semantics of each
structure:

```{r}
head(l, 1)
head(mx, 1)
head(df, 1)
```

For both `mx` and `df`, `head` returned the first row instead of
the first element from the underlying structures.  There is no trivial way to
know which functions will treat objects as their meta-data augmented selves, or
as the underlying structure.  This can lead to surprises, but if you understand
what the underlying data structure is then you are prepared to recognize and
adapt to them when they happen.

# Everything is Data - Part I

Certainly this is true of anything that happens on a computer, but R embraces
this wholeheartedly in a way that many programming languages don't.  I still
recall my consternation when I first started using R and typed the following
into the terminal thinking I had a variable defined by that name:

```{r}
var    # my actual variable was var1
```

R happily went ahead and displayed the contents of the `var` function as it
would display the contents of any "normal" variable.  That functions were stored
in variables as any other data felt more like a curiosity or even an annoyance
than anything else:

```{r}
var(1:10)
variance <- var
var <- 1:10
variance(var)
```

This was before I fully appreciated that while R can behave like a traditional
imperative language, it has a strong functional bent.  For example, if we wish
to sort each element of a list `l`:

```{r}
l <- list(sample(5), 3:1)
```

We can use the `lapply` to apply a function to each element of the list:

```{r}
lapply(l, sort)
```

Each element of `l` was sorted with `sort`, and the results were collected into
a list.  This is no faster than and semantically equivalent to[^list-init]:

```{r}
l2 <- list()
for(i in seq_along(l)) l2[[i]] <- sort(l[[i]])
l2
```

But the simplicity and grace of the `lapply` version is undeniable.


```{r}
lapply(iris[1:4], sd)   # use the standard deviation statistic
```

This is the type of thing that is a real head scratcher if you think of data
frames as internally represented by tabular structures, but just right
if you realize they are just lists of atomic vectors.

<!-- Does this go elsewhere? Needs some serious copy edits -->

Complexity in R is mostly combinatorial: simple foundational concepts are
assembled into complex data structures and ideas.  It is limitless, but also
tractable because the building blocks are simple and well thought out.
Understanding the foundational concepts is the key to truly enjoying R.

Since functions are data it follows that you can manipulate functions to modify
their behavior just as you might change the value of "traditional" data:

```{r}
Negate(isTRUE)(FALSE)
```

And while functions[^fun-closure] are not strictly vectors, they can be made
into them to be manipulated as such.  More importantly, vectors can be turned
back into functions:

```{r}
var(c(1, 2, 3, NA))
var.list <- as.list(stats::var)
str(var.list)
```

The list form of the function has as named elements the parameter list, and the
last unnamed elements the code of the function.  We can then manipulate the list
as we would a data list, and turn it back into a function:

```{r}
var.list$na.rm <- TRUE  # change na.rm default to TRUE
var2 <- as.function(var.list, envir=environment(stats::var))
var2(c(1, 2, 3, NA))
```

Even the code of the function can be manipulated:

```{r}
var.list[[length(var.list)]][[2]] <- quote(stop("we broke the function"))
var3 <- as.function(var.list, envir=environment(stats::var))
var3  # notice the first line
var3(c(1, 2, 3, NA))
```

The examples here are not particularly useful on their own right other than to
illustrate the data-nature of code in R.  You can do some pretty remarkable
things by exploiting this aspect of R.  For example [`covr`][3] computes test
coverage by injecting instrumentation into package functions.  For more details
and examples of what you can do see the [RPN Parsing post][4].



```{r}
Reduce(setdiff, list(1:3, 2:5, 4:8))
```

* Reduce
* do.call
* outer


That was before I became


* Functions
* Language
* do.call
* Functional

# Freedom

* rpn calculator


I believe the latter approach is better if you intend to use R on a regular
basis.

And once
you fully grasp the key building blocks the freedom is exhilarating[^1].

It would take more than a blog post to teach the foundational blocks of R
semantics.  Instead, I will go through a few examples that illustrate how you
can build useful complexity out of simple in an effort to demonstrate that you
can do some pretty neat stuff once you truly understand the basics.

I'll illustrate in this post some of the semantics that I find particularly
satisfying.

# Vectors

# Concessions to Common Semantic Standards

Disguising things to look like other things: language like language, operators
like operators.


# Functions are Data

# Computing on the Language

RPN calculator.

```{r}
chr_to_name <- function(y)
  lapply(y, function(x) if(is.numeric(x)) x else as.name(x))

rpn <- function(...) {
  l <- chr_to_name(list(...))
  i <- 1
  while(length(l) >= i) {
    if(is.name(l[[i]])) {
      l[[i - 2]] <- as.call(l[i - c(0, 2, 1)])
      l[i:(i - 1)] <- NULL
      i <- i - 1
    } else {
      i <- i + 1
    }
  }
  l[[1]]
}


rpn <- function(...) {
  rpn_rec <- function(tl, hd=list())
    if(length(tl)) {
      hd <- if(is.numeric(tl[[1]])) c(hd, tl[1])
      else c(head(hd, -2), list(as.call(c(tl[1], tail(hd, 2)))))
      Recall(tl[-1], hd)
    } else hd[[1]]
  rpn_rec(chr_to_name(list(...)))
}


  rpn2 <- function(...) rpn_rec(list(), chr_to_name(list(...)))


rpn(3, 4, '+', 5, '*', pi, 2, '-', '/')
l <- list(3, 4, as.name('+'), 5, as.name('*'), pi, 2, as.name('-'), as.name('/'))

rpn(3, 4, '+', 5, '*')

rpn <- function(...) {
  l <- lapply(list(...), function(x) if(is.numeric(x)) x else as.name(x))
  for(i in seq(2, length(l), 1))
    if(!is.numeric(l[[i]])) l[[i]] <- as.call(l[i - 0:2])
  l[[length(l)]]
}
```

# Things that are Different Have The Same Semantics

# Limits

* Vectorization

# References

* R language definition
* Inferno

that's neither here
nor there.

What matters is that a few key decisions

Some of them are historical infelicities that
will forever remain etched in the foundations


One of the amazing things about R is how it is built on well thought-out basic
principles that extend elegantly.

# Vectors


# Matrices


# Lists

# List-Matrices

[^1]: Degree of exhilaration may vary.  Perhaps I don't get out much.
[^2]: My initial experience with R predates the tidyverse, so I don't have
  personal experience with this.  Nor have I done A/B testing with beginners
  that use the tidyverse vs. not, but I have no reason to doubt the assertion.
[^3]: As of this writing this is the case.
[^4]: A safer way to create matrices is with the `matrix` constructor, although
  ultimately what makes an R matrix a matrix is the presence of the 'dim'
  attribute.
[^5]: We won't cover the details here, but see the [objects chapter of the R
  Language Definition][2] for details.
[^6]: Arrays are matrices generalized to three or more dimensions.
[^7]: The `*apply` family of functions, `outer`.
[^wtf]: And yes, the occasional WTF.
[^list-init]: Better would be `l2 <- vector(type='list', length=2)`
[^fun-closure]: This only works for "closures", which are one of the types of
  functions in R.  For efficiency R implements different types of functions that
  are faster, but cannot be manipulated like closures can.
[^length-rows]: There is really no right answer for what the length of a two
  dimensional structure should be, so R returning the underlying length is as
  appropriate as anything else.  Since `length` is a generic function, it could
  in be extended to give different answers for different data structures.
[^vecsxp]: Vector lists are a newer (c.a. 1998) addition.  The list structure
  prior to that was a linked list, a.k.a. `LISTSXP` a.k.a. pairlist, so
  presumably that is why vector lists are called `VECSXP` internally since the
  `LISTSXP` name was taken.  `LISTSXP` are still used extensively in internal r
  code, but are mostly hidden from sight in regular use.


[1]: https://cran.r-project.org/doc/manuals/R-lang.html#Vector-objects
[2]: (https://cran.r-project.org/doc/manuals/R-lang.html#Object_002doriented-programming)
[3]: https://cran.r-project.org/package=covr
[4]: /2019/01/11/reverse-polish-notation-parsing-in-r/
[5]: http://user2014.r-project.org/files/chambers.pdf
[6]: http://www.win-vector.com/blog/2019/02/if-you-were-an-r-function-what-function-would-you-be/
