---
title: A Strategy for Faster Group Statisitics
author: ~
date: '2019-02-24'
slug: a-strategy-for-faster-group-statisitics
categories: []
tags: []
draft: true
image: /front-img/default.png
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: Front page summary
---

```{r echo=FALSE}
options(digits=3, datatable.print.topn=2)
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
library(ggplot2)
```
```{r, echo=FALSE}
writeFansi <- function(x) {
  writeLines(
    paste0(
      "<pre></pre><pre><code>",
      paste0(fansi::sgr_to_html(x), collapse="\n"),
      "</code></pre>"
  ) )
}
```

# Group Statics in R

<!-- this needs to become a shortcode -->
<img
  id='front-img' src='/front-img/default.png'
  class='post-inset-image'
/>

A well known limitation of R is that there is substantial overhead to evaluate R
expressions.  This is not as bad as it sounds as the key workhorse R functions
quickly hand off to pre-compiled routines, thereby avoiding this overhead.
As long as you avoid large numbers of calls to R functions as might happen in
R-level loops over long vectors, performance should be adequate.  Do note that
the `*pply` family[^for-ply] and similar[^purrr] functions are R-level loops.

Unfortunately there is one common use case where this limitation is
difficult to avoid: computation of group statistics on data sets with many
groups[^sac-on-so].  This will require an R function call for each group, <span
id='split-bad'></span>and the additional overhead of splitting the input such
that each group becomes its own R object that we can call the function on.

For example, in:

```{r}
x   <- c(1, 2, 3, 4, 6, 8)
grp <- rep(1:2, each=3)
tapply(x, grp, var)
```

`tapply` breaks up `x` according to the groups in `grp` and applies the `var`
function to each of the resulting pieces of `x`.  The result is a
vector[^tapply-return] with as many elements as there are groups and with the
groups as the names.  Here we illustrate with an example with 10 million values
and ~1 million groups[^exaggerate]:

```{r eval=FALSE}
RNGversion("3.5.2"); set.seed(42)
n     <- 1e7
n.grp <- 1e6
grp   <- sample(n.grp, n, replace=TRUE)
x     <- runif(n)

system.time(x.grp <- tapply(x, grp, var))
```
```
   user  system elapsed
 48.933   0.787  50.474
```

Compare with:

```{r eval=FALSE}
system.time(var(x))
```
```
   user  system elapsed
  0.061   0.001   0.062
```

**Note**: _Throughout this post I only show one run from `system.time`, but I
have run the timings several times to confirm they are stable, and I show
a representative run[^rep-bench]._

Even though both commands are running a similar number of computations in
compiled code[^num-calc], the version that calls the R level function once is
several orders of magnitude faster than the one that calls it for each group.

As alluded to [earlier](#split-bad) R call overhead is not the only difference
between the two examples. `tapply` must also split the input vector into groups.
To evaluate the relative cost of each task, we can separate the `tapply` call
into its `split` and `*pply` components[^tapply-approx].  `split` breaks up the
input vector into a list with one element for each group:

```{r eval=FALSE}
system.time(x.split <- split(x, grp))
```
```
   user  system elapsed
  7.124   0.617   7.966
```
```{r eval=FALSE}
str(x.split[1:3])   # show first 3 groups:
```
```
List of 3
 $ 1: num [1:11] 0.216 0.537 0.849 0.847 0.347 ...
 $ 2: num [1:7] 0.0724 0.1365 0.5364 0.0517 0.1887 ...
 $ 3: num [1:7] 0.842 0.323 0.325 0.49 0.342 ...
```

`vapply` then collapses each group into one number with the `var` statistic:
<span id='var-orig'></span>
```{r eval=FALSE}
system.time(x.grp.2 <- vapply(x.split, var, 0))
```
```
   user  system elapsed
 40.001   0.629  41.921
```
```{r eval=FALSE}
str(x.grp.2)
```
```
 Named num [1:999953] 0.079 0.0481 0.0715 0.0657 0.0435 ...
 - attr(*, "names")= chr [1:999953] "1" "2" "3" "4" ...
```
```{r eval=FALSE}
all.equal(c(x.grp), x.grp.2)
```
```
[1] TRUE
```

While the `var` computation accounts for the bulk  of the elapsed time, the
splitting can become significant with functions that are faster than
`var`[^var-speed].  For example, with `sum` which is one of the lowest overhead
statistics in R, the grouping becomes the limiting element:

```{r eval=FALSE}
system.time(vapply(x.split, sum, 0))
```
```
   user  system elapsed
  0.605   0.011   0.620
```

A side-by-side comparison of the two timings makes this obvious.  The "group"
timing is the same for both functions.

```{r base-times, echo=FALSE}
funs <- paste0("Function: ", c('var', 'sum'))
times <- data.frame(
  Function=rep(factor(funs, levels=funs), each=2),
  Step=factor(c('group', 'apply'), levels=c('group', 'apply')),
  time=c(7.966, 41.921, 7.966, 0.803)
)
ggplot(times, aes(x=Step, y=time)) +
  geom_col() +
  facet_wrap(~Function) +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

`tapply` is not the only function that computes group statistics in R, but it is
one of the simpler and faster ones.

# `data.table` to the Rescue

Thankfully the `data.table` package implements several optimizations to assist
with this problem.


```{r eval=FALSE}
library(data.table)  # 1.12.0
setDTthreads(1)      # turn off data.table parallelization
```

**Note**: _I turned off parallelization for `data.table` as it did not make a
significance difference in this particular example, and it muddied the
benchmarks[^parallel]_.

```{r eval=FALSE}
DT <- data.table(grp, x)
DT
```
```
             grp     x
       1: 914807 0.116
       2: 937076 0.734
      ---
 9999999: 714619 0.547
10000000: 361639 0.442
```

In order to compute a group statistic with `data.table` we use:

```{r eval=FALSE}
DT[, var(x), keyby=grp]
```
```
            grp     V1
     1:       1 0.0790
     2:       2 0.0481
    ---
999952:  999999 0.1707
999953: 1000000 0.0667
```

We use `keyby` instead of the traditional `by` because it instructs `data.table`
to group _and_ to order the return table by the groups, which matches what
`tapply` does[^dt-group].  Timings are a different story:
<span id='dt-var'></span>

```{r eval=FALSE}
system.time(x.grp.dt <- DT[, var(x), keyby=grp][['V1']])
```
```
   user  system elapsed
  1.821   0.062   1.909
```
```{r eval=FALSE}
all.equal(c(x.grp), x.grp.dt, check.attributes=FALSE)
```
```
[1] TRUE
```

We've reproduced the same results at ~25x the speed.

It is not possible to separate out the statistic computation cleanly from the
grouping in `data.table`[^dt-verbose], but we can get a rough idea by setting a
key on `grp`.  This orders the table by the group, which is most of the overhead
of grouping.

```{r eval=FALSE}
setkey(DT, grp)   # grouping takes advantage of this key
DT                # notice the table is now ordered by group
```
```
              grp     x
       1:       1 0.216
       2:       1 0.537
      ---
 9999999: 1000000 0.180
10000000: 1000000 0.919
```

What remains is mostly the statistic computation:

```{r eval=FALSE}
system.time(DT[, var(x), keyby=grp])
```
```
   user  system elapsed
  0.458   0.017   0.477
```

This is close to two orders of magnitude faster than with [`vapply`](#var-orig):

```{r var-dt, echo=FALSE}
times <- data.frame(
  Function=rep(c('base - var', 'data.table - var'), each=2),
  Step=factor(c('group', 'apply'), levels=c('group', 'apply')),
  time=c(7.966, 41.921, 1.909 - .477, .477)
)
ggplot(times, aes(x=Step, y=time)) +
  geom_col() +
  facet_wrap(~Function) +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

# What is this Sorcery?

How is it that `data.table` runs the R `var` function so much faster than
`vapply`?  `vapply` adds some overhead, but not much as shown when run with the
essentially NULL-op unary `+`[^null-op]:

```{r eval=FALSE}
system.time(vapply(1:1e6, `+`, 0L))
```
```
   user  system elapsed
  0.358   0.006   0.365
```

It turns out we are being lied to: `data.table` does not call the R `var`
function ~1 million times.  Instead it intercepts the call `var(x)` and
substitutes it with a compiled code routine that computes all the groups without
intervening R-level evaluations. The `data.table` authors call this "GForce"
optimization[^gforce].

The substitution is only possible if there is a pre-existing compiled code
equivalent for the R-level function.  `data.table` ships with such
equivalents for the "base" R[^base-r] functions `min`, `max`,
`mean`[^mean-diff], `median`, `prod`, `sum`, `sd`, `var`, `head`, `tail`, and
`[`[^subset-gforce] as well as for the `data.table` functions `first` and
`last`.

Additionally, the substitution only happens with the simplest of calls.  Adding
a mere unary `+` call causes it to fail:

```{r eval=FALSE}
system.time(DT[, +var(x), keyby=grp])
```
```
   user  system elapsed
 42.564   0.628  45.394
```

If you need a different function or a non-trivial expression you'll have be
content with falling back to standard evaluation.  You can check whether
`data.table` uses it by setting the verbose option[^dt-verbose]:

```{r eval=FALSE}
options(datatable.verbose=TRUE)
DT[, var(x), keyby=grp]
```
```{r echo=FALSE, results='asis'}
writeFansi(
"Detected that j uses these columns: x
Finding groups using uniqlist on key ... 0.113s elapsed (0.045s cpu)
Finding group sizes from the positions (can be avoided to save RAM) ... 0.020s elapsed (0.012s cpu)
lapply optimization is on, j unchanged as 'var(x)'
GForce \033[43moptimized j to 'gvar(x)'\033[m
Making each group and running j (\033[43mGForce TRUE\033[m) ..."
)
```
```{r eval=FALSE}
DT[, +var(x), keyby=grp]
```
```{r echo=FALSE, results='asis'}
writeFansi(
"Detected that j uses these columns: x
Finding groups using uniqlist on key ... 0.034s elapsed (0.029s cpu)
Finding group sizes from the positions (can be avoided to save RAM) ... 0.009s elapsed (0.009s cpu)
lapply optimization is on, j unchanged as '+var(x)'
GForce is on, \033[43mleft j unchanged\033[m
Old mean optimization is on, left j unchanged.
Making each group and running j (\033[43mGForce FALSE\033[m)")
```

# Blood From a Turnip

Imagine we wish to compute the slope of a bi-variate least squares regression.
The formula is:

$$\frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i -
\bar{x})^{2}}$$

The R equivalent is:<span id='slope-ex'></a>

```{r eval=FALSE}
slope <- function(x, y) {
  x_ux <- x - mean(x)
  uy <- mean(y)
  sum(x_ux * (y - uy)) / sum(x_ux ^ 2)
}
```

In order to use this function with our `split`/`*pply` paradigm we need a little
additional manipulation:

```{r eval=FALSE}
system.time({
  id <- seq_along(grp)
  id.split <- split(id, grp)
  res.slope.base <- vapply(id.split, function(id) slope(x[id], y[id]), 0)
})
```
```
   user  system elapsed
 17.745   0.318  18.453
```

Instead of splitting a single variable, we split an index that we then use to
subset each variable.

There is no such additional complexity with `data.table`:

```{r eval=FALSE}
y <- runif(n)
DT <- data.table(grp, x, y)
system.time(res.slope <- DT[, slope(x, y), keyby=grp])
```
```
   user  system elapsed
 11.501   0.136  11.679
```

The timings improve over base, but not by the margins we saw previously.  This
is because `data.table` cannot use the `sum` and `mean` "GForce"
counterparts when they are inside another function, or even when they are part
of complex expressions.

So, what are we to do?  `data.table` is the speed demon in this area, so short
of writing custom compiled code ourselves, are we out of luck?

Fortunately for us there is one last resort: with a little work we can break up
the computation into pieces and take advantage of "GForce".  First we are going
to go through a version of this that doesn't quite work, but has the benefit of
exposing the strategy more clearly.

We start by computing the `$\bar{x}$` and `$\bar{y}$`:

```{r eval=FALSE}
DT <- data.table(grp, x, y)
DT[, `:=`(ux=mean(x), uy=mean(y)), keyby=grp]
DT
```
```
              grp     x     y    ux    uy
       1:       1 0.216 0.950 0.551 0.444
       2:       1 0.537 0.914 0.551 0.444
      ---
 9999999: 1000000 0.180 0.589 0.548 0.578
10000000: 1000000 0.919 0.914 0.548 0.578
```

The <code>&#96;:=&#96;(...)</code> call adds columns to the data
table with the results of the computations, recycling the group mean values for
each element of the group.  With `$\bar{x}$` and `$\bar{y}$` available the
remaining calculations are straightforward:

```{r eval=FALSE}
DT[, `:=`(x_ux=x - ux, y_uy=y - uy)]
DT[, `:=`(x_ux.y_uy=x_ux * y_uy, x_ux2=x_ux^2)]
DTsum <- DT[, .(x_ux.y_uy=sum(x_ux.y_uy), x_ux2=sum(x_ux2)), keyby=grp]
res.slope.dt1 <- DTsum[, .(grp, x_ux.y_uy/x_ux2)]
res.slope.dt1
```
```
            grp     V2
     1:       1 -0.706
     2:       2  0.134
    ---
999952:  999999  0.461
999953: 1000000  0.171
```

I structured the calculations so that the steps that involve `keyby=grp` use
only simple calls to "GForce" supported functions.  The other steps use the
operators `*`, `-`, `/`, and `^`, which have the property that **results are the
same whether computed with or without groups**.  It follows that the lack of
"GForce" counterparts for the operators is a non-issue.

This produces the same result:

```{r eval=FALSE}
all.equal(res.slope.base, res.slope.dt1, check.attributes=FALSE)
```
```
[1] TRUE
```

As I mentioned previously this doesn't quite work. [`data.table` does not
currently implement "GForce" within `:=`][12], so the first `keyby` call does
not benefit from it:

```{r eval=FALSE}
DT[, `:=`(ux=mean(x), uy=mean(y)), keyby=grp]
```

There is still a small performance gain, but it is not inherent to this
method[^false-gain]:

```{r eval=FALSE}
DT <- data.table(grp, x, y)
system.time({
  DT[, `:=`(ux=mean(x), uy=mean(y)), keyby=grp]
  DT[, `:=`(x_ux=x - ux, y_uy=y - uy)]
  DT[, `:=`(x_ux.y_uy=x_ux * y_uy, x_ux2=x_ux^2)]
  DTsum <- DT[, .(x_ux.y_uy=sum(x_ux.y_uy), x_ux2=sum(x_ux2)), keyby=grp]
  res.slope.dt2 <- DTsum[, .(grp, x_ux.y_uy/x_ux2)]
})
```
```
   user  system elapsed
  7.749   0.335   8.143
```

There is a reasonable work-around for this issue: first compute `$\bar{x}$` and
`$\bar{y}$` values into a separate table, which will use "GForce":

```{r eval=FALSE}
DT <- data.table(grp, x, y)
setkey(DT, grp)
DTsum <- DT[, .(ux=mean(x), uy=mean(y)), keyby=grp]
```

Then we compute the `$(x - \bar{x})$` and `$(y - \bar{y})$` values by joining
(a.k.a merging) our original table to the summary table with the `$\bar{x}$` and
`$\bar{y}$` values.  In `data.table` this is done by subsetting for rows with
another `data.table`[^dt-merge]:

```{r eval=FALSE}
DT[DTsum, `:=`(x_ux=x - ux, y_uy=y - uy)]
DT
```
```
              grp     x     y    x_ux   y_uy
       1:       1 0.216 0.950 -0.3353 0.5058
       2:       1 0.537 0.914 -0.0146 0.4697
      ---
 9999999: 1000000 0.180 0.589 -0.3681 0.0109
10000000: 1000000 0.919 0.914  0.3711 0.3359
```

If you look back you can see that we used `setkey` right before computing
`DTsum`[^setkey-cost].  We did this for two reasons:

1. `setkey` tells `data.table` what column to join on when we subset with
   another `data.table` as above, and it makes the join fast.
2. Keys can also be used to make grouping faster, so by setting the key early we
   used it both for the group and the join.

From this point we can resume the same logic as before:

```{r eval=FALSE}
DT[, `:=`(x_ux.y_uy=x_ux * y_uy, x_ux2=x_ux^2)]
DTsum <- DT[, .(x_ux.y_uy=sum(x_ux.y_uy), x_ux2=sum(x_ux2)), keyby=grp]
res.slope.dt3 <- DTsum[, .(grp, V1=x_ux.y_uy / x_ux2)]
all.equal(res.slope.dt1, res.slope.dt3, check.attributes=FALSE)
```
```
[1] TRUE
```

Let's time the whole thing:

```{r eval=FALSE}
DT <- data.table(grp, x, y)
system.time({
  setkey(DT, grp)
  DTsum <- DT[, .(ux=mean(x), uy=mean(y)), keyby=grp]
  DT[DTsum, `:=`(x_ux=x - ux, y_uy=y - uy)]
  DT[, `:=`(x_ux.y_uy=x_ux * y_uy, x_ux2=x_ux^2)]
  DTsum <- DT[, .(x_ux.y_uy=sum(x_ux.y_uy), x_ux2=sum(x_ux2)), keyby=grp]
  res.slope.dt3 <- DTsum[, .(grp, V1=x_ux.y_uy / x_ux2)]
})
```
```
   user  system elapsed
  3.278   0.403   3.697
```

Now we're talking: a ~3x improvement over the original.

```{r echo=FALSE}
times <- data.frame(
  Function=c('base - slope', 'data.table - slope', 'data.table - slope'),
  Version=c('normal', 'normal', 'optim'),
  time=c(18.453, 11.679, 3.697)
)
ggplot(times, aes(x=Version, y=time)) +
  geom_col() +
  facet_grid(.~Function, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

With a little extra work we are able to get "GForce"-like performance in an
application were at first blush one might assume it is not possible.
Should `data.table` add "GForce" support for `:=` I estimate the timings could
get down to 2.5 seconds or less[^no-setkey].  Hopefully the authors will see fit
to implement this feature.

# What About `dplyr`?

Like `data.table`, `dplyr` implements a special evaluation like "GForce".
`dplyr` calls this "Hybrid Evaluation"[^hybrid-eval].  For slow functions like
`var`[^var-speed] this leads to substantial performance improvements over the
base solution:

```{r eval=FALSE}
library(dplyr)
TB <- tibble(grp, x)
system.time(TB %>% group_by(grp) %>% summarize(var(x)))
```
```
   user  system elapsed
  9.823   0.609  10.639
```

Performance-wise the "Hybrid Evaluation" component is comparable to
`data.table`'s "Gforce" and is in mostly responsible for the improvements over
base's performance:

```{r eval=FALSE}
TB.g <- TB %>% group_by(grp)
system.time(TB.g %>% summarize(var(x)))
```
```
   user  system elapsed
  0.521   0.003   0.527
```

On the whole `dplyr` underperforms `data.table` because its grouping step is
slower.

```{r var-dt-dplyr, echo=FALSE}
times <- data.frame(
  Function=rep(c('base - var', 'data.table - var', 'dplyr-var'), each=2),
  Step=factor(c('group', 'apply'), levels=c('group', 'apply')),
  time=c(7.966, 41.921, 1.909 - .477, .477, 10.693 - 0.527, 0.527)
)
ggplot(times, aes(x=Step, y=time)) +
  geom_col() +
  facet_wrap(~Function) +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

Since the `dplyr` "Hybrid Evaluation" is effective, we should be able to apply
the same strategy as with `data.table` to optimize the computation of `slope`.
Unfortunately it is an uphill battle as shown by the times of the baseline run:

```{r eval=FALSE}
TB <- tibble(grp, x, y)
system.time(res.dplyr1 <- TB %>% group_by(grp) %>% summarise(slope(x, y)))
```
```
   user  system elapsed
  32.65    1.39   34.51
```

With no attempted optimizations `dplyr` is ~3x slower than the equivalent
`data.table` call and close to ~2x slower than the base version.

Using the same methodology that we optimized `data.table` with we get some
improvement, but we are still slower than the base method:

```{r eval=FALSE}
system.time({
  TB.s <- TB %>% group_by(grp) %>% summarize(ux=mean(x), uy=mean(y))
  res.dplyr2 <- TB %>%
    inner_join(TB.s, by='grp') %>%
    mutate(x_ux=x-ux, y_uy=y-uy, x_ux.y_uy=x_ux*y_uy, x_ux2=x_ux^2) %>%
    group_by(grp) %>%
    summarize(x_ux.y_uy=sum(x_ux.y_uy), x_ux2=sum(x_ux2)) %>%
    ungroup %>%
    mutate(slope=x_ux.y_uy / x_ux2) %>%
    select(grp, slope)
})
```
```
   user  system elapsed
  26.07    1.45   28.34
```

Mostly this is because we need to `group_by` twice, and also because the join
step is slower than in `data.table`.  We can avoid the join by using `mutate`
instead of `summarise` for the first grouped calculation but it appears "Hybrid
Eval" is not available or is ineffective for [`mean` within
`mutate`](#mutate-dplyr-optimization), similar to how "GForce" is not available
within `:=`.

And to confirm we're doing the same thing:

```{r eval=FALSE}
all.equal(res.slope.base, res.dplyr2[[2]], check.attributes=FALSE)
```
```
[1] TRUE
```

A summary of all the timings:

```{r slope-all-times, echo=FALSE}
times <- data.frame(
  Function=c(
    "base - slope", "data.table - slope", "data.table - slope",
    "dplyr - slope", "dplyr - slope"
  ),
  Version=c('normal', 'normal', 'optim', 'normal', 'optim'),
  time=c(18.453, 11.679, 3.679, 34.51, 28.34)
)
ggplot(times, aes(x=Version, y=time)) +
  geom_col() +
  facet_grid(.~Function, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

Certainly this test with many groups and few values per group punishes
systems that manage groups and joins with any inefficiency.  The `data.table`
team has produced a [broader set of benchmarks][6] that you can refer to for
some idea of performance for `dplyr` and `data.table` under different
circumstances.

# Conclusions

The optimization we reviewed in this post is not intended for day to day usage.
It requires a fair bit of work to get right, and 2-3x improvements over baseline
results may not always warrant the effort.  In a context where the manipulation
is written once and used repeatedly as in a package or a production
process, it could be worthwhile.

I hope that the `data.table` authors will expand the "GForce" functionality to
`:=`, and eventually to more complex expressions.  I recognize though that the
latter will require a substantial re-working of the engine, and in the meantime
we have the strategy described here to hold us over.

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id='feedback-cont'></div>

# Appendix

## Acknowledgments

* [Michael Chirico][14] for pointing me to the ["GForce" in `:=` issue][12].
* [Matt Dowle][17] and the other [`data.table` authors][15] for the fastest
  data-munging package in R.
* [Hadley Wickham][18] and the [`ggplot2` authors][19] for `ggplot2` with which
  I made the plots in this post.
* [Hadley Wickham][18] and the [`dplyr` authors][16] for `dplyr`.

## Exposed Mean Optimization

Prior to the implementation of "GForce", the `data.table` team introduced an
optimization for `mean` that can still be used in cases where "GForce" cannot.
For example, in:

```{r eval=FALSE}
DT[, +mean(x), keyby=grp]
```

The original optimization is used since "GForce" is not available.  There are
still ~1 million R-level calls, but instead of calling the relatively slow R
`mean` function, `data.table` uses a faster version.  `mean`, unlike `sum`, is
an S3 generic closure, which adds R-level S3 dispatch that is much slower for
when not actually needed than the corresponding compiled code level S3 dispatch.

Going back to the [`slope` example](#slope-ex), we can get some of the
performance benefits by exposing the body of the function so that `data.table`
can replace the `mean` calls with the faster version:

```{r eval=FALSE}
DT <- data.table(grp, x, y)
system.time(
  res.slope.5 <-DT[,
    {
      x_ux <- x - mean(x)
      uy <- mean(y)
      sum(x_ux * (y - uy)) / sum(x_ux ^ 2)
    },
    keyby=grp
  ]
)
```
```
   user  system elapsed
  7.983   0.067   8.089
```

## Mutate `dplyr` Optimization

Forsaking "Hybrid Eval" for the first `mutate` in order to avoid the join
makes things a little slower:

```{r eval=FALSE}
system.time({
  res.dplyr3 <- TB %>%
    group_by(grp) %>%
    mutate(ux=mean(x), uy=mean(y)) %>%
    ungroup %>%
    mutate(x_ux=x - ux, y_uy=y - uy, x_ux.y_uy = x_ux * y_uy, x_ux2 = x_ux^2) %>%
    group_by(grp) %>%
    summarize(sx_ux.y_uy=sum(x_ux.y_uy), sx_ux2=sum(x_ux2)) %>%
    ungroup %>%
    mutate(slope= sx_ux.y_uy / sx_ux2) %>%
    select(grp, slope)
})
```
```
   user  system elapsed
  30.00    1.03   31.71
```


[^gforce]: See `?'datatable-optimize'` for additional details.
[^hybrid-eval]: Hybrid eval has evolved a bit between the 0.7.x and 0.8.x
releases.  The [0.8.0 release notes][7] has good details, as well
as the currently unreleased [hybrid evaluation vignette][3].  The [0.7.3
vignette][2] has some historical context, and highlights that the original
hybrid eval could handle more complex expressions, but that was dropped because
the performance improvements did not warrant the additional complexity.
[^for-ply]: Even though there is much ado about the differences between explicit
loops such as `for` and `while`, vs implicit ones vs `*pply`, from a performance
perspective they are essentially the same so long as the explicit loops
pre-allocate the result vector (e.g. `res <- numeric(100); for(i in
seq_len(100)) res[i] <- f(i)`.
[^purrr]: There are many ways to iteratively call R functions in R, including
third party libraries like [purrr][4].
[^vec-size]: Numeric vectors require 8 bytes per element plus some overhead for
the object meta data.
[^num-calc]: The grouped calculation will require `$3(g - 1)$` more divisions
than the non-grouped version, where `$g$` is the number of groups.  However,
since there are over 60 arithmetic operations involved in a `$n = 10$` group the
overall additional time due to the extra calculations will not dominate even
with the relatively expensive division operation.
[^tapply-approx]: This approximation is semantically "close enough" for the
simple example shown here, but will not generalize for more complex inputs.
[^exaggerate]: Using a large number of small groups is designed to exaggerate
the computational issues in this type of calculation.
[^sac-on-so]: Anyone who has spent any time answering R tagged questions on
Stack Overflow can attest that computing statistics on groups is probably the
single most common question people ask.
[^parallel]: In particular, we ended up with benchmarks that had `user` times
greater than `ellapsed` due to the multi-core processing.  Anecdotally on my
two core system the benchmarks were roughly 20-30% faster with parallelization
on.  It is worth noting that parallelization is reasonably effective for the
`gforce` step where it close to doubles the processing speed.
[^dplyr-vs-dt]: Four years ago I did compare [dplyr vs. data.table][5] in split
apply combine analysis, and while the narrow results here are in line with what
I observed than, much has changed for both packages so I do not want to
generalize without further testing.
[^null-op]: Even the unary `+` operator will have some overhead, so not all that
time is attributable to `vapply`.
[^benchmarks]: The `data.table` team has run [a broad set of benchmarks][6]
across multiple different data munging applications.
[^mean-diff]: "GForce" `mean` uses the simpler [`sum(x) / length(x)`][10]
calculation, instead of the more precise mean algorithm used by [base R][9].
`dplyr` also uses the [more precise algorithm][8].  You can reduce the
optimization level on `data.table` to use a version of `mean` that aligns with
the base version, but that version does not use "GForce" so is substantially
slower, although still faster than the base `mean` function.
[^base-r]: By "base" R I mean the packages that are loaded by default in a clean
R session, including `base`, `stats`, etc.
[^early-dplyr]: Earlier versions of [hybrid evaluation][2] were more capable
able to fold hybrid evaluated calls into the expression, but that was abandoned
in 0.8.0 due to a poor complexity to performance trade-off.
[^sum-vs-mean]: The astute reader might notice that `sum(x) / length(x)` is a
fair bit faster than `mean(x)`.  Mostly this is because `mean` is an S3 generic
and the dispatched function has a fair bit more R code than `sum`.  Additionally
the `mean` algorithm includes a precision improvement pass, but that is a small
part of the overall cost.
[^dt-verbose]: Setting `options(datatable.verbose=TRUE)` will actually return
this information, but unfortunately in my single threaded testing it seemed to
also affect the timings, so I do not rely on it.
[^dt-only]: While in theory similar optimizations can be done with `dplyr`, we
found that the additional overhead of `dplyr` defeated the optimization.  If you
are looking for additional speed you should consider just switching to
`data.table`.
[^var-speed]: `var` is particularly slow in R3.5.2 due to the [`stopifnot` calls
that it uses][11].  It should [become faster in R3.6.0][13].
[^tapply-return]: The return value is actually an array, although in this case
it only has one dimension.
[^rep-bench]: Normally I would let the knitting process run the benchmarks, but
it becomes awkward with relatively slow benchmarks like those in this post.
[^false-gain]: This is somewhat misleading as the performance comes primarily
from a [lesser optimization of `mean`](#exposed-mean-optimization) that becomes
available when `data.table` can "see" the mean call as it can here.   We could
have written our initial slope calculation outside of a function body to get the
same effect.  We're eliding this detail for the sake of the narrative arc.
[^subset-gforce]: For single constant indices e.g. `x[3]` but not `x[1:3]` or
`x[y]`.
[^dt-group]: For more details see the ["Aggregation" section of that `data.table`
intro vignette][20].
[^setkey-cost]: One drawback of using `setkey` is that it sorts the entire
table, which is potentially costly if you have many columns.  You should
consider subsetting down to the required columns before you use `setkey`.
Another option if you are not looking to join tables is to use `setindex`.
[^no-setkey]: Another benefit of "GForce" in `:=` is that we can then use
`setindex` instead of `setkey` to accelerate the group computations.  The group
computation will be a little slower with `setindex`, but `setkey` is
substantially slower than `setindex`, so on the whole this should lead to a
performance improvement.
[^dt-merge]: `merge.data.table` just uses `[.data.table` internally, so there is
no real benefit other than an interface familiar to those that use
`base::merge`.

[1]: https://stackoverflow.com/a/29806540/2725969
[2]: https://s3.amazonaws.com/assets.rdocumentation.org/rpackages/unarchived/dplyr/0.7.3/vignettes/hybrid-evaluation.html
[3]: https://github.com/tidyverse/dplyr/blob/235d07643c0b82862a50f9459124694471a31076/vignettes/future/dplyr_0.8.0_new_hybrid.Rmd
[4]: https://cran.r-project.org/web/packages/purrr/index.html
[5]: /content/post/2014/04/18/datatable-vs-dplyr-in-split-apply-comgine/
[6]: https://h2oai.github.io/db-benchmark/groupby.html
[7]: https://www.tidyverse.org/articles/2018/12/dplyr-0-8-0-release-candidate/#redesigned-hybrid-evaluation
[8]: https://github.com/tidyverse/dplyr/blob/v0.8.0.1/inst/include/dplyr/hybrid/scalar_result/mean_sd_var.h#L122
[9]: https://github.com/wch/r-source/blob/tags/R-3-5-2/src/main/summary.c#L485
[10]: https://github.com/Rdatatable/data.table/blob/1.12.0/src/gsumm.c#L460
[11]: https://github.com/HenrikBengtsson/Wishlist-for-R/issues/70
[12]: https://github.com/Rdatatable/data.table/issues/1414
[13]: https://stat.ethz.ch/pipermail/r-devel/2019-March/077449.html
[14]: https://twitter.com/michael_chirico/status/1099807071363452928
[15]: https://cran.r-project.org/web/packages/data.table/index.html
[16]: https://cran.r-project.org/web/packages/dplyr/index.html
[17]: https://github.com/mattdowle
[18]: https://github.com/hadley
[19]: https://cran.r-project.org/web/packages/ggplot2/index.html
[20]: https://cloud.r-project.org/web/packages/data.table/vignettes/datatable-intro.html
