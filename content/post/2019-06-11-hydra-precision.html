---
title: "Hydra Chronicles, Part III: Catastrophic Imprecision"
author: ~
date: '2019-06-18'
slug: hydra-precision
categories: [r]
tags: [optimization]
image: /post/2019-06-11-hydra-precision_files/user-imgs/mtcars2.png
imagerect: /post/2019-06-11-hydra-precision_files/user-imgs/iris.png
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: "Overcoming floating point precision problems in a
group-stats rematch with `data.table`."
---



<STYLE type='text/css' scoped>
PRE.fansi SPAN {padding-top: .25em; padding-bottom: .25em};
</STYLE>
<div id="recap" class="section level1">
<h1>Recap</h1>
<figure class="post-inset-image">
<div class="post-inset-image-frame">
<img
  id='front-img' 
  src='/post/2019-06-11-hydra-precision_files/user-imgs/mtcars2.png'
  class='post-inset-image'
/>
</div>
<figcaption>
<code>mtcars</code> in binary.
</figcaption>
</figure>
<p>Last week we <a href="/2019/06/10/base-vs-data-table/">slugged it out</a> with the reigning group-stats heavy weight
champ <code>data.table</code>. The challenge was to compute the slope of a bivariate
regression fit line over 10MM entries and ~1MM groups. The formula is:</p>
<p><span class="math display">\[\frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^{2}}\]</span></p>
<p>While this may seem like a silly task, we do it because computing group
statistics is both useful and a <a href="/2019/02/24/a-strategy-for-faster-group-statisitics/#group-statistics-in-r">weakness in R</a>. We implemented
<a href="/2019/06/10/base-vs-data-table/#so-you-think-you-can-group-stat"><code>group_slope</code></a> based on <a href="https://github.com/JohnMount">John Mount’s</a> <a href="https://github.com/WinVector/FastBaseR/blob/f4d4236/R/cumsum.R#L105"><code>cumsum</code> idea</a>, and for
a brief moment we thought we scored an improbable win over
<code>data.table</code><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Unfortunately our results did not withstand
scrutiny.</p>
<p>Here is the data we used:</p>
<pre class="r"><code>RNGversion(&quot;3.5.2&quot;); set.seed(42)
n     &lt;- 1e7
n.grp &lt;- 1e6
grp   &lt;- sample(n.grp, n, replace=TRUE)
noise &lt;- rep(c(.001, -.001), n/2)  # more on this later
x     &lt;- runif(n) + noise
y     &lt;- runif(n) + noise          # we&#39;ll use this later</code></pre>
<p>The slow and steady base R approach is to define the statistic as a function,
<code>split</code> the data, and apply the function with <code>vapply</code> or similar.</p>
<pre class="r"><code>slope &lt;- function(x, y) {
  x_ux &lt;- x - mean.default(x)
  y_uy &lt;- y - mean.default(y)
  sum(x_ux * y_uy) / sum(x_ux ^ 2)
}
id &lt;- seq_along(grp)
id.split &lt;- split(id, grp)
slope.ply &lt;- vapply(id.split, function(id) slope(x[id], y[id]), 0)</code></pre>
<p>Our <a href="/2019/06/10/base-vs-data-table/#so-you-think-you-can-group-stat">wild child version</a> is <a href="/2019/06/10/base-vs-data-table/#all-timings">4-6x</a> faster, and should produce the
same results. It doesn’t quite:</p>
<pre class="r"><code>slope.gs &lt;- group_slope(x, y, grp)  # our new method
all.equal(slope.gs, slope.ply)</code></pre>
<pre><code>[1] &quot;Mean relative difference: 0.0001161377&quot;</code></pre>
<p>With a generous tolerance we find equality:</p>
<pre class="r"><code>all.equal(slope.gs, slope.ply, tolerance=2e-3)</code></pre>
<pre><code>[1] TRUE</code></pre>
<blockquote>
<p><strong>Disclaimer</strong>: I have no special knowledge of floating point precision
issues. Everything I know I learned from research and experimentation while
writing this pots. If your billion dollar Mars lander burns up on entry
because you relied on information in this post, you only have yourself
to blame.</p>
</blockquote>
</div>
<div id="oh-the-horror" class="section level1">
<h1>Oh the Horror!</h1>
<p>As we’ve alluded to previously <code>group_slope</code> is running into precision issues,
but we’d like to figure out exactly what’s going wrong. Let’s find the group
with the worst relative error, which for convenience we’ll call <code>B</code>. Its index
in the group list is then <code>B.gi</code>:<span id="prec-error"></span></p>
<pre class="r"><code>B.gi &lt;- which.max(abs(slope.gs / slope.ply))
slope.gs[B.gi]</code></pre>
<pre><code> 616826
-3014.2</code></pre>
<pre class="r"><code>slope.ply[B.gi]</code></pre>
<pre><code>   616826
-2977.281</code></pre>
<p><span id="obs-err"></span>That’s a ~1.2% error, which for computations is
downright ghastly, and on an entirely different plane of existence than the
comparatively quaint<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>:</p>
<pre class="r"><code>1 - 0.7 == 0.3</code></pre>
<pre><code>[1] FALSE</code></pre>
<p>Let’s look at the values in our problem group <code>616826</code>. It turns out there are
only two of them:</p>
<pre class="r"><code>x[grp == 616826]</code></pre>
<pre><code>[1] 0.4229786 0.4229543</code></pre>
<pre class="r"><code>y[grp == 616826]</code></pre>
<pre><code>[1] 0.7637899 0.8360645</code></pre>
<p>The <code>x</code> values are close to each other, but well within the resolution of the
<a href="https://en.wikipedia.org/wiki/Double-precision_floating-point_format">double precision format</a> (doubles henceforth), which are what R “numeric”
values are stored as. It gets a bit worse though because the next step is
<code>$\sum(x - \bar{x})^2$</code>:</p>
<pre class="r"><code>(B.s &lt;- sum((x[grp == 616826] - mean(x[grp == 616826]))^2))</code></pre>
<pre><code>[1] 0.0000000002946472</code></pre>
<p>Still, nothing too crazy. In order to see why things go pear shaped we need to
look back at the algorithm we use in <code>group_slope</code>. Remember that we’re doing
all of this in vectorized code, so we don’t have the luxury of using things like
<code>sum(x[grp == 616826])</code> where we specify groups explicitly to sum. Instead, we
use <code>cumsum</code> on group ordered data. Here is a visual recap of the key steps
(these are steps 4 and 5 of the <a href="/2019/06/10/base-vs-data-table/#algo-visual">previously described algorithm</a>):</p>
<p><img src="/post/2019-06-11-hydra-precision_files/figure-html/cumsum-review-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We use <code>cumsum</code> and some clever indexing to compute the values of <code>A.n</code>
(total cumulative sum of our group ordered statistic up to the last value before
our group <code>B</code>) and <code>B.n</code> (same quantity, but up to the last value in our group
<code>B</code>):</p>
<pre id='A.n-B.n-def'></pre>
<pre><code>                    A.n                     B.n
462824.4016201458289288 462824.4016201461199671</code></pre>
<p>To compute the group sum <code>B.s</code>, we take the difference of these two numbers.
Unfortunately, when values are so close to each other, doing so is the computing
equivalent of walking down the stairs into a dark basement with ominous
background music. To leave no doubt about the carnage that is about to unfold
(viewer discretion advised):<span id="carnage"></span></p>
<pre class="r"><code>B.s.new &lt;- B.n - A.n       # recompute B.s as per our algorithm
rbind(A.n, B.n, B.s.new)</code></pre>
<pre></pre>
<PRE class="fansi fansi-output"><CODE>                                           [,1]
A.n      <span style='background-color: #00BB00;'>462824.40162014</span><span>58289287984371185302734
B.n      </span><span style='background-color: #00BB00;'>462824.40162014</span><span>61199671030044555664062
B.s.new       0.0000000002910383045673370361328
</span></CODE></PRE>
<p>Doubles have approximately 15 digits of precision; we highlight those in green.
This would suggest <code>B.s.new</code> has no real precision left. At the same time, the
<a href="#obs-err">observed error of ~1.2%</a> suggests there must be some precision. In
order to better understand what is happening we need to look at the true nature
of doubles.</p>
</div>
<div id="interlude-ieee-754" class="section level1">
<h1>Interlude — IEEE-754</h1>
<p>Double precision floating points are typically encoded in base two in binary as
per the <a href="https://en.wikipedia.org/wiki/Double-precision_floating-point_format">IEEE-754 standard</a>:</p>
<figure class="aligncenter" style="max-width: 100%;">
<a href='#image-credits' title='Click for image credits.' class=image-credit>
<img
  id='ieee-754-illustration'
  src='/post/2019-06-11-hydra-precision_files/user-imgs/IEEE-754-double.png'
/>
</a>
<figcaption>
IEEE-754 Double Precision: each box represents a bit, big endian.
</figcaption>
</figure>
<p>We’ll mostly use a mixed binary/decimal representation, where the fraction is
shown in binary, and the exponent in decimal<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>:</p>
<pre></pre>
<PRE class="fansi fansi-output"><CODE><span style='color: #BBBBBB;background-color: #0000BB;'>+18</span><span>     +10        0        -10       -20       -30 -34  &lt;- Exponent (2^n)
 |       |         |         |         |         |   |
</span><span style='background-color: #BBBB00;'>0</span><span style='background-color: #00BB00;'>11100001111111010000110011011010000100100111110111111</span><span>: A.n
</span><span style='background-color: #BBBB00;'>0</span><span style='background-color: #00BB00;'>11100001111111010000110011011010000100100111111000100</span><span>: B.n
</span><span style='background-color: #BBBB00;'>0</span><span>00000000000000000000000000000000000000000000000000</span><span style='background-color: #00BB00;'>101</span><span>: B.s.new
</span></CODE></PRE>
<p>For the sign bit 0 means positive and 1 means negative. Each power of two in
the fraction that lines up with a 1-bit is added to the value<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. To
illustrate we can compute the integral value of <a href="#A.n-B.n-def"><code>A.n</code></a> by adding
all the 1-bit powers of two with positive exponents from the fraction:</p>
<pre class="r"><code>2^18 + 2^17 + 2^16 + 2^11 + 2^10 + 2^9 + 2^8 + 2^7 + 2^6 + 2^5 + 2^3</code></pre>
<pre><code>[1] 462824</code></pre>
<p>This is not enough to differentiate between <code>A.n</code> and <code>B.n</code>. Even with all 52
bits<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> we can barely tell the two numbers apart. As a result, when
we take the difference between those two numbers to produce <code>B.s.new</code>, most of
the fractions cancel out and leaving only three bits of precision<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>.
The actual encoding of <code>B.s.new</code> will have an additional 50 trailing zeroes, but
those add no real precision.</p>
<p>What about all those digits of seeming precision we see in the decimal
representation of <code>B.s.new</code>? Well check this out:</p>
<pre class="r"><code>2^-32 + 2^-34</code></pre>
<pre><code>[1] 2.910383045673370361328e-10</code></pre>
<pre class="r"><code>B.s.new</code></pre>
<pre><code>[1] 2.910383045673370361328e-10</code></pre>
<p>The entire precision of <code>B.s</code> is encapsulated in <code>$2^{-32} + 2^{-34}$</code>.
All those extra decimal digits are just an artifact of the conversion to
decimal, not evidence of precision.</p>
<p>Conversely, binary double precision floats can struggle to represent seemingly
simple decimal numbers. For example, 0.1 cannot be expressed exactly with any
finite number of bits. The built-in <code>iris</code> data set makes for a good
illustration as most measures are taken to one decimal place. Let’s look at it
in full binary format:</p>
<figure class="aligncenter" style="max-width: 100%;">
<!--<a href='#image-credits' title='Click for image credits.' class=image-credit>-->
<img src="/post/2019-06-11-hydra-precision_files/figure-html/bin_rep-1.png" width="672" style="display: block; margin: auto;" />
<!--</a>-->
<figcaption>
<code>iris</code> underlying binary representation, big endian
</figcaption>
</figure>
<p>As in our <a href="#ieee-754-illustration">previous illustration</a> the sign bit (barely
visible) is shown in yellow, the exponent in blue, and the fraction in green.
Dark colors represent 1-bits, and light ones 0-bits. Let’s zoom into the first
ten elements of the second column:</p>
<p><img src="/post/2019-06-11-hydra-precision_files/figure-html/bin-rep-zoom-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>And the corresponding decimal values:</p>
<pre class="r"><code>iris$Sepal.Width[1:10]</code></pre>
<pre><code> [1] 3.5 3.0 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1</code></pre>
<p>The first two elements are an integer and a decimal ending in <code>.5</code>. These can
be represented exactly in binary form, as is somewhat implicit in the trailing
zeroes in the first two rows<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>. The other numbers cannot be
represented exactly, as is implicit in the use of every available bit. We can
also see this by showing more digits in the decimals:</p>
<pre class="r"><code>options(digits=18)
iris$Sepal.Width[1:10]</code></pre>
<pre><code> [1] 3.50000000000000000 3.00000000000000000 3.20000000000000018
 [4] 3.10000000000000009 3.60000000000000009 3.89999999999999991
 [7] 3.39999999999999991 3.39999999999999991 2.89999999999999991
[10] 3.10000000000000009</code></pre>
<p>Before we move on, it’s worth remembering that there is a difference between
recorded precision and measured precision. There is zero chance that every iris
measured had dimensions quantized in millimeters. The measurements themselves
have no precision past the first decimal, so that the binary representation
is only correct 15 decimal places or so is irrelevant for the purposes of this
and most other data. The additional precision is mostly used as a buffer so
that we need not worry about precision loss in typical computations.</p>
</div>
<div id="how-bad-is-it-really" class="section level1">
<h1>How Bad Is It, Really?</h1>
<p>Given the binary representation of our numbers, we should be able to estimate
what degree of error we could observe. The last significant bit in <code>A.n</code> and
<code>B.n</code> corresponds to <code>$2^{-34}$</code>, so at the limit each could be off by as much
as <code>$\pm2^{-35}$</code>. It follows that <code>B.s</code>, which is the difference of <code>A.n</code> and
<code>B.n</code>, could have up to twice the error:</p>
<p><span class="math display">\[2 \times \pm2^{-35} = \pm2^{-34}\]</span></p>
<p>With a baseline value of <code>B.s</code> of <code>$2^{-32} + 2^{-34}$</code>, the relative error
becomes<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>:</p>
<p><span class="math display">\[\pm\frac{2^{-34}}{2^{-32} + 2^{-34}} = \pm20\%\]</span></p>
<p>If the theoretical precision is only <code>$\pm20\%$</code>, how did we end up with only a
<code>$~1.2\%$</code> error? Sheer luck, it turns out.</p>
<p>Precisely because <code>cumsum</code> is prone precision issues, <a href="https://github.com/wch/r-source/blob/R-3-5-branch/src/main/cum.c#L30">R internally</a> uses an
<a href="https://en.wikipedia.org/wiki/Extended_precision#Working_range">80 bit extended precision double</a> for the accumulator. The 80 bit
values are rounded down to 64 bits prior to return to R. Of the additional 15
bits 10 are added to the fraction<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>.</p>
<p>Let’s look again at the <code>A.n</code> and <code>B.n</code> values, but this time comparing the
underlying 80 bit representations with the 64 bit one visible from R, first for <code>A.n</code>:<span id="a-err"></span></p>
<PRE class="fansi fansi-output"><CODE><span style='color: #BBBBBB;background-color: #0000BB;'>+18</span><span>     +10        0        -10       -20       -30       -40
 |       |         |         |         |         |   *     |   *
</span><span style='background-color: #BBBB00;'>0</span><span style='background-color: #00BB00;'>11100001111111010000110011011010000100100111110111111</span><span>00000000000: 64 bit
</span><span style='background-color: #BBBB00;'>0</span><span style='background-color: #00BB00;'>11100001111111010000110011011010000100100111110111110</span><span style='background-color: #AFFF87;'>1110000010</span><span>1: 80 bit
</span><span style='background-color: #BBBB00;'>0</span><span>00000000000000000000000000000000000000000000000000000000</span><span style='background-color: #AFFF87;'>1011101</span><span>1: A.n Err
 
</span></CODE></PRE>
<p>The extended precision bits are shown in light green. The additional bit beyond
that is the guard bit used in rounding<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>.</p>
<p>And <code>B.n</code>:</p>
<PRE class="fansi fansi-output"><CODE><span style='color: #BBBBBB;background-color: #0000BB;'>+18</span><span>     +10        0        -10       -20       -30       -40
 |       |         |         |         |         |   *     |   *
</span><span style='background-color: #BBBB00;'>0</span><span style='background-color: #00BB00;'>11100001111111010000110011011010000100100111111000100</span><span>00000000000: 64 bit
</span><span style='background-color: #BBBB00;'>0</span><span style='background-color: #00BB00;'>11100001111111010000110011011010000100100111111000011</span><span style='background-color: #AFFF87;'>1111100010</span><span>0: 80 bit
</span><span style='background-color: #BBBB00;'>0</span><span>0000000000000000000000000000000000000000000000000000000000</span><span style='background-color: #AFFF87;'>11110</span><span>0: B.n Err
</span></CODE></PRE>
<p>The rounding errors <code>A.n.err</code> and <code>B.n.err</code> between the 80 and 64 bit
representations are small compared to the difference between the <code>A.n</code> and
<code>B.n</code>. Additionally, the rounding errors are in the same direction. As a
result the total error caused by the rounding from 80 bits to 64 bits, in
“binary” representation, should be <code>$(0.00111100 - 1.01111011) \times 2^{-38}$</code>,
or a hair under <code>$2^{-38}$</code>:</p>
<pre class="r"><code>A.n.err.bits &lt;- c(1,0,1,1,1,0,1,1)
B.n.err.bits &lt;- c(0,0,1,1,1,1,0,0)
exps &lt;- 2^-(38:45)
(err.est &lt;- sum(B.n.err.bits * exps) - sum(A.n.err.bits * exps))</code></pre>
<pre><code>[1] -3.609557e-12</code></pre>
<p>Our estimate of the error matches almost exactly the actual error:<span id="prec-loss-err"></span></p>
<pre class="r"><code>(err.obs &lt;-  B.s.new - B.s)</code></pre>
<pre><code>[1] -3.608941e-12</code></pre>
<p>We can do a reasonable job of predicting the bounds of error, but what good is
it to know that our result might be off by as much as 20%?</p>
</div>
<div id="a-new-hope" class="section level1">
<h1>A New Hope</h1>
<p>Before we give up it is worth pointing out that precision loss in some cases is
a feature. For example, <a href="https://github.com/wrathematics/float">Drew Schmidt’s <code>float</code></a> package implements single
precision numerics for R as an explicit trade-off of precision for memory and
speed. In deep learning, <a href="https://www.ibm.com/blogs/research/2018/12/8-bit-precision-training/">reduced precision is all the rage</a>. In a sense,
we are trading off precision for speed in our <code>cumsum</code> group sums approach,
though not explicitly.</p>
<p>Still, it would be nice if we could make a version of this method that
doesn’t suffer from this precision infirmity. And it turns out we can! The
main source of precision loss is due to the accumulated sum eventually growing
much larger than any given individual group in size. This became particularly
bad for <a href="#carnage">cumulative sum of <code>$(x - \bar{x})^2$</code></a> due to the squaring
that exacerbates relative magnitude differences.</p>
<p>A solution to our problem is to use a two-pass calculation. The first-pass is
as before, producing the imprecise group sums. In the second-pass we insert the
negative of those group sums as additional values in each group:</p>
<p><img src="/post/2019-06-11-hydra-precision_files/figure-html/error-correct-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>If the first-pass group sum was fully precise, the values inside the
highlighting boxes in the last panel should be zero. The small values we see
inside the boxes represent the errors of each group computation<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a>.
We can then take those errors and add them back to the first-pass group sums.</p>
<p>While the second-pass is still subject to precision issues, these are greatly
reduced because the cumulative sum is reset to near zero after each group. This
prevents the accumulator from becoming large relative to the values.</p>
<p><code>.group_sum_int2</code> below embodies the approach. One slight modification from the
visualization is that we subtract the first-pass group value from the last value
in the group instead of appending it to the group:</p>
<pre class="r"><code>.group_sum_int2 &lt;- function(x, last.in.group){
  x.grp &lt;- .group_sum_int(x, last.in.group)     # imprecise pass
  x[last.in.group] &lt;- x[last.in.group] - x.grp  # subtract from each group
  x.grp + .group_sum_int(x, last.in.group)      # compute errors and add
}</code></pre>
<p>We can use it with <a href="#group_slope2"><code>group_slope2</code></a>, a variation on <code>group_slope</code>
that can accept custom group sum functions:</p>
<pre class="r"><code>sys.time(slope.gs2 &lt;- group_slope2(x, y, grp, gsfun=.group_sum_int2))</code></pre>
<pre><code>   user  system elapsed 
  2.248   0.729   2.979 </code></pre>
<p><code>sys.time</code> returns the median time of eleven runs of <code>system.time</code>, and is
defined <a href="/2019/06/10/base-vs-data-table/#sys.time">in our previous post</a>.</p>
<pre class="r"><code>all.equal(slope.ply, slope.gs2)</code></pre>
<pre><code>[1] TRUE</code></pre>
<pre class="r"><code>quantile((slope.ply - slope.gs2)/slope.ply, na.rm=TRUE)</code></pre>
<pre><code>       0%       25%       50%       75%      100% 
-1.91e-12  0.00e+00  0.00e+00  0.00e+00  8.90e-12 </code></pre>
<p>This does not match the original calculations exactly, but there is essentially
no error left. Compare to the single pass calculation:</p>
<pre class="r"><code>quantile((slope.ply - slope.gs)/slope.ply, na.rm=TRUE)</code></pre>
<pre><code>       0%       25%       50%       75%      100% 
-1.24e-02 -1.92e-11 -2.16e-15  1.92e-11  8.33e-05 </code></pre>
<p>The new error is <em>ten orders of <strong>magnitude</strong></em> smaller than the original.</p>
<p>Two-pass precision improvement methods have been around for long time. For
example R’s own <code>mean</code> uses a <a href="https://github.com/wch/r-source/blob/R-3-3-branch/src/main/summary.c#L434">variation on this method</a>.</p>
<p>If you are still wondering how we came up with the 80 bit representations of the
<a href="#eighty-bit">cumulative sums earlier</a>, wonder no more. We use a method similar
to the above, where we compute the cumulative sum up to <code>A.n</code>, append the
negative of <code>A.n</code> and find the representation error for <code>A.n</code>. We than add that
error back by hand to the 64 bit representation<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a>. Same thing for
<code>B.n</code>.</p>
</div>
<div id="i-swear-its-a-feature" class="section level1">
<h1>I Swear, It’s a Feature</h1>
<p>We noted earlier that precision loss can be a feature. We can make it so in
this case by providing some controls for it. We saw earlier that it is possible
to <a href="#prec-loss-err">estimate precision loss</a>, so we can use this decide whether
we want to trigger the second precision correcting pass. <a href="#adjustable-precision-group-sums"><code>.group_sum_int3</code> in
the appendix</a> does exactly this by providing a
<code>p.bits</code> parameter. This parameter specifies the minimum number of bits of
precision in the resulting group sums. If as a result of the first pass we
determine the worst case loss could take us below <code>p.bits</code>, we run the second
precision improvement pass:</p>
<p>To illustrate we will time each of the group sum functions against the <code>$(x - \bar{x})^2$</code> which we’ve stored in the <a href="#x_ux2"><code>x_ux2</code> variable</a>:</p>
<pre class="r"><code>microbenchmark::microbenchmark(times=10,
  .group_sum_int(x_ux2, gnc),            # original single pass
  .group_sum_int2(x_ux2, gnc),           # two pass, always
  .group_sum_int3(x_ux2, gnc, p.bits=2), # check, single pass
  .group_sum_int3(x_ux2, gnc, p.bits=3)  # check, two pass
)</code></pre>
<pre><code>Unit: milliseconds
                                    expr   min    lq mean median  uq max neval
              .group_sum_int(x_ux2, gnc)  44.0  94.5  101     97 115 153    10
             .group_sum_int2(x_ux2, gnc) 196.2 258.2  277    271 294 365    10
 .group_sum_int3(x_ux2, gnc, p.bits = 2)  79.7 119.4  119    126 128 133    10
 .group_sum_int3(x_ux2, gnc, p.bits = 3) 294.5 308.3  324    312 349 371    10</code></pre>
<p>In this example the worst case precision is 2 bits, so with <code>p.bits=2</code> we run in
one pass, whereas with <code>p.bits=3</code> two passes are required. There is some
overhead in checking precision, but it is small enough relative to the cost of
the second pass.</p>
<p>Let’s try our slope calculation, this time again demanding at least 16 bits of
precision. For our calculation 16 bits of precision implies the error will be
at most <code>$\pm2^{-16} \approx \pm1.53 \times 10^{-5}$</code>.</p>
<pre class="r"><code>.group_sum_int_16 &lt;- function(x, last.in.group)
  .group_sum_int3(x, last.in.group, p.bits=16)

sys.time(slope.gs3 &lt;- group_slope2(x, y, grp, gsfun=.group_sum_int_16))</code></pre>
<pre><code>   user  system elapsed 
  2.052   0.652   2.706 </code></pre>
<pre class="r"><code>all.equal(slope.ply[!is.na(slope.ply)], slope.gs3[!is.na(slope.ply)])</code></pre>
<pre><code>[1] TRUE</code></pre>
<pre class="r"><code>quantile((slope.ply - slope.gs3) / slope.ply, na.rm=TRUE)</code></pre>
<pre><code>       0%       25%       50%       75%      100%
-2.91e-08 -2.51e-14  0.00e+00  2.51e-14  4.73e-09</code></pre>
<p>Indeed the errors are no larger than that<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>. The precision check
occurs only at the group sum step, so if other steps steps accumulate errors it
is possible for the final precision of a more complex computation to end up
below the specified levels.</p>
</div>
<div id="verdict" class="section level1">
<h1>Verdict</h1>
<p>Even with the precision correcting pass we remain faster than <code>data.table</code> under
the same formulation<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a>:</p>
<p><img src="/post/2019-06-11-hydra-precision_files/figure-html/timings-final-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Applying the unconditional second pass is fast enough that the reduced precision
version does not seem worth the effort. For different statistics it may be
warranted.</p>
</div>
<div id="conclusions" class="section level1">
<h1>Conclusions</h1>
<p>I would classify this as a moral victory more than anything else. We can be as
fast as <code>data.table</code>, or at a minimum close enough, using base R only. When I
wrote the <a href="/2019/02/24/a-strategy-for-faster-group-statisitics/">original group stats post</a> I didn’t think we could get this
close. Mostly this is a curiosity, but for the rare cases where speedy group
stats are required and <code>data.table</code> is not available, we now know there is an
option.</p>
<p>One other important point in all this is that double precision floating point
numbers are dangerous. They pack precision so far beyond typical measurements
that it is easy to get lulled into a false sense of security about them. Yes,
for the most part you won’t have problems, but <em>there be dragons</em>. For a more
complete treatment of floating point precision issues see David Goldberg’s
<a href="https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html">“What Every Computer Scientist Should Know About Floating-Point
Arithmetic”</a>.</p>
<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id="feedback-cont">

</div>
</div>
<div id="appendix" class="section level1">
<h1>Appendix</h1>
<div id="acknowledgments" class="section level2">
<h2>Acknowledgments</h2>
<div id="software" class="section level3">
<h3>Software</h3>
<ul>
<li><a href="https://github.com/hadley">Hadley Wickham</a> and the <a href="https://cran.r-project.org/web/packages/ggplot2/index.html"><code>ggplot2</code> authors</a> for <code>ggplot2</code> with which
I made the plots in this post.</li>
<li>Olaf Mersmann etal. for creating <a href="https://cran.r-project.org/web/packages/microbenchmark/index.html">microbenchmark</a>, and <a href="https://github.com/joshuaulrich">Joshua
Ulrich</a> for maintaining it.</li>
<li><a href="/2019/02/24/a-strategy-for-faster-group-statisitics/">Romain Francois</a> for <a href="https://github.com/ThinkR-open/seven31"><code>seven31</code></a>, which allowed I used when I first
started exploring the binary representation of IEEE-754 double precision
numbers.</li>
<li><a href="/about/#acknowledgments">General acknowledgements</a>.</li>
</ul>
</div>
<div id="image-credits" class="section level3">
<h3>Image Credits</h3>
<ul>
<li>This floating point representation is a recreation of <a href="https://en.wikipedia.org/wiki/File:IEEE_754_Double_Floating_Point_Format.svg">IEEE-754 Double
Floating Point Format</a>, by Codekaizen. It is recreated from scratch with
a color scheme that better matches the rest of the post.</li>
</ul>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<ul>
<li>David Goldberg, <a href="https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html">“What Every Computer Scientist Should Know About
Floating-Point Arithmetic”</a>, 1991.</li>
<li>Richard M.Heiberger and Burt Holland, <a href="https://link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf">“Computational Precision and Floating
Point Arithmetic”, Appendix G to “Statistical Analysis and Data
Display”</a>, Springer 2015, second edition.</li>
</ul>
</div>
</div>
<div id="worst-case" class="section level2">
<h2>Worst Case</h2>
<p>If we had been unlucky maybe the true values of <code>A.n</code> and <code>B.n</code> would have been
as follows:</p>
<PRE class="fansi fansi-output"><CODE><span style='color: #BBBBBB;background-color: #0000BB;'>+18</span><span>     +10        0        -10       -20       -30       -40
 |       |         |         |         |         |   *     |   *
</span><span style='background-color: #BBBB00;'>0</span><span style='background-color: #00BB00;'>11100001111111010000110011011010000100100111110111111</span><span>00000000000: 64 bit
</span><span style='background-color: #BBBB00;'>0</span><span style='background-color: #00BB00;'>11100001111111010000110011011010000100100111110111110</span><span style='background-color: #AFFF87;'>1000000000</span><span>1: 80 bit
</span><span style='background-color: #BBBB00;'>0</span><span>00000000000000000000000000000000000000000000000000000</span><span style='background-color: #AFFF87;'>0111111111</span><span>1: A.n Err
</span></CODE></PRE>
<p>And:</p>
<PRE class="fansi fansi-output"><CODE><span style='color: #BBBBBB;background-color: #0000BB;'>+18</span><span>     +10        0        -10       -20       -30       -40
 |       |         |         |         |         |   *     |   *
</span><span style='background-color: #BBBB00;'>0</span><span style='background-color: #00BB00;'>11100001111111010000110011011010000100100111111000100</span><span>00000000000: 64 bit
</span><span style='background-color: #BBBB00;'>0</span><span style='background-color: #00BB00;'>11100001111111010000110011011010000100100111111000100</span><span style='background-color: #AFFF87;'>1000000000</span><span>0: 80 bit
</span><span style='background-color: #BBBB00;'>1</span><span>00000000000000000000000000000000000000000000000000000</span><span style='background-color: #AFFF87;'>1000000000</span><span>0: B.n Err
</span></CODE></PRE>
<p>These have the same 64 bit representations as the original values, but the error
relative to the “true” 80 bit representations is quite different:</p>
<pre class="r"><code>A.n.err.bits &lt;- c(0,1,1,1,1,1,1,1,1,1,1)
B.n.err.bits &lt;- -c(1,0,0,0,0,0,0,0,0,0,0)  # note this is negative
exps &lt;- 2^-(35:45)

(err.est.max &lt;- sum(B.n.err.bits * exps) - sum(A.n.err.bits * exps))</code></pre>
<pre><code>[1] -5.817924e-11</code></pre>
<pre class="r"><code>err.est.max / B.s</code></pre>
<pre><code>[1] -0.1974539</code></pre>
<pre><code>err.obs / B.s</code></pre>
<p>In this case we get roughly the maximum possible error, which is ~16 times
larger than the <a href="#a-err">observed one</a>.</p>
</div>
<div id="functions-and-computations" class="section level2">
<h2>Functions and Computations</h2>
<div id="adjustable-precision-group-sums" class="section level3">
<h3>Adjustable Precision Group Sums</h3>
<p>We take a worst-case view of precision loss, comparing the highest magnitude we
reach in our cumulative sums to the smallest magnitude group. This function
does not account for NA or Inf values, although it should be possible to adapt
it as per our <a href="/2019/06/10/base-vs-data-table/#cumulative-group-sum-with-na-and-inf">previous post</a>.</p>
<pre class="r"><code>.group_sum_int3 &lt;- function(x, last.in.group, p.bits=53, info=FALSE) {
  xgc &lt;- cumsum(x)[last.in.group]
  gmax &lt;- floor(log2(max(abs(range(xgc)))))
  gs &lt;- diff(c(0, xgc))
  gsabs &lt;- abs(gs)
  gmin &lt;- floor(log2(min(gsabs[gsabs &gt; 0])))
  precision &lt;- 53 + (gmin - gmax)
  if(precision &lt; p.bits) {
    x[last.in.group] &lt;- x[last.in.group] - gs
    gs &lt;- gs + .group_sum_int(x, last.in.group)
  }
  if(info) # info about precision and second pass
    structure(gs, precision=precision, precision.mitigation=precision &lt; p.bits)
  else gs
}</code></pre>
</div>
<div id="group_slope2" class="section level3">
<h3>group_slope2</h3>
<p>Variation on <a href="/2019/06/10/base-vs-data-table/#so-you-think-you-can-group-stat"><code>group_slope</code></a> that accepts alternate group sum functions.</p>
<pre class="r"><code>group_slope2 &lt;- function(x, y, grp, gsfun=.group_sum_int) {
  ## order inputs by group
  o &lt;- order(grp)
  go &lt;- grp[o]
  xo &lt;- x[o]
  yo &lt;- y[o]

  ## group sizes and group indices
  grle &lt;- rle(go)
  gn &lt;- grle[[&#39;lengths&#39;]]
  gnc &lt;- cumsum(gn)              # Last index in each group
  gi &lt;- rep(seq_along(gn), gn)   # Group recycle indices

  ## compute mean(x) and mean(y), and recycle them
  ## to each element of `x` and `y`:
  sx &lt;- gsfun(xo, gnc)
  ux &lt;- (sx/gn)[gi]
  sy &lt;- gsfun(yo, gnc)
  uy &lt;- (sy/gn)[gi]

  ## (x - mean(x)) and (y - mean(y))
  x_ux &lt;- xo - ux
  y_uy &lt;- yo - uy

  ## Slopes!
  x_ux.y_uy &lt;- gsfun(x_ux * y_uy, gnc)
  x_ux2 &lt;- gsfun(x_ux ^ 2, gnc)
  setNames(x_ux.y_uy / x_ux2, grle[[&#39;values&#39;]])
}</code></pre>
</div>
<div id="x_ux2" class="section level3">
<h3>x_ux2</h3>
<p><code>x_ux2</code> contains the group-ordered values of <code>$(x - \bar{x})^2$</code>. To compute it
we start by ordering the inputs and computing indices:</p>
<pre class="r"><code>o &lt;- order(grp)
go &lt;- grp[o]
xo &lt;- x[o]
grle &lt;- rle(grp)
gn &lt;- grle[[&#39;lengths&#39;]]
gnc &lt;- cumsum(gn)
gi &lt;- rep(seq_along(gn), gn)</code></pre>
<p>We then compute <code>$\bar{x$</code> for each group, and use that for the final step:</p>
<pre class="r"><code>ux &lt;- .group_sum_int(xo, gnc) / gn
x_ux2 &lt;- (xo - ux[gi])^2</code></pre>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The win was for a literal implementation of the slope formula,
under a <a href="/2019/06/10/base-vs-data-table/#reformulated-slope">reformulated version</a> as shown by <a href="https://twitter.com/michael_chirico">Michael Chirico</a>
<code>data.table</code> is faster.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>One quirk of IEEE-754 floating point numbers is that some decimals
such as 0.3 and 0.7 cannot be represented exactly in the encoding. Some
calculations such as this one expose that quirk.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>We show the entire fraction, including the implicit
leading bit, and we show the de-biased exponent explicitly. See the <a href="https://en.wikipedia.org/wiki/Double-precision_floating-point_format">IEEE-754
wikipedia page for details</a>.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>Only the most significant bit’s exponent is recorded. We highlight
that by coloring it differently. Note that for <code>B.s.new</code>, the most
significant exponent is actually -32, which is not shown here to try to keep
the representation simple.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>If you counted closely you would see we show 53 bits in the
fraction, not 52. This is because in reality “doubles” only store the last
52 bits of the fraction. The first unstored bit is always assumed to be 1,
so 52 bits are encoded explicitly, and the additional 53rd implicitly. For
clarity we show that first bit explicitly. There is an exception to the first
bit assumption: when the exponent is all zeroes the first implicit bit will be
assumed to be zero as well. This corresponds to <a href="https://en.wikipedia.org/wiki/Denormal_number">“subnormal” numbers</a>.<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>Internally the <code>B.s</code> will be stored with all 53 bits of
precision, but the trailing 50 bits will be all zeroes and carry no real
precision.<a href="#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>Integers of magnitude less than <span class="math inline">\(2^53\)</span> can be represented
exactly, as well as any decimal that can be expressed as a negative power of
two (e.g. <code>$2^-1 = 0.5$</code>, <code>$2^-2 = 0.25$</code>, etc.) or combination thereof for
which the most significant bit is within 53 bits of the least significant one.
Similarly a subset of integers larger than <code>$2^53$</code> can be represented exactly
with the same constraint.<a href="#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>The relative error value will fluctuate depending on the value we
are measuring it to. Our reference value is in binary <code>1.01</code> (exp:
<code>$2^{-32}</code>), but it could also have been <code>1.11</code> or <code>0.01</code>, so for 3 bit values
the relative error can be as low as ~14% or as high as 100%.<a href="#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p>Unlike with normal double precision numbers, there is no implicit
leading 1 bit in 80 bit extended precision, so even though there are 16
additional bits, there are only 15 effective additional bits.<a href="#fnref9" class="footnote-back">↩</a></p></li>
<li id="fn10"><p>It is unclear to me exactly when guard (and round and sticky) bits
are available or not. Presumably they are part of the registers, so as long
as numbers reside in them they are accessible. My experiments suggest this,
but I was not able to find clear evidence this is actually the case.<a href="#fnref10" class="footnote-back">↩</a></p></li>
<li id="fn11"><p>We exaggerated the errors for expository purposes.<a href="#fnref11" class="footnote-back">↩</a></p></li>
<li id="fn12"><p>since R doesn’t support 80 bit doubles at the user level, we had to
do the binary addition by hand…<a href="#fnref12" class="footnote-back">↩</a></p></li>
<li id="fn13"><p>We need to exclude NAs because in the comparisons because there
are several groups where under full precision, we end up with the calculation
<code>0/0 == NA</code>, but under 16 bit precision we have <code>0/(0 + 2^-16) == 0</code>. That
is, with some small error what would be an undefined slope becomes a zero
slope.<a href="#fnref13" class="footnote-back">↩</a></p></li>
<li id="fn14"><p>It is possible to <a href="/2019/06/10/base-vs-data-table/#reformulated-slope">reformulate the statistic</a> to a form that
allows <code>data.table</code> to be fastest, although that form has precision issues of
its own.<a href="#fnref14" class="footnote-back">↩</a></p></li>
</ol>
</div>
