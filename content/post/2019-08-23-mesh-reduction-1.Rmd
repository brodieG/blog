---
title: RTIN Mesh Reduction
author: ~
date: '2019-08-23'
slug: mesh-reduction-1
categories: [R]
tags: [algorithms,viz]
image:
  /post/2019-08-23-mesh-reduction-1_files/images/batch-2.png
imagerect: ~
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: Front page summary
---

```{r echo=FALSE, warning=FALSE}
options(digits=3)
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
library(ggplot2)
thm.blnk <- list(
  theme(
    axis.text.x=element_blank(), axis.text.y=element_blank(),
    axis.ticks.x=element_blank(), axis.ticks.y=element_blank(),
    panel.grid=element_blank()
  ),
  ylab(NULL),
  xlab(NULL)
)
```

# Off Course, of Course

<!-- this needs to become a shortcode -->
<img
  id='front-img'
  src='/post/2019-08-23-mesh-reduction-1_files/images/batch-2.png'
  class='post-inset-image'
/>

A few months ago a stumbled on Vladimir Agafonkin's ([&commat;mourner][5]) fantastic
[observable notebook][6] on adapting the Right-Triangulated Irregular
Networks (RTIN) algorithm to JavaScript.  The subject matter is interesting and
the exposition superb.  I'll admit I envy the elegance of natively interactive
notebooks.

I almost left it at that, but then the irrepressible "but can we do that in R"
bug in whispered into my ear, once again setting me hopelessly
off-course[^off-course].  Obviously we can do it by porting the code directly,
but the loop heavy nature of it would doom it.  We could also trivially port it
to C and interface that into R, but where's the fun in that?

# Right Triangles

You should look at [&commat;mourner's][5] [notebook][6] or the original [Will Evans et
al. 1997 paper][8] for in-depth details as the point of this post is the
translation to an R idiomatic implementation.  Notwithstanding I can't resist
indulging in some of my newly acquired `rayrender`ing powers for some
illustrations.

The basic idea is to approximate a triangle mesh with fewer triangles while
keeping the approximation error within certain tolerances.  For example here we
approximate the beloved `volcano` mesh (glass) at three increasingly relaxed
tolerance levels:

<!--
Maybe better here would be to have a full resolution version of first level of
approx on volcano, with glass, and then show the increasing levels with the
original glass?  Should we use the full mesh of volcano, or the already
partially reduced one?
-->

<img class='aligncenter'
  src='/post/2019-08-23-mesh-reduction-1_files/images/glass-and-mesh-3-f-merge.png'
/>

If you look closely you'll see that even in the higher fidelity approximation
the wire mesh ends up above or below the glass surface, although on the whole
the approximation is good.

If we flatten the elevation component of the meshes and change our viewpoint it
becomes apparent that all the triangles in the approximation are right angled
isoceles triangles.  This is where the "Right-Triangulated" part of the
algorithm name comes from.

<img class='aligncenter'
  src='/post/2019-08-23-mesh-reduction-1_files/images/batch-4.png'
/>

Another important feature of this mesh approximation is that it is hierarchical.
When we stack the three meshes we can see that each low resolution triangle is
exactly replaced by higher resolution ones when it does not meet the error
tolerance:

<img class='aligncenter'
  src='/post/2019-08-23-mesh-reduction-1_files/images/batch-1.png'
/>

If a triangle too coarse for the desired level of fidelity, it is broken up into
two smaller triangles.  The process is repeated until the approximation error
can be tolerated.

In order to decide whether an approximate is faithful enough we measure the
error between each level of mesh approximation at the middle of
the hypotenuse of each triangle.  For the sake of exposition we will temporarily
abandon volcano and switch to a simple 3x3 elevation surface, and we will show
approximations where at each level every triangle is the same size.

<img class='aligncenter'
  id=three-by-three
  src='/post/2019-08-23-mesh-reduction-1_files/images/simple-mesh-s-merge.png'
/>

At the most granular level (gold) there is no error as the mesh corresponds
exactly to the elevation surface.  At the first level of approximation (silver)
error appears around the periphery of the surface as that is where the triangle
edges skip over surface vertices.  We show these errors as a checkered cylinder.
At the coarsest level of approximation (copper) we generate additional error on
the center of the surface and show that too as a checkered cylinder.

Errors are measured at the middle of the hypotenuse of the triangles.  This is
not arbitrary: as we merge triangles together to increase coarseness, the
vertices that are lost correspond to that point of the parent triangle.  An
implication is that we never measure errors from different levels of
approximation at overlapping locations, so we can store the errors in a single
matrix:

<!--
Error matrix, just the cylinders
-->

For clarity the surface was constructed so that all the errors are in the same
direction and directly visible from our observation point.  In practice the
errors can occur on either side of the surface, and we would record their
absolute values.  The corners of the matrix will never have any error as even
the coarsest level of approximation will have vertices at those locations.

We can now compare our recorded errors to different tolerance levels, and use
that to decide what triangles to draw.  A possible algorithm is to draw the
child triangles of any triangle for which its error or the error of any of its
descendants exceeds the threshold.  Here we represent a possible threshold as
the "water" level; errors poking above it must be addressed:

<!--
Error threshold such that worst error in middle and center error trigger.

Image is side by side (left/right):

* Left: error matrix, water level added
* Right: mesh approximation, gold/silver, gold drawn last
-->

Looks great.  Unlike with our original `volcano` meshes we are treating the mesh
color to correspond to a specific level of approximation.  But notice what
happens if we increase the tolerance level:

<!--
Error threshold such that only the worst error triggers

Image is side by side (left/right):

* Left: error matrix, water level added
* Right: mesh approximation, complete mess
-->

Gah, there are gaps!  This is particularly apparent if we try to draw the
approximated surface, rotated here so we can see the gap:

<!--
Glass surface approximated and rotated showing gap
-->

To resolve this we carry over errors from children triangles to their parents
and override use the largest of the child errors and the parent error as the
parent error.<span id='carry-over-viz'></span>

<!--
Two error matrices:

* Left: original, but with arrows showing errors converging onto parent
* Right: original, but with parent error adding child error stacked on top of it
-->

This ensure that the mesh has no T intersections and as such is
continuous:

<!--
Use same water level as the one that caused the problem before.

Image is side by side (left/right):

* Left: error matrix, water level added
* Right: mesh approximation, complete mess
-->

# Algorithm in Action

We now have a rough idea of the algorithm.  Unfortunately I learned the hard way
that "a rough idea" is about as useful when refactoring an algorithm as it is to
know that a jar of nitroglycerin is "kinda dangerous" when you're tasked with
transporting it.  In order to safely adapt the algorithm to R we need
full mastery of the details.  I'm still covered in gouges and bruises from the
sharp corner cases and the off-by-who-knows-how-many-errors I smacked into.

The outline of the algorithm is as follows:

```{r eval=FALSE}
for (every triangle in mesh) {
  while (searching for triangle vertex coordinates) {
    # ... Compute coordinates of vertices `a`, `b`, and `c` ...
  }
  # ... Compute approximation error at midpoint of `a` and `b` ...
}
```

The `for` loop iterates as many times as there are triangles in the mesh.  In
our [3 x 3 grid](#three-by-three) there are six triangles (4 at the first level
of approximation, and another 2 at the next level).  For each of these
triangles, the `while` loop identifies the vertex coordinates `$(ax,ay)$`,
`$(bx,by)$`, and `$(cx,cy)$`.  This allow us to compute the error at the
midpoint of the hypotenuse.

Let's observe the algorithm find the first triangle to get a sense for how it
works.  We'll use a 5 x 5 grid this time[^min-complex], and regrettably abandon
the 3D realm for the sake of clarity.  The visualization is generated with the
help of [`watcher`][9], and it displays the state of the system _after_ the
evaluation of the highlighted line:

<div id='flipbook1'></div>

When the `while` loop ends the target triangle has been found, shown in yellow.
In the following frames the algorithm computes the location of the midpoint `m`
of the points `a` and `b` (these two always define to the hypotenuse), computes
the approximation error at that point, and records it in the error matrix.

Since this is the most granular approximation level there are no child errors to
propagate[^no-child-error] and the algorithm skips over that part.

What happens next is pretty remarkable.  At the top of the loop we're about to
repeat we have:

```{r eval=FALSE}
for (i in (nTriangles - 1):0) {
  id <- i + 2L
  # ... rest of code omitted
```

This innocent-seeming reset of `id` will ensure that the rest of the logic
will find the next smallest triangle, and the next after that, and so forth:

<img class='aligncenter'
  src='/post/2019-08-23-mesh-reduction-1_files/images/mesh-anim-4-abreast.png'
/>

At this point we've tiled the whole surface at the lowest approximation level,
but we need to repeat the process for the next approximation level.
"Magically" the smaller initial `id` values lead the `while` loop to exit
earlier, at the next larger triangle size.  Here is another flipbook showing the
search for the second triangle at the next approximation level[^second-triangle]:

<div id='flipbook2'></div>

In the last few frames we can see the error from the children elements (labeled
`lc` and `rc` for "left child" and "right child") carried over into the current
target point.  As we [saw earlier](#carry-over-viz) this ensures there are no
gaps in our final mesh.  Each error point will have errors carried over from up
to four neighboring children.  For the above point the error from the remaining
children is carried over a few hundred steps later:

<img class='aligncenter'
  src='/post/2019-08-23-mesh-reduction-1_files/images/mesh-anim-2-abreast.png'
/>

In this case it the additional child errors are smaller so they do not change
the outcome, but it is worth noting that each hypotenuse midpoint may add up to
four child errors.

Here is the entire process:

<div style='background-color: red'>
Initial 10 frames have bad counter.
</div>

<video id=mesh-anim style='display: block; margin: 0 auto;' controls loop>
<source
  src='/post/2019-08-23-mesh-reduction-1_files/images/out.mp4'
  type="video/mp4"
/>
</video>

# R's Kryptonite

We essentially transliterated the JavaScript algorithm into R.  While this
works, it does so slowly.  Compare the timings of our R function, the JavaScript
version, and finally a [C version](c-version):

```{r rtin-timings, echo=FALSE}
dat <- data.frame(Language=c('R', 'JS', 'C'), Time=c(2500,20.78,6.81))
ggplot(dat) +
  geom_col(aes(Language, Time)) +
  geom_text(aes(Language, Time, label=Time), vjust=-.5) +
  ylab('Time (ms)') +
  ggtitle('RTIN Error Computation Benchmarks', subtitle='257x257 Grid Size') +
  scale_y_continuous(expand = expand_scale(mult = c(0.05, .1)))
```

Our JavaScript timings are in line with those &commat;mourner reports
(~20ms)[^js-timings].
<div style='background-color: red'>
NOTE: on Chrome we're seeing timings start at ~38 and then
eventually drop to and stay at ~20ms.  The timings here are Firefox.  Mesh
extraction on Chrome starts at ~11ms and drops to ~1.5-2ms
</div>

I was surprised by how well JS stacks up relative to C.  Certainly a ~7x gap is
nothing to sneeze at, but I expected it to be bigger given JS is interpreted.
On the other hand R does terribly.  This is expected as R is not designed to run
tight loops on scalar values.  We could take some steps to get the loop to run a
little faster, but not to run fast enough to run real time at scale.

So what are we to do?  Well, as @mdsumner [enthusiastically puts it][7]:

> JavaScript is cool, C++ is cool. But #rstats can vectorize the absolute shit
> out of this. No loops until triangle realization
>
> -- @mdsumner

Indeed, we don't need to watch the [algorithm play out](#mesh-anim) very long to
see that its elegance comes at the expense of computation.  Finding each
triangle requires many steps, and they seem unlikely to be friendly to
microarchitecture optimizations[^branch-predict].

The triangles coordinates do not depend on each other, so it should be possible
calculate them directly based on the grid size.  We cannot completely vectorize
the calculation as we need to iteratively carry over errors across approximation
levels.  Fortunately the level count will be proportional to
`$\log_2(\sqrt{points})$`, so we only need to explicitly loop a few times.

Before we get too deep into this, I'd like to make it clear that vectorizing the
algorithm for use in R is interesting, but also _mostly_ useless.  The
JavaScript version runs plenty fast, and if you must have it in R you can
trivially transliterate the code [into C](#c-transliteration), which can then be
accessed from R.  The vectorization version does have some use as it is not
constrained to `$2^k + 1$` square grids.

So, let's go vectorize the absolute $#!+ out of this.  It'll be easy, they
said.  It'll be fun.  Right.

# Vectorize The Absolute $#!+ Out Of This

<div style='background-color: red;'>Update all links to split post</div>

I wish I could tell you that I carefully figured out all the intricacies of the
algorithm before I set off on my initial vectorization attempt, that I didn't
waste hours going cross-eyed at triangles poorly drawn on scraps of paper, and
that I didn't produce algorithms which closer inspection showed to be [laughably
incorrect][10].  Instead I'll have to be content with telling you that I did get
this to work eventually.  A victory that had it wasted lives like it wasted my
time would supplant those of Pyrrhus in the history books[^grandiloquence].

While I was generally aware that there is more to the problem than generating
equal sized triangles, I underestimated the degrees of freedom available and
wasted quite a bit of time going in circles as a result.  There are eight
different triangle orientations, split into two groups of four.  The groups
alternate between approximation levels.  Here we show the orientations for the
2nd and 3rd most granular levels of a 9 x 9 grid[^nine-nine].  The triangles
are colored by orientation.  For reasons that will become clearer later we call
the group with diagonal hypotenuses "Diagonal", and the one with
vertical/horizontal ones "Axis":<span id='sqr-vs-dia'></span>

```{r echo=FALSE}
source('../../static/script/_lib/plot.R')
source('../../static/script/mesh-viz/viz-lib.R')
source('../../static/script/mesh-viz/rtin-vec.R')
source('../../static/script/mesh-viz/rtin-vec2.R')
source('../../static/script/mesh-viz/extract-vec.R')
# source('static/script/mesh-viz/viz-lib.R')
# source('static/script/mesh-viz/rtin-vec.R')
err <- matrix(0, 9, 9)
# note diamond/square is backwards here from what we use in viz-lib
tri.di <- extract_mesh2(err, 1, 4)
tri.sq <- extract_mesh2(err, 1, 3)
tris <- tris_to_df(list(tri.sq, tri.di), err)
tris[['type']] <- factor(
  c(
    rep('Diagonal', length(unlist(tri.sq))),
    rep('Axis', length(unlist(tri.di)))
  ),
  levels=c('Diagonal', 'Axis')
)
points <- tris_to_df(seq_along(err), err)
p <- ggplot(tris) +
  geom_polygon(aes(x=x, y=y, group=id, fill=I(color))) +
  geom_point(data=points, aes(x=x, y=y), size=.5, shape=3, color='white') +
  facet_wrap(~type) +
  coord_fixed() +
  thm.blnk
pdim <- gtable_dim(ggplotGrob(p))
old.opt <- knitr::opts_chunk$get(c('fig.width', 'fig.height'))
do.call(knitr::opts_chunk$set, list(fig.width=pdim[1], fig.height=pdim[2]))
```
```{r square-vs-diamond, echo=FALSE}
p
do.call(knitr::opts_chunk$set, old.opt)
```

The "Diagonal" triangulation, the one with vertical and horizontal hypotenuses,
has just one tiling pattern.  A particular triangle "shape" (a specific color in
the plot above) must always be arranged in the same way relative to the other
shapes that share the vertex opposite their hypotenuses.  Any other arrangement
would prevent us from fully tiling the grid without breaking up triangles.

On the other hand the "Axis" triangulation, the one with the diagonal
hypotenuses, has multiple possible tilings:

```{r echo=FALSE}
# should have used the offsets for this, but we hadn't written that yet.
raw1 <- c(03,19,01, 03,19,21, 03,23,21, 03,23,05)
raw2 <- raw1 + rep(c(2,-2), each=length(raw1)/2)
rawc1 <- rep(raw1, 2) + rep(0:1 * 4, each=length(raw1))
rawc2 <- rep(raw2, 2) + rep(0:1 * 4, each=length(raw2))
rawc3 <- rep(raw1[1:6], 4) + rep(0:3 * 2, each=6)
f1 <- c(rawc1, rawc2 + 18, rawc1 + 36, rawc2 + 54)
f2 <- rep(rawc1, 4) + rep(0:3 * 18, each=length(rawc1))
# f3 <- c(matrix(seq_along(err), nrow(err), byrow=TRUE))[f2]
f3 <- rep(rawc3, 4) + rep(0:3 * 18, each=length(rawc3))
tris.f1 <- tris_to_df(f1, err)
nms <- sprintf( "Faux Diagonal %d", 1:3)
tris.f1[['type']] <- nms[1]
tris.f2 <- tris_to_df(f2, err)
tris.f2[['type']] <- nms[2]
tris.f3 <- tris_to_df(f3, err)
tris.f3[['type']] <- nms[3]
tris.f <- rbind(subset(tris, type=='Diagonal'), tris.f1, tris.f2, tris.f3)
tris.f[['type']] <- factor(tris.f[['type']], levels=unique(tris.f[['type']]))

p <- ggplot(tris.f) +
  geom_polygon(aes(x=x, y=y, group=id, fill=I(color))) +
  geom_point(data=points, aes(x=x, y=y), size=.5, shape=3, color='white') +
  facet_wrap(~type, nrow=1) +
  coord_fixed() +
  thm.blnk
pdim <- gtable_dim(ggplotGrob(p))
old.opt <- knitr::opts_chunk$get(c('fig.width', 'fig.height'))
do.call(knitr::opts_chunk$set, list(fig.width=pdim[1], fig.height=pdim[2]))
```
```{r fake-squares, echo=FALSE}
p
```

We show just three alternate tilings, although there are `$2^{16}$`.  All of
them perfectly cover the grid with equal sized triangles.  "Faux Diagonal 1" is
even a pretty convincing replica.  Unfortunately only one of them fits exactly
into the next level of coarseness, "Axis" [shown previously](#sqr-vs-dia),
seen here in a green outline:

```{r fake-diamonds-outline, echo=FALSE}
bad.1 <- ids_to_df(c(11,13,15,17) + rep(0:3 * 18, each=4), err)
bad.2 <- ids_to_df(c(11,13,15,17) + rep(c(0, 36), each=4), err)
bad.3 <- ids_to_df(c(11,15,31,35) + rep(c(0, 36), each=4), err)
bad.1[['type']] <- factor(nms[1], levels=levels(tris.f[['type']]))
bad.2[['type']] <- factor(nms[2], levels=levels(tris.f[['type']]))
bad.3[['type']] <- factor(nms[3], levels=levels(tris.f[['type']]))

p + geom_polygon(
  data=subset(tris, type=='Axis', -type),
  aes(x, y, group=id), fill=NA, color='green'
) + geom_point(
  data=rbind(bad.1, bad.2, bad.3), aes(x, y),
  fill='blue', color='grey20', shape=21, size=2
)
do.call(knitr::opts_chunk$set, old.opt)
```

It is only with "Diagonal" that every edge of the parent approximation level
completely overlaps with edges of the child.  Every other arrangement has at
least one parent edge crossing a hypotenuse of a child triangle.  These
crossings are what the blue circles highlight.  Ironically, the best looking
fake is the worst offender.

Herein lies the beauty of [&commat;mourner's][6] implementation; by deriving
child triangles from splitting the parent ones, we guarantee that the child
triangles will conform to the parents.  As I learned painfully no such
guarantees apply when we compute the coordinates directly.

There are many ways to solve this vectorization problem, but the simplest I
could come up with was to treat it as a tiling task.  The idea is to define the
smallest tessellating patterns that extend to the correct layout.  These
patterns can then be repeated as needed in internally vectorized
manner[^int-vec].  This is what they look like for our 9 x 9 example:

```{r echo=FALSE}
tris2 <- transform(
  tris,
  alpha=ave(
    seq_along(id), factor(id), FUN=function(i) all(x[i] <= .5 & y[i] <= .5)
  )
)
p <- ggplot(tris2) +
  geom_polygon(
    aes(x=x, y=y, group=id, fill=I(color), alpha=alpha)
  ) +
  geom_point(data=points, aes(x=x, y=y), size=.5, shape=3, color='white') +
  facet_wrap(~type) +
  scale_alpha_continuous(guide=FALSE, range=c(0.2,1)) +
  coord_fixed() +
  thm.blnk
pdim <- gtable_dim(ggplotGrob(p))
old.opt <- knitr::opts_chunk$get(c('fig.width', 'fig.height'))
do.call(knitr::opts_chunk$set, list(fig.width=pdim[1], fig.height=pdim[2]))
```
```{r square-vs-diamond-tile, echo=FALSE}
p
do.call(knitr::opts_chunk$set, old.opt)
```

Unfortunately this is insufficient.  We must also for each triangle hypotenuse
midpoint track their children so that we may [carry over](#carry-over-viz) their
errors.  One nice benefit of the tessellating pattern is that if we define the
parent-child relationship for the simple tile, we can copy that relationship
along with the rest of the tile.  In fact, we don't actually care about the
triangles.  What we actually want are the hypotenuses (black), their midpoints
(<span style='background-color: #8da0cb'>lavender</span>) and endpoints(hollow <span
style='color: #66c2a5'>green</span> ), and the corresponding child
hypotenuse midpoints (<span style='background-color: #fc8d62'>orange</span>).
The parent/child relation is shown with the arrows:

```{r echo=FALSE}
p.dg <- ids_to_df(c(offset.dg[,1,] * 9 + offset.dg[,2,] + 1), matrix(0,9,9))
p.ax <- ids_to_df(c(offset.ax[,1,] * 9 + offset.ax[,2,] + 1), matrix(0,9,9))
p.dg[['type']] <- factor('Diagonal', levels=c('Diagonal', 'Axis'))
p.dg[['ptype']] <- 'child'
p.dg[8:12, 'ptype'] <- 'mid'
p.dg[1:8, 'ptype'] <- 'ends'
p.ax[['type']] <- factor('Axis', levels=c('Diagonal', 'Axis'))
p.ax[['ptype']] <- 'child'
p.ax[9:12, 'ptype'] <- 'mid'
p.ax[1:8, 'ptype'] <- 'ends'
p.all <- rbind(p.dg, p.ax)
p.all[['grp']] <- (seq_len(nrow(p.all)) - 1L) %% 4L

to_arrows <- function(grp) {
  ends <- grp[1,,drop=FALSE]
  ends.b <- grp[2,,drop=FALSE]
  mid <- grp[3L,,drop=FALSE]

  child <- grp[-3L,]
  child[['xend']] <- mid[, 'x']
  child[['yend']] <- mid[, 'y']
  x.diff <- child[['xend']] - child[['x']]
  y.diff <- child[['yend']] - child[['y']]
  off.c <- 5
  child[['xend']] <- child[['xend']] - x.diff / off.c
  child[['x']] <- child[['x']] + x.diff / off.c
  child[['yend']] <- child[['yend']] - y.diff / off.c
  child[['y']] <- child[['y']] + y.diff / off.c

  x.diff <- ends.b[['x']] - ends[['x']]
  y.diff <- ends.b[['y']] - ends[['y']]
  off.e <- 30
  ends[['xend']] <- ends.b[['x']] - x.diff / off.e
  ends[['x']] <- ends[['x']] + x.diff / off.e
  ends[['yend']] <- ends.b[['y']] - y.diff / off.e
  ends[['y']] <- ends[['y']] + y.diff / off.e
  rbind(ends, child)
}
p.arr <-
  do.call(rbind, lapply(split(p.all, p.all[c('type', 'grp')]), to_arrows))

p2 <- ggplot(tris2) +
  geom_polygon(aes(x=x, y=y, group=id, fill=I(color)), alpha=.2) +
  geom_point(data=points, aes(x=x, y=y), size=.5, shape=3, color='white') +
  geom_segment(
    data=subset(p.arr, ptype=='child'),
    arrow=arrow(length=unit(.05, 'inches'), type='closed', angle=25),
    aes(x, y, xend=xend, yend=yend),
    color='grey50'
  ) +
  geom_segment(
    data=subset(p.arr, ptype=='ends'),
    aes(x, y, xend=xend, yend=yend)
  ) +
  geom_point(
    data=subset(p.all, ptype=='mid'),
    aes(x, y, colour=I('#8da0cb')),
    size=3
  ) +
  geom_point(
    data=subset(p.all, ptype=='ends'),
    aes(x, y, colour=I('#66c2a5'), fill='grey90'),
    shape=21, size=2
  ) +
  geom_point(
    data=subset(p.all, ptype=='child'),
    aes(x, y, colour=I('#fc8d62'))
  ) +
  facet_wrap(~type) +
  coord_fixed() +
  thm.blnk

pdim <- gtable_dim(ggplotGrob(p2))
old.opt <- knitr::opts_chunk$get(c('fig.width', 'fig.height'))
do.call(knitr::opts_chunk$set, list(fig.width=pdim[1], fig.height=pdim[2]))
```
```{r square-vs-diamond-points, echo=FALSE}
p2
do.call(knitr::opts_chunk$set, old.opt)
```

# Action!

Let's look at this in action with the next grid size up for effect.  As we did
in our [earlier visualization](#mesh-anim) we'll track the coordinates in the
left panel side (both "Diagonal" and "Axis", alternating), and the computed mesh
approximation errors in the right hand panel:

<div style='background-color: red;'>Regen video with seq.r line order change</div>

<video id=mesh-anim-vec style='display: block; margin: 0 auto;' controls loop>
<source
  src='/post/2019-08-23-mesh-reduction-1_files/images/out-vec.mp4'
  type="video/mp4"
/>
</video>

Indeed we can see that for each "layer", we start with a repeatable template
sized to the layer.  We then fill a column with it, then the rest of the
surface, and finally we compute errors.  Here is the same thing as a flipbook
with code so we can see how it's actually done.  Remember that in these
flipbooks the state shown is immediately _after_ the highlighted line is
evaluated.  We'll focus on the second set of layers:

<div id='flipbook-vec'></div>

`o`, as shown in the first frame[^first-frame] contains the *o*ffsets that
represent the template tile.  These offsets are generated by `init_offsets`,
which in _simplified_[^simplified-offsets] form is:

```{r eval=FALSE}
init_offsets <- function(i, j, n, layers) {
  o <- if(j == 'axis') offset.ax else offset.dg    # raw coords
  o <- o * 2^(i - 1)                               # scale
  array(o[,1,] + o[,2,] * (n + 1) + 1, dim(o)[-2]) # x/y -> id
}
```

`offset.ax` and `offset.dg` contain x/y coordinates for each of the points in
the template tile.  `init_offsets` scales those to the appropriate size for the
layer and collapses them to "linearized" coordinates<span id=linearized></span>:

<!-- really shouldn't be doing these inline styles... -->
<style>
span.bgw-hmid   { padding: .1em 0; background-color: #8da0cb;}
span.bgw-hend   { padding: .1em 0; color: #66c2a5;}
span.bgw-hchild { padding: .1em 0; background-color: #fc8d62;}
</style>

```{r eval=FALSE}
init_offsets(i=2, j='axis', n=16, layers=log2(16))
```
<pre><code>     [,1] [,2] [,3] [,4] [,5]
[1,]  <span class='bgw-hend'>  1    5 </span><span class='bgw-hmid'>   3 </span><span class='bgw-hchild'>  21   19 </span>
[2,]  <span class='bgw-hend'>  5   73 </span><span class='bgw-hmid'>  39 </span><span class='bgw-hchild'>  55   21 </span>
[3,]  <span class='bgw-hend'> 73   69 </span><span class='bgw-hmid'>  71 </span><span class='bgw-hchild'>  53   55 </span>
[4,]  <span class='bgw-hend'> 69    1 </span><span class='bgw-hmid'>  35 </span><span class='bgw-hchild'>  19   53 </span>
</code></pre>
```{r eval=FALSE}
init_offsets(i=2, j='diag', n=16, layers=log2(16))
```
<pre><code>     [,1] [,2] [,3] [,4] [,5] [,6] [,7]
[1,]  <span class='bgw-hend'>  1   73 </span><span class='bgw-hmid'>  37 </span><span class='bgw-hchild'>  71   35    3   39 </span>
[2,]  <span class='bgw-hend'>  9   73 </span><span class='bgw-hmid'>  41 </span><span class='bgw-hchild'>  75   43   39    7 </span>
[3,]  <span class='bgw-hend'> 73  137 </span><span class='bgw-hmid'> 105 </span><span class='bgw-hchild'> 139  107  103   71 </span>
[4,]  <span class='bgw-hend'> 73  145 </span><span class='bgw-hmid'> 109 </span><span class='bgw-hchild'> 143  107   75  111 </span>
</code></pre>

In "linearized" form for our 17 x 17 example, 1 corresponds to (x,y) coordinate
(1,1), 2 to (1,2),  18 to (2,1), 289 to (17,17), etc.[^linearized].  The colors
match those of the visualization.  We can shift "linearized" coordinates,
by simple arithmetic.  For example, adding one to a coordinate will move it up
one row[^shift-with-care].  Adding `(n + 1)`, where `n <- nrow(map) - 1`, shifts
coordinates by one column.  So in:

```{r eval=FALSE}
seq.r <- (seq_len(tile.n) - 1) * n / tile.n
o <- rep(o, each=tile.n) + seq.r
```

We first repeat the template tile four times, and then shift them by `seq.r`:

```{r eval=FALSE}
seq.r
```
```
[1]  0  4  8 12
```

Due to how we repeat `o` each value of `seq.r` will be recycled for every value
of `o`.  The result is to copy the template tile to fill the column.  Similarly:

```{r eval=FALSE}
o <- rep(o, each=tile.n) + seq.r * (n + 1)
```

Will fill the rest of the surface by repeating and shifting the column.

The points are arranged by type in the original offset list so we can easily
subset for a particular type of point from the repeated tile set.  This is
exactly how the visualization colors the points as the point type is never
explicitly stated in the code.  Additionally, the relative positions of parents
and children are also preserved.

# Loopy Vectorization?

Hold on a sec, weren't we going to vectorize the $#!+ out of this?  What about
all those for loops?  They are nested three levels deep!  A veritable viper's
nest of looping control structures.  Did we just turn ourselves around and end
up with more loops than we started with?  Strictly speaking we did, but what
matters is how many R-level calls there are, not how many R-level loops.
Particularly once we start getting to large map sizes, the bulk of the
computations are being done in statically compiled code.

Our vectorized algorithm got through the 17 x 17 map in 240 R-level
steps[^step-count].  The original one requires 40,962 for the same map!  The
vectorized algorithm R-level call count will grow with `$log{n}$` where `n` is
the number of rows/cols in the map.  The original transliteration will grow with
`$n^2 .log(n)$`$.  There are a similar number of calculations overall, but as
you can see in the animation the vectorized version "batches" many of them into
single R-level calls to minimize the R interpreter overhead.  So yes, despite
the `for` loops our code is very vectorized, and it shows up in the timings:

```{r vec-timings, echo=FALSE}
dat <- data.frame(Language=c('R-vec', 'JS', 'C'), Time=c(49.15,20.78,6.81))
ggplot(dat) +
  geom_col(aes(Language, Time)) +
  geom_text(aes(Language, Time, label=Time), vjust=-.5) +
  ylab('Time (ms)') +
  ggtitle('RTIN Error Computation Benchmarks', subtitle='257x257 Grid Size') +
  scale_y_continuous(expand = expand_scale(mult = c(0.05, .1)))
```

Now we're in business.  We're beating neither C nor JavaScript, but
we're in the same conversation, and there is room to do better.  This
initial vectorized implementation is geared for clarity over speed.  There are
some simple changes that will double the speed[^double-speed], but we have even
grander aspirations.

Before we move on, a quick recap of some vectorization concepts we applied here:

1. Identify repeating / parallelizable patterns.
2. Structure the data in such a way that the repeating patterns can be processed
   by internally vectorized functions.
3. Allow R-level loops so long as the number of their iterations is small
   relative to those carried out in internally vectorized code.

To achieve 2. we resorted to carefully structured seed data, and
repeated it either explicitly or implicitly with vector recycling.  We did this
both for the actual data, as well as for the indices we used to subset the data
for use in the vectorized operations.

# Follies in Optimization

If you've read any of my previous blog posts you probably realize by now that I
have a bit of a thing &lt;cough&gt;unhealthy obsession&lt;/cough&gt; for making
my R code run fast.  I wouldn't put what's coming next into the "premature
optimization is the root of all evil" category, but rational people will
justifiable question why anyone would spend as much time as I have trying to
make something that doesn't need to be any faster, faster.

Well, I have an itch I want to scratch and I will go ahead and scratch it,
rationality be damned.

Let's look back at our first vectorized implementation.  In particular at the
child error carry over section at the end:

```{r eval=FALSE}
# Carry over child errors
err.val <- do.call(pmax, err.vals)
err.ord <- order(err.val)
errors[o.m[err.ord]] <- err.val[err.ord]
```

You'll notice a seemingly odd `order` call.  This is necessary because for the
"Axis" tiles there is overlap between tiles.  Compare our earlier example with a
single tile and then with the tile repeated to fill a column:

```{r echo=FALSE}
p.ax1 <- transform(p.ax, sgrp=1)
p.ax2 <- rbind(transform(p.ax1, sgrp=2), transform(p.ax1, y=y + .5, sgrp=3))
ax.lvl <- c('Axis Single', 'Axis Column')

p.ax1[['type']] <- factor('Axis Single', levels=ax.lvl)
p.ax2[['type']] <- factor('Axis Column', levels=ax.lvl)

p.all2 <- rbind(p.ax1, p.ax2)
p.all2[['grp']] <- (seq_len(nrow(p.all2)) - 1L) %% 4L

p.arr <- do.call(
  rbind, lapply(split(p.all2, p.all2[c('grp', 'sgrp')]), to_arrows)
)

tris3 <- subset(tris2, type == 'Axis')
tris3[['type']] <- NULL

p2 <- ggplot(tris3) +
  geom_polygon(aes(x=x, y=y, group=id, fill=I(color)), alpha=.2) +
  geom_point(data=points, aes(x=x, y=y), size=.5, shape=3, color='white') +
  geom_point(
    data=data.frame(
      x=.25, y=.5,
      type=factor('Axis Column', levels=ax.lvl)
    ), aes(x=x, y=y),
    color='green', size=10
  ) +
  geom_segment(
    data=subset(p.arr, ptype=='child'),
    arrow=arrow(length=unit(.05, 'inches'), type='closed', angle=25),
    aes(x, y, xend=xend, yend=yend),
    color='grey50'
  ) +
  geom_segment(
    data=subset(p.arr, ptype=='ends'),
    aes(x, y, xend=xend, yend=yend)
  ) +
  geom_point(
    data=subset(p.all2, ptype=='mid'),
    aes(x, y, colour=I('#8da0cb')),
    size=3
  ) +
  geom_point(
    data=subset(p.all2, ptype=='ends'),
    aes(x, y, colour=I('#66c2a5'), fill='grey90'),
    shape=21, size=2
  ) +
  geom_point(
    data=subset(p.all2, ptype=='child'),
    aes(x, y, colour=I('#fc8d62'))
  ) +
  facet_wrap(~type) +
  coord_fixed() +
  thm.blnk

pdim <- gtable_dim(ggplotGrob(p2))
old.opt <- knitr::opts_chunk$get(c('fig.width', 'fig.height'))
do.call(knitr::opts_chunk$set, list(fig.width=pdim[1], fig.height=pdim[2]))
```
```{r tile-overlap, echo=FALSE}
p2
do.call(knitr::opts_chunk$set, old.opt)
```

<span id='vec-inefficiency'></span>
The midpoint highlighted in green is at the overlap of two tiles.  There are two
not-so-great things about this.  The first is that the midpoint estimate and
errors get computed twice, once for each tile.  The second is that we are
carrying over errors from children in different tiles, which is what requires us
to order `err.val` prior to final insertion into the error matrix.  If we didn't
we risk a lesser error from one tile overwriting a larger error from another.
This issue afflicts all the non-peripheral midpoints in the "Axis" tile
arrangement[^not-diag].

A potential improvement is to directly compute the midpoint locations.  As we
can see here they are arranged in patterns that we should be able to compute
without having to resort to tiling:

```{r echo=FALSE}
axis <- c(3, 7, 19, 23, 27, 39, 43, 55, 59, 63, 75, 79)
diag <- c(11, 13, 15, 17) + rep(0:3, each=4) * 2 * 9

p.ax3 <- transform(ids_to_df(axis, err), type='Axis', pcolor='#8da0cb')
p.dg3 <- transform(ids_to_df(diag, err), type='Diagonal', pcolor='#8da0cb')

p.all <- rbind(p.ax3, p.dg3)

p2 <- ggplot(tris2) +
  geom_polygon(aes(x=x, y=y, group=id, fill=I(color)), alpha=.2) +
  geom_point(data=points, aes(x=x, y=y), size=.5, shape=3, color='white') +
  geom_point(data=p.all, aes(x, y), colour='#8da0cb', size=3) +
  facet_wrap(~type) +
  coord_fixed() +
  thm.blnk

pdim <- gtable_dim(ggplotGrob(p2))
old.opt <- knitr::opts_chunk$get(c('fig.width', 'fig.height'))
do.call(knitr::opts_chunk$set, list(fig.width=pdim[1], fig.height=pdim[2]))
```
```{r midpoints-2, echo=FALSE}
p2
do.call(knitr::opts_chunk$set, old.opt)
```

<!--
Show the midpoints in a 9x9 grid, side by side for each type of point.
-->

Furthermore, we can split the midpoints into two groups (A and B below) such
that all points related to a particularly midpoint can be computed with fixed
offsets relative to that point:

```{r echo=FALSE}
p.ax3.a <- cbind(subset(p.ax3, (x * 4) %% 2 == 0), subtype='A')
p.ax3.b <- cbind(subset(p.ax3, (x * 4) %% 2 != 0), subtype='B')

sw.x <- with(p.dg3, ((x * 8 - 1) / 2) %% 2 > 0)
sw.y <- with(p.dg3, ((y * 8 - 1) / 2) %% 2 > 0)
p.dg3.a <- cbind(subset(p.dg3, xor(sw.x, sw.y)), subtype='A')
p.dg3.b <- cbind(subset(p.dg3, !xor(sw.x, sw.y)), subtype='B')

ggplot(rbind(p.ax3.a, p.ax3.b, p.dg3.a, p.dg3.b)) +
  geom_point(aes(x, y, color=I(pcolor))) +
  facet_grid(subtype ~ type)

s <- 1/8
p.ax.c <- rbind(
  transform(p.ax3, x=x+s, y=y+s),
  transform(p.ax3, x=x+s, y=y-s),
  transform(p.ax3, x=x-s, y=y-s),
  transform(p.ax3, x=x-s, y=y+s)
)
p.ax.c <- transform(
  p.ax.c, pfill=ifelse(x < 0 | y < 0 | x > 1 | y > 1, NA, '#fc8d62')
)
p.dg.c <- rbind(
  transform(p.dg3, y=y+s),
  transform(p.dg3, y=y-s),
  transform(p.dg3, x=x+s),
  transform(p.dg3, x=x-s)
)
p.dg.c <- transform(p.dg.c, pfill='#fc8d62')

p2 <- ggplot(tris2) +
  geom_polygon(aes(x=x, y=y, group=id, fill=I(color)), alpha=.2) +
  geom_point(data=points, aes(x=x, y=y), size=.5, shape=3, color='white') +
  geom_point(data=p.all, aes(x, y), colour='#8da0cb', size=3) +
  geom_point(
    data=rbind(p.ax.c, p.dg.c),
    aes(x, y, fill=I(pfill)), color='#fc8d62', shape=21
  ) +
  facet_wrap(~type) +
  coord_fixed() +
  thm.blnk

pdim <- gtable_dim(ggplotGrob(p2))
old.opt <- knitr::opts_chunk$get(c('fig.width', 'fig.height'))
do.call(knitr::opts_chunk$set, list(fig.width=pdim[1], fig.height=pdim[2]))
```
```{r midpoints-child-2, echo=FALSE}
p2
do.call(knitr::opts_chunk$set, old.opt)
```

<div style='background-color: red'>Add arrows to midpoints?</div>

All is not perfect though: "Axis" is again a problem as the peripheral midpoints
end up generating out-of-bounds children, shown as hollow points above.  I won't
get into the details, but handling the out-of-bounds children requires
separate treatment for the left/top/right/bottom periphery as well as the
inner midpoints.  This makes the code [uglier][11], but also faster:

```{r vec-timings-2, echo=FALSE}
dat <- data.frame(Language=c('R-vec-2', 'JS', 'C'), Time=c(15.11,20.78,6.81))
ggplot(dat) +
  geom_col(aes(Language, Time)) +
  geom_text(aes(Language, Time, label=Time), vjust=-.5) +
  ylab('Time (ms)') +
  ggtitle('RTIN Error Computation Benchmarks', subtitle='257x257 Grid Size') +
  scale_y_continuous(expand = expand_scale(mult = c(0.05, .1)))
```

And something remarkable happens as we increase grid sizes:

```{r vec-timings-3, echo=FALSE, warning=FALSE}
library(patchwork)
times <- readRDS('../../static/data/rtini-data.RDS')
names(times)[3] <- 'Language'

p1 <- ggplot(
  times,
  aes(
    x=factor(
      ind, levels=as.character(sort(as.integer(as.character(unique(ind)))))
    ),
    y=values, color=Language
  )
) +
  geom_point(position=position_dodge(width=.6), alpha=.5) +
  ggtitle("Normal Scale") +
  ylab('Seconds') +
  xlab('Grid Size') +
  scale_color_manual(values=c('#66c2a5','#fc8d62','#8da0cb')) +
  NULL

p2 <- ggplot(
  times,
  aes(
    x=factor(
      ind, levels=as.character(sort(as.integer(as.character(unique(ind)))))
    ),
    y=values, color=Language
  )
) +
  geom_point(position=position_dodge(width=.6), alpha=.5) +
  scale_y_log10() +
  ggtitle("Log Scale") +
  ylab('Seconds (log10)') +
  xlab('Grid Size') +
  scale_color_manual(values=c('#66c2a5','#fc8d62','#8da0cb')) +
  NULL

p1 + p2 + plot_layout(guides='collect') +
  plot_annotation(title='RTIN Error Computation Benchmarks vs. Grid Size') &
  theme(legend.position='bottom')
```

The R implementation beats both the C and JS implementations by a factor of
2-3x.  At the lower sizes the overhead of the R calls holds us back, but as the
overall computations increase by `$N^2$`, the number of R calls only increase by
`$log(N)$`.

Another observation is that JS is faster than C at the largest
grid sizes, even though the C implementation is almost identical to the
JS one.  You can tell from the logged plot that it appears that JS has some more
initial overhead, but the scaling rate is slower.  This might be because we use
64-bit doubles in C so we can interface with R, whereas the JS version uses
32-bit floats[^c-vs-js].





They key is to figure out the pattern of offsets that gives us the correct set
of absolute coordinates.  The "Diagonal" triangles are pretty straightforward as
they have a regular pattern.  The "Axis" triangles are more challenging as their
tiling to offsets from "row" to "row" and "column" to "column":

Worse, some of the child points end up out of bounds, so we will need to account
for those.

<!--
Midpoints, but with triangles drawn in, and oob triangles highlighted in a
different color.

Maybe this gets merged into the previous plot.
-->




# Conclusions

Point is to highlight potential techniques for vectorizing R code.

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id='feedback-cont'></div>

# Appendix

## Acknowledgments

* patchwork
* rgl
* rayrender
* Agafonkin
* reshape2
* ggplot2
* ffmpeg
* inline
* Brewer

## R Transliteration

This a reasonably close transliteration of the original [&commat;mourner][5]
[implementation][6] of the [RTIN error computation algorithm][8].  In addition
to the conversion to R, I have made some mostly non-semantic changes so that the
code is easier to instrument, and so that the corresponding visualization occurs
more naturally.  In particular, variables are initialized to NA so they won't
plot until set.  This means the corners of the error matrix remain NA as those
are not computed.

This only computes the error.  Separate code is required to extract meshes.

```{r echo=FALSE}
source(
  '../../static/script/mesh-viz/rtin2.R', deparseCtrl='all', keep.source=TRUE
)
```
```{r}
errors_rtin2
```

## C Transliteration

```{r}
errors_rtin_c <- inline::cfunction(sig=c(terr='numeric', grid='integer'), body="
  int gridSize = asInteger(grid);
  int tileSize = gridSize - 1;
  SEXP errSxp = PROTECT(allocVector(REALSXP, gridSize * gridSize));
  double * terrain = REAL(terr);
  double * errors = REAL(errSxp);
  errors[0] = errors[gridSize - 1] = errors[gridSize * gridSize - 1] =
    errors[gridSize * gridSize - gridSize] = 0;

  int numSmallestTriangles = tileSize * tileSize;
  // 2 + 4 + 8 + ... 2^k = 2 * 2^k - 2
  int numTriangles = numSmallestTriangles * 2 - 2;
  int lastLevelIndex = numTriangles - numSmallestTriangles;

  // iterate over all possible triangles, starting from the smallest level
  for (int i = numTriangles - 1; i >= 0; i--) {

    // get triangle coordinates from its index in an implicit binary tree
    int id = i + 2;
    int ax = 0, ay = 0, bx = 0, by = 0, cx = 0, cy = 0;
    if (id & 1) {
      bx = by = cx = tileSize; // bottom-left triangle
    } else {
      ax = ay = cy = tileSize; // top-right triangle
    }
    while ((id >>= 1) > 1) {
      int mx = (ax + bx) >> 1;
      int my = (ay + by) >> 1;

      if (id & 1) { // left half
        bx = ax; by = ay;
        ax = cx; ay = cy;
      } else { // right half
        ax = bx; ay = by;
        bx = cx; by = cy;
      }
      cx = mx; cy = my;
    }
    // calculate error in the middle of the long edge of the triangle
    double interpolatedHeight =
      (terrain[ay * gridSize + ax] + terrain[by * gridSize + bx]) / 2;
    int middleIndex = ((ay + by) >> 1) * gridSize + ((ax + bx) >> 1);
    double middleError = abs(interpolatedHeight - terrain[middleIndex]);

    if (i >= lastLevelIndex) { // smallest triangles
      errors[middleIndex] = middleError;

    } else { // bigger triangles; accumulate error with children
      double leftChildError =
        errors[((ay + cy) >> 1) * gridSize + ((ax + cx) >> 1)];
      double rightChildError =
        errors[((by + cy) >> 1) * gridSize + ((bx + cx) >> 1)];

      double tmp = errors[middleIndex];
      tmp = tmp > middleError ? tmp : middleError;
      tmp = tmp > leftChildError ? tmp : leftChildError;
      tmp = tmp > rightChildError ? tmp : rightChildError;
      errors[middleIndex] = tmp;
    }
  }
  UNPROTECT(1);
  return errSxp;
")
```


## System Info

```{r child='../../static/script/_lib/flipbook/flipbook.Rmd', results='asis'}
```
<script type='text/javascript'>
const imgDir = '/post/2019-08-23-mesh-reduction-1_files/images/flipbook/';
const fps = 4;
new BgFlipBook({
  targetId: 'flipbook1', imgDir:imgDir, imgStart: 7, imgEnd: 53,
  imgPad: "0000", fps: fps, loop: false
})
new BgFlipBook({
  targetId: 'flipbook2', imgDir:imgDir, imgStart: 803, imgEnd: 844,
  imgPad: "0000", fps: fps, loop: false
})
new BgFlipBook({
  targetId: 'flipbook-vec', imgDir:imgDir + '../flipbook-vec/',
  imgStart: 65, imgEnd: 0119,
  imgPad: "0000", fps: fps, loop: false
})
</script>


[^off-course]: The beauty is I wasn't going anywhere in particular anyway, so
  why not do this.
[^min-complex]: A 5x5 mesh is the smallest mesh that clearly showcases the
  complexity of the algorithm.
[^no-child-error]: We can split this triangle once again, but at that level
  every point on the elevation map coincides with a vertex so there is no error.
[^second-triangle]: The first triangle is not very interesting because the child
  errors are smaller than the parent error so there is no error to carry over.
[^js-timings]: My processor is clocked at a particularly measly 1.2GHz, so it is
  plausible that [&commat;mourner][5] ran his benchmarks on something clocked
  2x-3x faster.  It seems also likely this algorithm is CPU limited as most
  operations do not involve memory access, and the data set is small enough to
  fit in higher level cache.
[^branch-predict]: In particular it seems unlikely the `while` loop condition
  and the `if` statements therein could be easily predicted by a hardware branch
  predictor, and that's assuming JS code can even be branch predicted.
[^grandiloquence]: If nothing else I feel I've earned the right not to be
  embarrassed by misplaced eructations of grandiloquence.
[^nine-nine]: Some tiling issues become apparent at this size that are less
  obvious at smaller sizes.
[^int-vec]: Internally vectorized operations loop through elements of a vector
  with statically compiled code instead of explicit R loops such as `for`,
  `lapply`.
[^first-frame]: First frame in the flipbook, but step 65 in the overall
  animation.
[^simplified-offsets]: The actual implementation needs to handle several special
 cases, such as the largest size "Diagonal" that only contains one hypotenuse
 instead of the typical four, and the smallest size "Axis" that does not have
 any children.  The unused variable `layers` is used to identify the corner
 cases in the full version.
[^linearized]: [&commat;mourner's][6] implementation used this approach, and
  since it saves space and may be faster computationally I adopted it.  To
  preserve my sanity I transpose the coordinates in the actual implementation so
  that the matrix representation aligns with the plotted one (i.e. x corresponds
  to columns, y to rows, albeit with y values in the wrong direction).  In other
  words, `x <- (id - 1) %/% nrow(map)`, and `y <- (id - 1) %% nrow(map)`
[^shift-with-care]: But watch out for row shifts that cause column shifts, or
  column shifts that take you out of bounds.
[^step-count]: We're under-counting as the calls used in the vectorized version
  end to be more complex, and we also have `init_offsets` that hides some calls.
[^double-speed]: As we'll see [later](#vec-inefficiency) the ordering step at
  the end is only required for "Axis" tiles, and can be skipped for the first
  layer altogether as in that layer "Axis" tiles have no children (this is also
  the largest layer so there is a big gain from skipping it).
[^not-diag]: In the "Diagonal" tiles the midpoints are on the inside of the tile
  so there is no overlap issue.
[^spare-you]:  Okay, I'm really sparing myself.  Even with `watcher` putting
  together the algorithm visualizations is quite the production.
[^c-vs-js]: Or it could be any other number of things, I'm guessing and can't be
  bothered testing this with a stand-alone implementation.

When I say this I'm completely relying on no one else being neurotic
  enough to try to optimize the JS code further, as I'm sure it could be.

[5]: https://twitter.com/mourner
[6]: https://observablehq.com/@mourner/martin-real-time-rtin-terrain-mesh
[7]: https://twitter.com/mdsumner/status/1161994475184373761?s=20
[8]: https://www.cs.ubc.ca/~will/papers/rtin.pdf
[9]: /2019/10/30/visualizing-algorithms/
[10]: https://twitter.com/BrodieGaslam/status/1166885035489804289?s=20
[11]: https://github.com/brodieG/rtini/blob/v0.1.0/R/error.R#L98
