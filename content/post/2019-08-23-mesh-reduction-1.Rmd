---
title: "RTINI Part I: The MARTINI Mesh Reduction Algorithm"
author: ~
date: '2020-01-23'
slug: mesh-reduction-1
categories: [R]
tags: [algorithms,viz]
image:
  /post/2019-08-23-mesh-reduction-1_files/images/batch-2.png
imagerect: ~
imagemrgvt: 0%
imagemrghz: 0%
weight: 1
contenttype: article
description: "An illustrated examination of @mourner's MARTINI Javascript mesh
reduction algorithm."
output:
  blogdown::html_page:
    keep_md: true
    md_extensions: +raw_attribute
---

```{r echo=FALSE, child='../../static/chunks/init.Rmd'}
```
```{r echo=FALSE}
suppressMessages(library(ggplot2))
thm.blnk <- list(
  theme(
    axis.text.x=element_blank(), axis.text.y=element_blank(),
    axis.ticks.x=element_blank(), axis.ticks.y=element_blank(),
    panel.grid=element_blank()
  ),
  ylab(NULL),
  xlab(NULL)
)
```

# Off Course, of Course

<!-- this needs to become a shortcode -->
<img
  id='front-img'
  src='/post/2019-08-23-mesh-reduction-1_files/images/batch-2.png'
  class='post-inset-image'
/>

A few months ago a stumbled on Vladimir Agafonkin's ([&commat;mourner][5])
fantastic [observable notebook][6] on adapting the Right-Triangulated Irregular
Networks (RTIN) algorithm to JavaScript.  The subject matter is interesting and
the exposition superb.  I'll admit I envy the elegance of natively interactive
notebooks.

I almost left it at that, but then the irrepressible "but can we do that in R"
bug whispered into my ear.  Since this problem also presents some interesting
visualization challenges, I could hardly help once again going hopelessly
off-course[^off-course] to write a post about it.

Ultimately we're looking to reformulate the MARTINI JavaScript algorithm into
something that can run reasonably efficiently in R.  The loop heaviness of the
current implementation makes it anathema to R.  Of course we could also
trivially port it to C and interface that into R, but where's the fun in that?

Be sure to look at [&commat;mourner's notebook][6] or the original [Will
Evans et al. 1997 paper][8] for more background about the algorithm.  In this
post we'll focus on specific details of the MARTINI implementation and **how**
it does what it does.

# Right Triangles

I will provide some basic background if only to indulge in some ray tracing.
After all, what better way to illustrate a 3D algorithm in?  The algorithm
generates triangle meshes that approximate surfaces with elevations defined on a
regular grid in the x-y plane.  The beloved `volcano` height map that comes
bundled with R is exactly this type of surface.  Approximations can be
constrained to an error tolerance, and here we show the effect of specifying
those tolerance on `volcano` as a percentage of its total height:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><span
  class='bgw-img-wrap-frame bgw-inner'><figure><img
  class='bgw-img-wrap-panel'
  src=/post/2019-08-23-mesh-reduction-1_files/images/surface-1-small.png
  style='max-width: 300px;'
/><figcaption
>Full Detail (N=8,192)</figcaption></figure><figure><img
  class='bgw-img-wrap-panel'
  src=/post/2019-08-23-mesh-reduction-1_files/images/surface-2-small.png
  style='max-width: 300px;'
/><figcaption
>Tolerance: 2% (N=1,538)</figcaption></figure></span><span
  class='bgw-img-wrap-frame bgw-inner'><figure><img
  class='bgw-img-wrap-panel'
  src=/post/2019-08-23-mesh-reduction-1_files/images/surface-3-small.png
  style='max-width: 300px;'
/><figcaption
>Tolerance: 11% (N=122)</figcaption></figure><figure><img
  class='bgw-img-wrap-panel'
  src=/post/2019-08-23-mesh-reduction-1_files/images/surface-4-small.png
  style='max-width: 300px;'
/><figcaption
>Tolerance: 53% (N=13)</figcaption></figure></span></span></div>
```

We're not going to spend time in this post evaluating the effectiveness of
the RTIN approximation method as our focus is on the mechanics of the algorithm.
That said, it's noteworthy that the "2%" approximation reduces the polygon count
over five-fold and looks good without any shading tricks.

One of the things that pulled me into this problem is the desire to better
visualize the relationship between the original mesh and various approximations.
This is easily solved with lines in 2D space with superimposition, but becomes
more difficult with 2D surfaces in 3D space.  Occlusion and other artifacts from
projection of the surfaces onto 2D displays get in the way.  Possible solutions
include juxtaposing in time as in [&commat;mourner's notebook][6] with the
slider, or faceting the 2D space as I did above, but neither lets us see the
details of the difference.

I ended up settling on the ["Ã -la-Pyramide-du-Louvre"][12] style visualization:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><a
  title='Click for full resolution.'
  href=/post/2019-08-23-mesh-reduction-1_files/images/glm-static-3.png><img
  class='bgw-img-wrap-panel bgw-zoomable'
  src=/post/2019-08-23-mesh-reduction-1_files/images/glm-static-small-3.png
  style='max-width: 300px;'
/></a><figcaption
>Tolerance: 2%</figcaption></figure><figure><a
  title='Click for full resolution.'
  href=/post/2019-08-23-mesh-reduction-1_files/images/glm-static-2.png><img
  class='bgw-img-wrap-panel bgw-zoomable'
  src=/post/2019-08-23-mesh-reduction-1_files/images/glm-static-small-2.png
  style='max-width: 300px;'
/></a><figcaption
>Tolerance: 11%</figcaption></figure><figure><a
  title='Click for full resolution.'
  href=/post/2019-08-23-mesh-reduction-1_files/images/glm-static-1.png><img
  class='bgw-img-wrap-panel bgw-zoomable'
  src=/post/2019-08-23-mesh-reduction-1_files/images/glm-static-small-1.png
  style='max-width: 300px;'
/></a><figcaption
>Tolerance: 53%</figcaption></figure></span></div>
```

Look closely (do zoom in!) and you will see where the surface weaves through the
approximation.  You might even notice that all the vertices of the approximation
are on the surface **and** are coincident with its vertices.  This is because
the approximated meshes are made from subsets of the vertices of the original
surface.

Let's flatten the elevation component of the meshes to look at the 2D structure
of the mesh in the x-y plane.  All the triangles are right angled isoceles
triangles in the x-y plane.  This is where the "Right-Triangulated" part of the
algorithm name comes from.

```{r flat-side-by-side, echo=FALSE, fig.width=3.125, fig.height=3.125, fig.show='hold', fig.align='default'}
old_chunk_hook <- knitr::knit_hooks$get('chunk')
knitr::knit_hooks$set(chunk=function(x, options) {
  paste0(
    '<div class=bgw-wide-window><span class=bgw-img-wrap-frame>',
    paste0(old_chunk_hook(x, options), collapse=''),
    '</span></div>'
  )
} )
source('../../static/script/mesh-viz/viz-lib.R')
vsq <- matrix(0, 65, 65)
vsq[1:65, 3:63] <- volcano[1:65,1:61]
vsq[1:65, 1:2] <- volcano[1:65, 1]
vsq[1:65, 64:65] <- volcano[1:65, 61]

elmax <- diff(range(vsq))

tol <- rev(c(50, 9.99754364417931, 1.9))
gold <- '#CCAC00'
mc <-  c(gold, 'grey50', '#CC3322')
mai <- rep(.1, 4)
errors <- rtini::rtini_error(vsq)
meshes <- lapply(tol, rtini::rtini_extract, error=errors)
de <- dim(errors)

xyz <- lapply(
  meshes, function(x) {
    ids <- rbind(matrix(unlist(x), 3L), matrix(unlist(x), 3L)[1,], NA)
    ids_to_xyz(ids, map=vsq, scale=rep(1, 3L))
} )
# switch x/y, xpd not working so we need to work-around
xpdoff <- .05
xyz <- lapply(
  xyz,
  function(w) {
    w$z <- w$x * (1 - xpdoff) + xpdoff/2
    w$x <- ((1 - w$y) * (1 - xpdoff) + xpdoff/2) * 64  + 1
    w$y <- (w$z) * 64 + 1
    w
} )
plot_tri_xy(
  xyz[[1]]$x, xyz[[1]]$y, de, lwd=1, col=mc[1], new=TRUE, mai=mai,
)
plot_tri_xy(
  xyz[[2]]$x, xyz[[2]]$y, de, lwd=1, col=mc[2], new=TRUE, mai=mai,
)
plot_tri_xy(
  xyz[[3]]$x, xyz[[3]]$y, de, lwd=1, col=mc[3], new=TRUE, mai=mai,
)
```
```{r echo=FALSE}
knitr::knit_hooks$set(chunk=old_chunk_hook)
```

Another important feature of this mesh approximation is that it is hierarchical.
When we stack the three meshes we can see that each of the larger triangles in
the low fidelity meshes is exactly replaced by a set of triangles from the
higher fidelity ones:

```{r flat-overlay, echo=FALSE, fig.width=4.2, fig.height=4.2}
plot_tri_xy(
  xyz[[1]]$x, xyz[[1]]$y, de, lwd=3, col=mc[1], mai=mai, new=TRUE, xpd=NA
)
plot_tri_xy(xyz[[2]]$x, xyz[[2]]$y, de, lwd=2, col=mc[2], new=FALSE)
plot_tri_xy(xyz[[3]]$x, xyz[[3]]$y, de, lwd=.75, col=mc[3], new=FALSE)
```

This hierarchy of nested-increasing-resolution meshes is a key part of the
algorithm.  We first compute the approximation errors of every triangle at every
resolution, and once we have that test those errors against a tolerance level to
figure out what set of triangles we want for our approximated mesh.

The algorithm measures the approximation error of each triangle at the
middle of the hypotenuse.  For the sake of exposition we will abandon volcano
and switch to a simple 3x3 elevation surface:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/simple-mesh-fin-1.png
  style='max-width: 300px;'
/><figcaption
>Full Detail</figcaption></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/simple-mesh-fin-2.png
  style='max-width: 300px;'
/><figcaption
>First Approximation</figcaption></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/simple-mesh-fin-3.png
  style='max-width: 300px;'
/><figcaption
>Second Approximation</figcaption></figure></span></div>
```

At the most granular level (gold) there is no error as the mesh corresponds
exactly to the elevation surface.  At the first level of approximation (silver)
error appears around the periphery of the surface as that is where the triangle
edges skip over surface vertices.  We show these errors as checkered cylinders.
At the coarsest level of approximation (copper) we generate additional error on
the center of the surface.

Errors are measured at the middle of the hypotenuse of the triangles.  This is
not arbitrary: as we merge triangles together to increase coarseness, the
vertices that are lost correspond to that point of the parent triangle.  An
implication is that we never measure errors from different levels of
approximation at overlapping locations, so we can store the errors in a matrix
the same size as the surface, whether a simple 3 x 3 surface or a more complex
one like `volcano`[^volcano-size]:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/err-simple.png
  style='max-width: 400px;'
/><figcaption
>3 x 3 Simple Surface Errors</figcaption></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/err-volc.png
  style='max-width: 400px;'
/><figcaption
>65 x 65 Volcano Errors</figcaption></figure></span></div>
```

For clarity the surface was constructed so that all the errors are in the same
direction and directly visible from our observation point.  In practice the
errors can occur on either side of the surface, and we would record their
absolute values.  The corners of the matrix will never have any error as even
the coarsest level of approximation will have vertices at those locations.

# Surface Extraction

We can now compare our recorded errors to different tolerance levels, and use
that to decide what triangles to draw.  A possible algorithm is to draw the
child triangles of any triangle for which its error or the error of any of its
descendants exceeds the threshold.  Here we represent a possible threshold as
the "water" level; errors visible above it must be addressed:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/water-fin-1.png
  style='max-width: 300px;'
/></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/composite-mesh-fin-1.png
  style='max-width: 300px;'
/></figure></span></div>
```

For each of the three errors that exceeds our threshold we split the
corresponding triangle at its hypotenuse midpoint to create a vertex at the
problematic point and remove the error.  This requires us to draw the highest
detail triangles for the bottom right part of the surface (gold), but lets us
use the first level of approximation for the top left.

As we increase our tolerance we can extend the approximation to the bottom of
the surface:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/water-fin-2.png
  style='max-width: 300px;'
/></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/composite-mesh-fin-2.png
  style='max-width: 300px;'
/></figure></span></div>
```

But if we further increase our tolerance we run into problems:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/water-fin-3.png
  style='max-width: 300px;'
/></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/composite-mesh-fin-3.png
  style='max-width: 300px;'
/></figure></span></div>
```

Even though we are willing to tolerate the error from using the highest level of
approximation for the top left part of the surface, doing so introduces a gap in
our surface.  One solution to this is to ensure that the error at any given
point is the greater of the actual error, or the errors associated with its
children.  <span id='carry-over-viz'></span>

```{=html}
<br />
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/err-carry-1.png
  style='max-width: 300px;'
/></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/err-carry-2.png
  style='max-width: 300px;'
/></figure></span></div>
<br />
```

This ensure that the mesh has no T intersections and as such is
continuous:

```{=html}
<div class=bgw-wide-window>
<span class=bgw-img-wrap-frame><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/water-fin-4.png
  style='max-width: 300px;'
/></figure><figure><img
  class=bgw-img-wrap-panel
  src=/post/2019-08-23-mesh-reduction-1_files/images/post-proc-res/composite-mesh-fin-4.png
  style='max-width: 300px;'
/></figure></span></div>
```

<!--
Use same water level as the one that caused the problem before.

Image is side by side (left/right):

* Left: error matrix, water level added
* Right: mesh approximation, complete mess
-->

# Algorithm in Action

We now have a rough idea of the algorithm.  Unfortunately I learned the hard way
that "a rough idea" is about as useful when refactoring an algorithm as it is to
know that a jar of nitroglycerin is "kinda dangerous" when you're tasked with
transporting it.  In order to safely adapt the algorithm to R we need
full mastery of the details.  I'm still covered in gouges and bruises from the
sharp corner cases and the off-by-who-knows-how-many-errors I smacked into.

The outline of the algorithm is as follows:

```{r eval=FALSE}
for (every triangle in mesh) {
  while (searching for triangle vertex coordinates) {
    # ... Compute coordinates of vertices `a`, `b`, and `c` ...
  }
  # ... Compute approximation error at midpoint of `a` and `b` ...
}
```

The `for` loop iterates as many times as there are triangles in the mesh.  In
our [3 x 3 grid](#three-by-three) there are six triangles (4 at the first level
of approximation, and another 2 at the next level).  For each of these
triangles, the `while` loop identifies the vertex coordinates `$(ax,ay)$`,
`$(bx,by)$`, and `$(cx,cy)$`.  This allow us to compute the error at the
midpoint of the hypotenuse.

Let's observe the algorithm find the first triangle to get a sense for how it
works.  We'll use a 5 x 5 grid this time[^min-complex], and regrettably abandon
the 3D realm for the sake of clarity.  The visualization is generated with the
help of [`watcher`][9], and it displays the state of the system _after_ the
evaluation of the highlighted line:

<div id='flipbook1' class='bgw-wide-window'></div>

When the `while` loop ends the target triangle has been found, shown in yellow.
In the following frames the algorithm computes the location of the midpoint `m`
of the points `a` and `b` (these two always define to the hypotenuse), computes
the approximation error at that point, and records it in the error matrix.

Since this is the most granular approximation level there are no child errors to
propagate[^no-child-error] and the algorithm skips over that part.

What happens next is pretty remarkable.  At the top of the loop we're about to
repeat we have:

```{r eval=FALSE}
for (i in (nTriangles - 1):0) {
  id <- i + 2L
  # ... rest of code omitted
```

This innocent-seeming reset of `id` will ensure that the rest of the logic
will find the next smallest triangle, and the next after that, and so forth:

<img class='aligncenter'
  src='/post/2019-08-23-mesh-reduction-1_files/images/mesh-anim-4-abreast.png'
/>

At this point we've tiled the whole surface at the lowest approximation level,
but we need to repeat the process for the next approximation level.
"Magically" the smaller initial `id` values lead the `while` loop to exit
earlier, at the next larger triangle size.  Here is another flipbook showing the
search for the second triangle at the next approximation level[^second-triangle]:

<div id='flipbook2' class='bgw-wide-window'></div>

In the last few frames we can see the error from the children elements (labeled
`lc` and `rc` for "left child" and "right child") carried over into the current
target point.  As we [saw earlier](#carry-over-viz) this ensures there are no
gaps in our final mesh.  Each error point will have errors carried over from up
to four neighboring children.  For the above point the error from the remaining
children is carried over a few hundred steps later:

<img class='aligncenter'
  src='/post/2019-08-23-mesh-reduction-1_files/images/mesh-anim-2-abreast.png'
/>

In this case it the additional child errors are smaller so they do not change
the outcome, but it is worth noting that each hypotenuse midpoint may add up to
four child errors.

Here is the whole thing at 60fps:

<video id=mesh-anim style='display: block; margin: 0 auto;' controls loop>
<source
  src='/post/2019-08-23-mesh-reduction-1_files/images/out.mp4'
  type="video/mp4"
/>
</video>

# R's Kryptonite

We essentially transliterated the JavaScript algorithm into R.  While this
works, it does so slowly.  Compare the timings of our R function, the JavaScript
version, and finally a [C version](c-version):

```{r rtin-timings, echo=FALSE}
dat <- data.frame(Language=c('R', 'JS', 'C'), Time=c(2500,20.78,6.81))
ggplot(dat) +
  geom_col(aes(Language, Time)) +
  geom_text(aes(Language, Time, label=Time), vjust=-.5) +
  ylab('Time (ms)') +
  ggtitle('RTIN Error Computation Benchmarks', subtitle='257x257 Grid Size') +
  scale_y_continuous(expand = expand_scale(mult = c(0.05, .1)))
```

Our JavaScript timings are in line with those &commat;mourner reports
(~20ms)[^js-timings].
<div style='background-color: red'>
NOTE: on Chrome we're seeing timings start at ~38 and then
eventually drop to and stay at ~20ms.  The timings here are Firefox.  Mesh
extraction on Chrome starts at ~11ms and drops to ~1.5-2ms
</div>

I was surprised by how well JS stacks up relative to C.  Certainly a ~7x gap is
nothing to sneeze at, but I expected it to be bigger given JS is interpreted.
On the other hand R does terribly.  This is expected as R is not designed to run
tight loops on scalar values.  We could take some steps to get the loop to run a
little faster, but not to run fast enough to run real time at scale.

So what are we to do?  Well, as @mdsumner [enthusiastically puts it][7]:

> JavaScript is cool, C++ is cool. But #rstats can vectorize the absolute shit
> out of this. No loops until triangle realization
>
> -- @mdsumner

Indeed, we don't need to watch the [algorithm play out](#mesh-anim) very long to
see that its elegance comes at the expense of computation.  Finding each
triangle requires many steps, and they seem unlikely to be friendly to
microarchitecture optimizations[^branch-predict].

The triangles coordinates do not depend on each other, so it should be possible
calculate them directly based on the grid size.  We cannot completely vectorize
the calculation as we need to iteratively carry over errors across approximation
levels.  Fortunately the level count will be proportional to
`$\log_2(\sqrt{points})$`, so we only need to explicitly loop a few times.

Before we get too deep into this, I'd like to make it clear that vectorizing the
algorithm for use in R is interesting, but also _mostly_ useless.  The
JavaScript version runs plenty fast, and if you must have it in R you can
trivially transliterate the code [into C](#c-transliteration), which can then be
accessed from R.  The vectorization version does have some use as it is not
constrained to `$2^k + 1$` square grids.

So, let's go vectorize the absolute $#!+ out of this.  It'll be easy, they
said.  It'll be fun.  Right.

# Conclusions

Point is to highlight potential techniques for vectorizing R code.

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id='feedback-cont'></div>

# Appendix

## Acknowledgments

* patchwork
* rgl
* rayrender
* Agafonkin
* reshape2
* ggplot2
* ffmpeg
* inline
* Brewer
* dplyr for bind_rows

## R Transliteration

This a reasonably close transliteration of the original [&commat;mourner][5]
[implementation][6] of the [RTIN error computation algorithm][8].  In addition
to the conversion to R, I have made some mostly non-semantic changes so that the
code is easier to instrument, and so that the corresponding visualization occurs
more naturally.  In particular, variables are initialized to NA so they won't
plot until set.  This means the corners of the error matrix remain NA as those
are not computed.

This only computes the error.  Separate code is required to extract meshes.

```{r echo=FALSE}
source(
  '../../static/script/mesh-viz/rtin2.R', deparseCtrl='all', keep.source=TRUE
)
```
```{r}
errors_rtin2
```

## C Transliteration

```{r}
errors_rtin_c <- inline::cfunction(sig=c(terr='numeric', grid='integer'), body="
  int gridSize = asInteger(grid);
  int tileSize = gridSize - 1;
  SEXP errSxp = PROTECT(allocVector(REALSXP, gridSize * gridSize));
  double * terrain = REAL(terr);
  double * errors = REAL(errSxp);
  errors[0] = errors[gridSize - 1] = errors[gridSize * gridSize - 1] =
    errors[gridSize * gridSize - gridSize] = 0;

  int numSmallestTriangles = tileSize * tileSize;
  // 2 + 4 + 8 + ... 2^k = 2 * 2^k - 2
  int numTriangles = numSmallestTriangles * 2 - 2;
  int lastLevelIndex = numTriangles - numSmallestTriangles;

  // iterate over all possible triangles, starting from the smallest level
  for (int i = numTriangles - 1; i >= 0; i--) {

    // get triangle coordinates from its index in an implicit binary tree
    int id = i + 2;
    int ax = 0, ay = 0, bx = 0, by = 0, cx = 0, cy = 0;
    if (id & 1) {
      bx = by = cx = tileSize; // bottom-left triangle
    } else {
      ax = ay = cy = tileSize; // top-right triangle
    }
    while ((id >>= 1) > 1) {
      int mx = (ax + bx) >> 1;
      int my = (ay + by) >> 1;

      if (id & 1) { // left half
        bx = ax; by = ay;
        ax = cx; ay = cy;
      } else { // right half
        ax = bx; ay = by;
        bx = cx; by = cy;
      }
      cx = mx; cy = my;
    }
    // calculate error in the middle of the long edge of the triangle
    double interpolatedHeight =
      (terrain[ay * gridSize + ax] + terrain[by * gridSize + bx]) / 2;
    int middleIndex = ((ay + by) >> 1) * gridSize + ((ax + bx) >> 1);
    double middleError = abs(interpolatedHeight - terrain[middleIndex]);

    if (i >= lastLevelIndex) { // smallest triangles
      errors[middleIndex] = middleError;

    } else { // bigger triangles; accumulate error with children
      double leftChildError =
        errors[((ay + cy) >> 1) * gridSize + ((ax + cx) >> 1)];
      double rightChildError =
        errors[((by + cy) >> 1) * gridSize + ((bx + cx) >> 1)];

      double tmp = errors[middleIndex];
      tmp = tmp > middleError ? tmp : middleError;
      tmp = tmp > leftChildError ? tmp : leftChildError;
      tmp = tmp > rightChildError ? tmp : rightChildError;
      errors[middleIndex] = tmp;
    }
  }
  UNPROTECT(1);
  return errSxp;
")
```


## System Info

```{r child='../../static/script/_lib/flipbook/flipbook.Rmd', results='asis'}
```
<script type='text/javascript'>
const imgDir = '/post/2019-08-23-mesh-reduction-1_files/images/flipbook/';
const fps = 4;
new BgFlipBook({
  targetId: 'flipbook1', imgDir:imgDir, imgStart: 7, imgEnd: 53,
  imgPad: "0000", fps: fps, loop: true
})
new BgFlipBook({
  targetId: 'flipbook2', imgDir:imgDir, imgStart: 803, imgEnd: 844,
  imgPad: "0000", fps: fps, loop: true
})
</script>


[^off-course]: The beauty is I wasn't going anywhere in particular anyway, so
  why not do this.
[^volcano-size]: Yes, yes, `volcano` is not really 65 x 65, more on that in a
  bit.
[^min-complex]: A 5x5 mesh is the smallest mesh that clearly showcases the
  complexity of the algorithm.
[^no-child-error]: We can split this triangle once again, but at that level
  every point on the elevation map coincides with a vertex so there is no error.
[^second-triangle]: The first triangle is not very interesting because the child
  errors are smaller than the parent error so there is no error to carry over.
[^js-timings]: My processor is clocked at a particularly measly 1.2GHz, so it is
  plausible that [&commat;mourner][5] ran his benchmarks on something clocked
  2x-3x faster.  It seems also likely this algorithm is CPU limited as most
  operations do not involve memory access, and the data set is small enough to
  fit in higher level cache.
[^branch-predict]: In particular it seems unlikely the `while` loop condition
  and the `if` statements therein could be easily predicted by a hardware branch
  predictor, and that's assuming JS code can even be branch predicted.



[5]: https://twitter.com/mourner
[6]: https://observablehq.com/@mourner/martin-real-time-rtin-terrain-mesh
[7]: https://twitter.com/mdsumner/status/1161994475184373761?s=20
[8]: https://www.cs.ubc.ca/~will/papers/rtin.pdf
[9]: /2019/10/30/visualizing-algorithms/
[10]: https://twitter.com/BrodieGaslam/status/1166885035489804289?s=20
[11]: https://github.com/brodieG/rtini/blob/v0.1.0/R/error.R#L98
[12]: https://en.wikipedia.org/wiki/Louvre_Pyramid



Blah blah.

