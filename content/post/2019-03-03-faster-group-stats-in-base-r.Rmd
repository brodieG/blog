---
title: Faster Group Stats in Base R
author: ~
date: '2019-03-03'
slug: faster-group-stats-in-base-r
categories: []
tags: []
image: /front-img/default.png
imagerect: ~
imagemrgvt: 0%
imagemrghz: 0%
draft: true
weight: 1
contenttype: article
description: Front page summary
---

```{r echo=FALSE}
options(digits=3)
library(ggplot2)
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
```

# blah blah

<!-- this needs to become a shortcode -->
<img
  id='front-img' src='/front-img/default.png'
  class='post-inset-image'
/>

# Baseline

As we saw in the prior
In one corner, the venerable `tapply`:

```{r eval=FALSE}
RNGversion("3.5.2"); set.seed(42)
n     <- 1e7
n.grp <- 1e6
grp   <- sample(n.grp, n, replace=TRUE)
x     <- runif(n)
y     <- runif(n)

system.time(x.ref <- c(tapply(x, grp, sum)))
```
```
   user  system elapsed
  8.539   0.321   9.030
```

In the other, `data.table`.

```{r}
library(data.table)
DT <- data.table(grp, x)
setDTthreads(1)
system.time(x.dt <- DT[, sum(x), keyby=grp][[2]])
```
```
   user  system elapsed
  0.982   0.059   1.048
```

We use one thread for more stable results, but also because for the final
computation we are planning on doing multi-threaded did not make a huge
difference on my system.  For this particular task multi threading can provide a
substantial boost to data table performance.

# It's All In the Capitalization

Our first solution uses `base::rowsum`.  As is clearly indicated by the lack of
capitalization and pluralization, `base::rowsum` is completely different from
`base::rowSums`.  Further, anyone who has read the "Semantics of Capitalization,
Punctuation, and Pluralization in R Function Names" memo can infer that the
former sums rows within groups, and the latter sums columns within rows.

You did get the memo, right?  Good.

Even though the function names are self explanatory, we will illustrate for
completeness:

```{r}
mx <- matrix(1:8, 4, byrow=TRUE) * 10 ^ (0:3)
mx
rowsum(mx, group=rep(c('odd', 'even'), 2))
```

`rowsum` preserved the two columns, but collapsed the rows by the `group` value.

```{r eval=FALSE}
rowSums(mx)       # note: output has been edited for clarity
```
```{r echo=FALSE}
matrix(rowSums(mx))
```

`rowSums` collapsed the columns but preserved the rows.  Normally `rowSums`
returns a vector, but here we display it as a one column matrix so the
relationship to the input matrix is clear.

I have known about `rowSums` for a long time, and I only more recently
discovered `rowsum`.  Imagine the excitement a taxidermist might feel on
realizing they were just given the corpse of an albino fox, and not that of a
white cat as they initially thought.  That should capture the magnitude _and_
significance of my excitement when I realized what `rowsum` does.

How can a base R function possibly compete for excitement with a dead fox?
Well, there are many base R functions that compute on arbitrary groups, and many
base R functions that work directly in compiled code, but as far as I know base
R functions that compute on arbitrary groups in compiled code are
rare[^knowledge-caveat].  This is very useful:

```{r eval=FALSE}
system.time(x.rs <- rowsum(x, grp))
```
```
   user  system elapsed
  2.505   0.096   2.626
```
```{r eval=FALSE}
all.equal(c(x.ref), c(x.rs), check.attributes=FALSE)
```
```
[1] TRUE
```

Almost four times faster than the original `tapply` solution, and not far from
the `data.table` one.

```{r echo=FALSE}
times <- data.frame(
  Function=c('tapply', 'rowsum', 'data.table'),
  time=c(9.030, 2.626, 1.126)
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col() +
  # facet_wrap(~Function) +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

I've wondered if I am alone in my delayed awareness of `rowsum`, but my attempts
to measure the relative popularity of the two functions via search engine were
fruitless.  Given the glaring semantic cues in `rowsum` vs `rowSums` I would
have thought they would do better...  Thankfully years of using "R" as a search
term have inured me to this type of search disappointment.

# Pixie Dust

Sorting values prior to running `unique` on them makes `unique` faster:

```{r eval=FALSE}
grp.o <- grp[order(grp)]
system.time(unique(grp.o))
```
```
   user  system elapsed
  0.453   0.057   0.514
```
```{r eval=FALSE}
system.time(unique(grp))
```
```
   user  system elapsed
  1.354   0.067   1.436
```

Internally `unique` uses a [hash table][2] to detect duplicate values, so it may
seem odd that the lookup speed is affected by ordering as the hashing time
should not be affected by the input order.  Most likely it is some combination
of caching and [branch prediction][3].

With respect to caching, In the sorted version, we will look up the same key
repeatedly, which presumably means the corresponding value will be warm and snug
in the [highest speed cache][4].  In the unsorted version, we will be jumping
all over the hash table, which will almost certainly require looking up values
in at best slower cache, or more likely in main memory.  Similarly, in branch
prediction, the processor will predict well just by assuming that a value
will be a duplicate when the duplicates are sequential, as they are in the
sorted case.  I have no idea if this is actually what is going on or not, but it
seems likely, and I'm tickled pink by the thought that I'm noticing these
effects from R-level for the first time.

Now, for why I'm ranting about this, it turns out that the combined operation of
sorting and `unique`ing is faster than the just `unique`ing:

```{r eval=FALSE}
system.time(unique(grp[order(grp)]))
```
```
   user  system elapsed
  0.909   0.158   1.081
```

This also has a big impact on `rowsum`, as it uses both [`unique`][6] on top of
its own [hash table][5]:

```{r eval=FALSE}
system.time({
  o <- order(grp)
  x.rs2 <- rowsum(x[o], grp[o], reorder=FALSE)
})
```
```
   user  system elapsed
  1.283   0.105   1.430
```

We are now within striking distance of single-threaded `data.table`:

```{r echo=FALSE}
times <- data.frame(
  Function=c('tapply', 'rowsum', 'rowsum-sorted', 'data.table'),
  time=c(9.030, 2.626, 1.430, 1.048)
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col() +
  # facet_wrap(~Function) +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

Now, you might be thinking, "oh, so maybe `data.table` isn't so special after
all".  Well, the only reason this `order` pixie dust business is even possible
is because `order` is so fast, and `order` is so fast because `data.table`
contributed their pixie dust to R:

<div class=>
```{r echo=FALSE}
blogdown::shortcode('tweet', '1106231241488154626')
```
</div>

One last thing before we move on: knowing that the data is sorted opens up
different algorithms for many tasks such as `unique` and `rowsum`.  Combined
with the ability to sort things quickly, we can re-implement `unique` and
`rowsum` to track groups by pre-sorting and comparing adjacent values instead of
using hashtables.  We pluralized them to indicate these are the sorting versions
of the functions, or maybe that they are the speedy versions.  My dog ate the
memo so I don't know anymore[^love-r-but]...

Here is what happens with `uniques`:<span id=unique-c></span>

```{r eval=FALSE}
uniques <- function(x, res.sorted=FALSE) {
  o <- order(x)
  xo <- x[o]
  i <- .uniques(xo)          # C code: see appendix
  if(!res.sorted) xo[i][order(o[i])] else xo[i]
}
system.time(uniques(grp))
```
```
   user  system elapsed
  0.587   0.018   0.615
```

This is almost twice as fast as the original version with the sorted data, and
it could be made faster if we were okay with getting results back sorted.
Similar story for `rowsums`:

```{r eval=FALSE}
rowsums <- function(x, group) {
  o <- order(group)
  .rowsums(x[o], group[o])    # C code: see appendix
}
system.time(rowsums(x, grp))
```
```
   user  system elapsed
  0.820   0.022   0.846
```

The implementations are not equivalent, but for these specific inputs they
should be comparable.  More details [in the appendix](#sorting-versions).

The main point I'm trying to make here is that it is a **big deal** that `order`
can order fast enough that we can switch what algorithms we use downstream.  A
big thank you to team `data.table` for sharing the pixie dust.

# So You Think You Can Sum?


Slope:
```{r eval=FALSE}
system.time({
o <- order(grp)
go <- grp[o]
xo <- x[o]
xs <- rowsum(xo, go)
xn <- rowsum(rep(1L, length(go)), go)
xi <- rep(seq_along(xs), xn)
xu <- (xs/xn)[xi]

yo <- y[o]
ys <- rowsum(yo, go)
yu <- (ys/xn)[xi]
x_ux <- xo - xu
y_uy <- yo - yu

rowsum(x_ux * y_uy, go) / rowsum(x_ux ^ 2, go)
})
```
```
   user  system elapsed
  4.655   0.772   5.516
```


```{r eval=FALSE}
# Note: adapted to handle na.rm as per winvector, don't necessarily
# handle corner cases correctly (0, 1 length vectors, others?)

# Note: this stuff is only fast because order(, method="radix") is fast
# Doesn't handle Infinite values

sum_g2 <- function(x, grp, na.rm=FALSE) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.rle.c <- cumsum(grp.rle[['lengths']])
  x.ord <- x[ord]
  has.na <- anyNA(x)

  if(has.na) {
    na.x <- is.na(x)
    x.ord[na.x] <- 0
  } else na.x <- logical()

  x.grp.c <- cumsum(x.ord)[grp.rle.c]
  x.grp.c[-1L] <- x.grp.c[-1L] - x.grp.c[-length(x.grp.c)]

  if(!na.rm && has.na)
    x.grp.c[match(grp.ord[na.x], grp.rle[['values']])] <- NA

  structure(x.grp.c, groups=grp.rle[['values']], n=grp.rle[['lengths']])
}
system.time(sum_g2(x, grp))
```
```
   user  system elapsed
  1.186   0.374   1.634
```
Try slope:
```{r eval=FALSE}

sum_grp1_int <- function(x, last.in.group) {
  x.g <- cumsum(x)[last.in.group]
  x.g[-1L] <- x.g[-1L] - x.g[-length(x.g)]
  x.g
}
sum_grp1 <- function(x, last.in.group, precise=FALSE){
  x.grp <- sum_grp1_int(x, last.in.group)
  if(precise) {
    x[last.in.group] <- x[last.in.group] - x.grp
    x.grp + sum_grp1_int(x, last.in.group)
  } else x.grp
}
slope1 <- function(x, y, grp, precise=FALSE) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.rle.c <- cumsum(grp.rle[['lengths']])
  x.ord <- x[ord]
  y.ord <- y[ord]

  x.grp.c <- sum_grp1(x.ord, grp.rle.c, precise=precise)
  y.grp.c <- sum_grp1(y.ord, grp.rle.c, precise=precise)

  ux <- x.grp.c / grp.rle[['lengths']]
  uy <- y.grp.c / grp.rle[['lengths']]

  xi <- rep(seq_along(ux), grp.rle[['lengths']])
  x_ux <- x.ord - ux[xi]
  y_uy <- y.ord - uy[xi]

  x_ux.y_uy <- sum_grp1(x_ux * y_uy, grp.rle.c, precise=precise)
  x_ux2 <- sum_grp1(x_ux ^ 2, grp.rle.c, precise=precise)
  x_ux.y_uy / x_ux2
}
system.time(slope1(x, y, grp, precise=FALSE))
```
```
   user  system elapsed
  2.049   0.805   2.871
```
Precision is garbage though:
```
> all.equal(res.slope.dt1[[2]], res4, tol=1e-2)
[1] TRUE
> all.equal(res.slope.dt1[[2]], res4, tol=1e-3)
[1] FALSE
```
But we can make it precise
```{r eval=FALSE}
system.time(slope1(x, y, grp, precise=TRUE))
```
```
   user  system elapsed
  2.481   1.087   3.754
  # second faster timing was from a fresh session, repeated several times
   user  system elapsed
  2.399   0.933   3.348
```

```{r eval=FALSE}
sum_g3 <- function(x, grp, na.rm=TRUE) {
  ord <- order(grp)
  id.ord <- id[ord]
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  max.grp <- max(grp.rle[['lengths']])

  # this NA handling doesn't work b/c for na.rm=FALSE you still get NAs

  res <- matrix(NA_real_, ncol=length(grp.rle[['lengths']]), nrow=max.grp)

  # each group that isn't as long as the longest group needs padding

  rle.len <- grp.rle[['lengths']]
  grp.pad <- max.grp - rle.len
  id.raw <- rep(1L, length(x))
  id.raw[(cumsum(rle.len) + 1L)[-length(rle.len)]] <-
    grp.pad[-length(rle.len)] + 1L
  id <- cumsum(id.raw)

  res[id] <- x[ord]
  structure(colSums(res, na.rm=na.rm), groups=grp.rle[['values']])
}
system.time(sum_g3(x, grp))
```
```
   user  system elapsed
  1.186   0.374   1.634
```
```{r eval=FALSE}
# lens: how long each group is
# maxlen: longest group
sum_grp2 <- function(x, lens, maxlen, mode='sum') {

  res <- matrix(NA_real_, ncol=length(lens), nrow=maxlen)

  # Generate indices that will map to the correct spots in `res` from `x`,
  # which means add whatever padding we need to the index value for the next
  # column

  len_1 <- lens[-length(lens)]
  grp.pad <- (maxlen + 1L) - len_1
  id.raw <- rep(1L, length(x))
  len_1[1L] <- len_1[1L] + 1L
  id.raw[cumsum(len_1)] <- grp.pad
  id <- cumsum(id.raw)

  # Inject the x values according to these indices that should place each gropu
  # in a column

  res[id] <- x

  if(identical(mode, 'sum')) colSums(res, na.rm=TRUE)
  else if(identical(mode, 'mean')) colMeans(res, na.rm=TRUE)
  else stop("Invalid mode")
}
slope2 <- function(x, y, grp) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.len <- grp.rle[['lengths']]
  max.grp <- max(grp.len)

  xo <- x[ord]
  xi <- rep(seq_along(grp.len), grp.len)
  xu <- sum_grp2(xo, grp.len, max.grp, 'mean')
  x_ux <- xo - xu[xi]

  yo <- y[ord]
  yu <- sum_grp2(yo, grp.len, max.grp, 'mean')
  y_uy <- yo - yu[xi]

  x_ux.y_uy <- sum_grp2(x_ux * y_uy, grp.len, max.grp, 'sum')
  x_ux2 <- sum_grp2(x_ux ^ 2, grp.len, max.grp, 'sum')

  x_ux.y_uy / x_ux2
}
slope2a_int <- function(xo, yo, lens) {
  max.grp <- lens[1]

  xi <- rep(seq_along(lens), lens)
  xu <- sum_grp2(xo, lens, max.grp, 'mean')
  x_ux <- xo - xu[xi]

  yu <- sum_grp2(yo, lens, max.grp, 'mean')
  y_uy <- yo - yu[xi]

  x_ux.y_uy <- sum_grp2(x_ux * y_uy, lens, max.grp, 'sum')
  x_ux2 <- sum_grp2(x_ux ^ 2, lens, max.grp, 'sum')

  x_ux.y_uy / x_ux2
}
slope2a <- function(x, y, grp, splits=5) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.len <- grp.rle[['lengths']]

  ord2 <- order(rep(grp.len, grp.len), decreasing=TRUE)
  ordg <- order(grp.len, decreasing=TRUE)
  grp.len.o <- grp.len[ordg]
  len.max <- grp.len.o[1L]  # will break if no groups
  len.min <- grp.len.o[length(grp.len.o)]

  # order the inputs

  ord3 <- ord[ord2]
  xo <- x[ord3]
  yo <- y[ord3]
  go <- grp.ord[ord2]

  # simple initial cut, just cut into equal splits

  cuts <- as.integer(
    round(seq(1L, length(grp.len) + 1L, length.out=splits + 1L))
  )
  grp.len.o.c <- cumsum(c(1L, grp.len.o))
  res <- vector("list", splits)

  for(i in seq_len(splits)){
    # Figure out that starting and ending elements for each group

    start.g <- cuts[i]
    end.g <- cuts[i + 1L]

    start <- grp.len.o.c[start.g]
    end <- grp.len.o.c[end.g] - 1L

    idx.g <- start.g:(end.g - 1L)
    idx <- start:end

    res[[i]] <- slope2a_int(xo[idx], yo[idx], grp.len.o[idx.g])
  }
  # Reorder back in ascending group order instead of group size order

  res.fin <- numeric(length(grp.len))
  res.fin[ordg] <- unlist(res)
  res.fin
}
system.time(slope2(x, y, grp))
RNGversion("3.5.2"); set.seed(42)
x2 <- runif(100)
y2 <- runif(100)
g2 <- sample(1:10, 100, rep=T)
```
```
   user  system elapsed
  2.827   0.932   3.807
  # can't reproduce the earlier timings...
   user  system elapsed
  3.185   1.285   4.783
  # now I can ...
```
```{r eval=FALSE}
system.time(slope2a(x, y, grp))
```
```
   user  system elapsed
  2.998   1.055   4.082
```
```{r eval=FALSE}
dummy <- function(sizes) {
  res <- vector("list", length(sizes))
  for(i in seq_along(sizes)) {
    mx <- matrix(numeric(), nrow=sizes[i], ncol=as.integer(1e6 / length(sizes)))
    res[[i]] <- colSums(mx, na.rm=TRUE)
  }
}
system.time(dummy(c(28, 21, 14, 7, 1)))
```

```{r eval=FALSE}

DT <- copy(DT.raw)
system.time(res.ref <- DT[, sum(x), keyby=grp][['V1']])
#   user  system elapsed
#  1.071   0.134   1.216
system.time(res <- sum_g2(x, grp))
#   user  system elapsed
#  1.286   0.309   1.692
all.equal(res, res.ref, check.attributes=FALSE) # TRUE

system.time(res2 <- sum_g2(x, grp))
system.time(res3 <- sum_g3(x, grp))
system.time(res4 <- rowsum(x, grp))

sum_winvector <- function(DF) {
  odata <- DF[order(DF$grp),,drop=FALSE]
  first_indices <- mark_first_in_each_group(odata, "grp")
  sum_g(odata[['x']], first_indices)
}
```
```
  user  system elapsed
 1.810   0.740   2.651
```
Note on vector size[^vec-size].


# Conclusions

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id='feedback-cont'></div>

# Appendix

## Acknowledgments

* Oleg Sklyar, Duncan Murdoch, Mike Smith, Dirk Eddelbuettel, Romain Francois,
  Karline Soetaert for [`inline`][7].

## Sorting Versions

I quickly threw together C functions that `unique` and `rowsum` on sorted data.
They support a very narrow set of inputs, but for those inputs the timing
comparisons relative to the base equivalents should be fair.  `.rowsums` does
benefit from being able to assume the input will be a vector and not a matrix,
but that should be a small difference.

I tried two variations from the one we looked at in the [main body](#unique-c)
of the post that generally support the timings against the base functions with
input sorting:

* 1e8 vector with ~10 sized groups
* 1e7 vector with ~1 sized groups

`rowsums` preserves the ~2x advantage in both scenarios.  `uniques` maintains
the advantage for the first scenario, but in the second is at parity.  This is
because for `uniques` we need to unsort the result and when there are no
duplicates the vector to unsort is large.  If we were allowed to return the
results sorted then `uniques` is faster, but by a smaller margin than with the
original example.

```{r eval=FALSE}
.uniques <- inline::cfunction(sig=c(x='integer'), body="
  R_xlen_t len, i, len_u = 1;
  SEXP res;
  int *xi = INTEGER(x);
  len = XLENGTH(x);

  if(len > 1) {
    // count uniques
    for(i = 1; i < len; ++i) {
      if(xi[i - 1] != xi[i]) {
        ++len_u;
    } }
    // allocate and record uniques
    res = PROTECT(allocVector(INTSXP, len_u));
    int *resi = INTEGER(res);
    *resi = 1;
    R_xlen_t j = 1;
    for(i = 1; i < len; ++i) {
      if(xi[i - 1] != xi[i]) {
        resi[j++] = i + 1;  // 1 based indexing
    } }
    UNPROTECT(1);
  } else {
    res = x;
  }
  return res;
")

.rowsums <- inline::cfunction(
  sig=c(x='numeric', g='integer'),
  body="
  R_xlen_t len, i, len_u = 1;
  SEXP res, res_x, res_g;
  int *gi = INTEGER(g);
  double *xi = REAL(x);
  len = XLENGTH(g);
  if(len != XLENGTH(x)) error(\"Unequal Length Vectors\");
  res = PROTECT(allocVector(VECSXP, 2));

  if(len > 1) {
    // count uniques
    for(i = 1; i < len; ++i) {
      if(gi[i - 1] != gi[i]) {
        ++len_u;
    } }
    // allocate and record uniques
    res_x = PROTECT(allocVector(REALSXP, len_u));
    res_g = PROTECT(allocVector(INTSXP, len_u));

    double *res_xi = REAL(res_x);
    int *res_gi = INTEGER(res_g);
    R_xlen_t j = 0;

    res_xi[0] = 0;
    for(i = 1; i < len; ++i) {
      res_xi[j] += xi[i - 1]; // we don't check for double overflow...
      if(gi[i - 1] != gi[i]) {
        res_gi[j] = gi[i - 1];
        ++j;
        res_xi[j] = 0;
    } }
    res_xi[j] += xi[i - 1];
    res_gi[j] = gi[i - 1];

    SET_VECTOR_ELT(res, 0, res_x);
    SET_VECTOR_ELT(res, 1, res_g);
    UNPROTECT(2);
  } else {
    // Don't seem to need to duplicate x/g
    SET_VECTOR_ELT(res, 0, x);
    SET_VECTOR_ELT(res, 1, g);
  }
  UNPROTECT(1);
  return res;
")
```

[^knowledge-caveat]: Given how long it's taken me to find out about `rowsum` it
is fair to question whether I would know whether there are many other functions
of this kind out there or not.
[^vec-size]: Numeric vectors require 8 bytes per element plus some overhead for
the object meta data.
[^love-r-but]: I love R, but the madness around [text decoration
conventions][1] is something that I could do without.  Sorry for the rant, but I
was particularly triggered by this example.

[1]: https://twitter.com/BrodieGaslam/status/976616435836510210
[2]: https://en.wikipedia.org/wiki/Hash_table
[3]: https://stackoverflow.com/a/11227902/2725969
[4]: https://www.extremetech.com/extreme/188776-how-l1-and-l2-cpu-caches-work-and-why-theyre-an-essential-part-of-modern-chips
[5]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/unique.c#L1514
[6]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/library/base/R/rowsum.R#L26
[7]: https://cran.r-project.org/web/packages/inline/index.html
