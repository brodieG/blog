---
title: Faster Group Stats in Base R
author: ~
date: '2019-03-03'
slug: faster-group-stats-in-base-r
categories: []
tags: []
image: /front-img/default.png
imagerect: ~
imagemrgvt: 0%
imagemrghz: 0%
draft: true
weight: 1
contenttype: article
description: Front page summary
---

```{r echo=FALSE}
options(digits=3)
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
```

# blah blah

<!-- this needs to become a shortcode -->
<img
  id='front-img' src='/front-img/default.png'
  class='post-inset-image'
/>

# Baseline

In one corner, the venerable `tapply`:

```{r eval=FALSE}
RNGversion("3.5.2"); set.seed(42)
n     <- 1e7
n.grp <- 1e6
grp   <- sample(n.grp, n, replace=TRUE)
x     <- runif(n)
y     <- runif(n)

system.time(x.ref <- c(tapply(x, grp, sum)))
```
```
   user  system elapsed
  8.539   0.321   9.030
```

In the other, `data.table`.

```{r}
library(data.table)
DT <- data.table(grp, x)
setDTthreads(1)
system.time(x.dt <- DT[, sum(x), keyby=grp][[2]])
```
```
   user  system elapsed
  0.982   0.059   1.048
```

We use one thread for more stable results, but also because for the final
computation we are planning on doing multi-threaded did not make a huge
difference on my system.  For this particular task multi threading can provide a
substantial boost to data table performance.

# Simple But Limited

Our first solution uses `base::rowsum`.  As is clearly indicated by the lack of
capitalization and pluralization, `base::rowsum` is completely different from
`base::rowSums`.  Further, anyone who has read the "Semantics of Capitalization,
Punctuation, and Pluralization in R Function Names" memo can infer that the
former sums rows within groups, and the latter sums columns within rows.

You did get the memo, right?  Good.

Even though the function names are self explanatory, we will illustrate for
completeness:

```{r}
mx <- matrix(1:8, 4, byrow=TRUE) * 10 ^ (0:3)
mx
```
```{r}
rowsum(mx, group=rep(c('odd', 'even'), 2))
```

`rowsum` preserved the two columns, but collapsed the rows by the `group` value.

```{r}
rowSums(mx)
```

`rowSums` collapsed the columns but preserved the rows.  You can think of the
resulting vector as a 4 x 1 "column".

I have known about `rowSums` for a long time, and I only more recently
discovered `rowsum`.  Imagine the excitement a taxidermist might feel on
realizing they were just given the corpse of an albino fox, and not that of a
white cat as they initially thought.  That should capture the magnitude _and_
significance of my excitement when I realized what `rowsum` does.

How can a base R function possibly compete for excitement with a dead fox?
Well, there are many base R functions that compute on arbitrary groups, and many
base R functions that work directly in compiled code, but as far as I know base
R functions that compute on arbitrary groups in compiled code are
rare[^knowledge-caveat].  This is very useful:

```{r eval=FALSE}
system.time(x.rs <- rowsum(x, grp))
```
```
   user  system elapsed
  2.505   0.096   2.626
```
```{r eval=FALSE}
all.equal(c(x.ref), c(x.rs), check.attributes=FALSE)
```
```
[1] TRUE
```

Almost four times faster than the original `tapply` solution, and not far from
the `data.table` one.

```{r echo=FALSE}
times <- data.frame(
  Function=c('tapply', 'rowsum', 'data.table'),
  time=c(9.030, 2.626, 1.126)
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col() +
  # facet_wrap(~Function) +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

I've wondered if I am alone in my delayed awareness of `rowsum`, but my attempts
to measure the relative popularity of the two functions via search engine were
fruitless.  Given the glaring semantic cues in `rowsum` vs `rowSums` I would
have thought they would do better...  Thankfully years of using "R" as a search
term have inured me to this type of search disappointment.

# Pixie Dust

Sorting values prior to running `unique` on them makes `unique` faster:

```{r eval=FALSE}
grp.o <- grp[order(grp)]
system.time(unique(grp.o))
```
   user  system elapsed
  0.453   0.057   0.514
```
```
```{r eval=FALSE}
system.time(unique(grp))
```
```
   user  system elapsed
  1.354   0.067   1.436
```

Internally `unique` uses a [hash table][2] to detect duplicate values, so it may
seem odd that the lookup speed is affected by ordering as the hashing time
should not be affected by the input order.  Most likely it is some combination
of caching and [branch prediction][3].

With respect to caching, In the sorted version, we will look up the same key
repeatedly, which presumably means the corresponding value will be warm and snug
in the [highest speed cache][4].  In the unsorted version, we will be jumping
all over the hash table, which will almost certainly require looking up values
in at best slower cache, or more likely in main memory.  Similarly, in branch
prediction, the processor will predict well just by assuming that a value
will be a duplicate when the duplicates are sequential, as they are in the
sorted case.  I have no idea if this is actually what is going on or not, but it
seems likely, and I'm tickled pink by the thought that I'm noticing these
effects from R-level for the first time.

Now, for why I'm ranting about this, it turns out that the combined operation of
sorting and `unique`ing is faster than the just `unique`ing:

```{r eval=FALSE}
system.time(unique(grp[order(grp)]))
```
```
   user  system elapsed
  0.909   0.158   1.081
```

This also has a big impact on `rowsum`, as it uses both `unique` on top of its
own [hash table][5]:

```{r eval=FALSE}
system.time({
  o <- order(grp)
  x.rs2 <- rowsum(x[o], grp[o], reorder=FALSE)
})
```
```
   user  system elapsed 
  1.283   0.105   1.430 
```

We are now within striking distance of single-threaded `data.table`:

```{r echo=FALSE}
times <- data.frame(
  Function=c('tapply', 'rowsum', 'rowsum-sorted', 'data.table'),
  time=c(9.030, 2.626, 1.430, 1.048)
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col() +
  # facet_wrap(~Function) +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

Now, you might be thinking, "oh, so maybe `data.table` isn't so special after
all".  Well, the only reason this `order` pixie dust business is even possible
is because `order` is so fast.  And `order` is so fast because `data.table`
contributed their pixie dust to R:

```{r echo=FALSE}
blogdown::shortcode('tweet', '1106231241488154626')
```


, where for the former we get a boost by
looking up the same hash key repeatedly, as it is most likely to be in hj

> system.time(unique(grp))

```{r eval=FALSE}

system.time({
  o <- order(grp)
  u1 <- unique(grp[o])
})
system.time({
  go <- sort(grp)
  u2 <- go[c(which(go[-length(go)] != go[-1L]), length(go))]
})
system.time(unique(grp))
```





```{r eval=FALSE}
system.time({
  o <- order(grp)
  x.grp.2 <- rowsum(x[o], grp[o])
})
```
```
   user  system elapsed
  1.408   0.113   1.556
```
Slope:
```{r}
system.time({
o <- order(grp)
go <- grp[o]
xo <- x[o]
xs <- rowsum(xo, go)
xn <- rowsum(rep(1L, length(go)), go)
xi <- rep(seq_along(xs), xn)
xu <- (xs/xn)[xi]

yo <- y[o]
ys <- rowsum(yo, go)
yu <- (ys/xn)[xi]
x_ux <- xo - xu
y_uy <- yo - yu

rowsum(x_ux * y_uy, go) / rowsum(x_ux ^ 2, go)
})
```
```
   user  system elapsed
  4.655   0.772   5.516
```


```{r eval=FALSE}
# Note: adapted to handle na.rm as per winvector, don't necessarily
# handle corner cases correctly (0, 1 length vectors, others?)

# Note: this stuff is only fast because order(, method="radix") is fast
# Doesn't handle Infinite values

sum_g2 <- function(x, grp, na.rm=FALSE) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.rle.c <- cumsum(grp.rle[['lengths']])
  x.ord <- x[ord]
  has.na <- anyNA(x)

  if(has.na) {
    na.x <- is.na(x)
    x.ord[na.x] <- 0
  } else na.x <- logical()

  x.grp.c <- cumsum(x.ord)[grp.rle.c]
  x.grp.c[-1L] <- x.grp.c[-1L] - x.grp.c[-length(x.grp.c)]

  if(!na.rm && has.na)
    x.grp.c[match(grp.ord[na.x], grp.rle[['values']])] <- NA

  structure(x.grp.c, groups=grp.rle[['values']], n=grp.rle[['lengths']])
}
system.time(sum_g2(x, grp))
```
```
   user  system elapsed
  1.186   0.374   1.634
```
Try slope:
```{r eval=FALSE}

sum_grp1_int <- function(x, last.in.group) {
  x.g <- cumsum(x)[last.in.group]
  x.g[-1L] <- x.g[-1L] - x.g[-length(x.g)]
  x.g
}
sum_grp1 <- function(x, last.in.group, precise=FALSE){
  x.grp <- sum_grp1_int(x, last.in.group)
  if(precise) {
    x[last.in.group] <- x[last.in.group] - x.grp
    x.grp + sum_grp1_int(x, last.in.group)
  } else x.grp
}
slope1 <- function(x, y, grp, precise=FALSE) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.rle.c <- cumsum(grp.rle[['lengths']])
  x.ord <- x[ord]
  y.ord <- y[ord]

  x.grp.c <- sum_grp1(x.ord, grp.rle.c, precise=precise)
  y.grp.c <- sum_grp1(y.ord, grp.rle.c, precise=precise)

  ux <- x.grp.c / grp.rle[['lengths']]
  uy <- y.grp.c / grp.rle[['lengths']]

  xi <- rep(seq_along(ux), grp.rle[['lengths']])
  x_ux <- x.ord - ux[xi]
  y_uy <- y.ord - uy[xi]

  x_ux.y_uy <- sum_grp1(x_ux * y_uy, grp.rle.c, precise=precise)
  x_ux2 <- sum_grp1(x_ux ^ 2, grp.rle.c, precise=precise)
  x_ux.y_uy / x_ux2
}
system.time(slope1(x, y, grp, precise=FALSE))
```
```
   user  system elapsed
  2.049   0.805   2.871
```
Precision is garbage though:
```
> all.equal(res.slope.dt1[[2]], res4, tol=1e-2)
[1] TRUE
> all.equal(res.slope.dt1[[2]], res4, tol=1e-3)
[1] FALSE
```
But we can make it precise
```{r eval=FALSE}
system.time(slope1(x, y, grp, precise=TRUE))
```
```
   user  system elapsed
  2.481   1.087   3.754
  # second faster timing was from a fresh session, repeated several times
   user  system elapsed
  2.399   0.933   3.348
```

```{r eval=FALSE}
sum_g3 <- function(x, grp, na.rm=TRUE) {
  ord <- order(grp)
  id.ord <- id[ord]
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  max.grp <- max(grp.rle[['lengths']])

  # this NA handling doesn't work b/c for na.rm=FALSE you still get NAs

  res <- matrix(NA_real_, ncol=length(grp.rle[['lengths']]), nrow=max.grp)

  # each group that isn't as long as the longest group needs padding

  rle.len <- grp.rle[['lengths']]
  grp.pad <- max.grp - rle.len
  id.raw <- rep(1L, length(x))
  id.raw[(cumsum(rle.len) + 1L)[-length(rle.len)]] <-
    grp.pad[-length(rle.len)] + 1L
  id <- cumsum(id.raw)

  res[id] <- x[ord]
  structure(colSums(res, na.rm=na.rm), groups=grp.rle[['values']])
}
system.time(sum_g3(x, grp))
```
```
   user  system elapsed
  1.186   0.374   1.634
```
```{r eval=FALSE}
# lens: how long each group is
# maxlen: longest group
sum_grp2 <- function(x, lens, maxlen, mode='sum') {

  res <- matrix(NA_real_, ncol=length(lens), nrow=maxlen)

  # Generate indices that will map to the correct spots in `res` from `x`,
  # which means add whatever padding we need to the index value for the next
  # column

  len_1 <- lens[-length(lens)]
  grp.pad <- (maxlen + 1L) - len_1
  id.raw <- rep(1L, length(x))
  len_1[1L] <- len_1[1L] + 1L
  id.raw[cumsum(len_1)] <- grp.pad
  id <- cumsum(id.raw)

  # Inject the x values according to these indices that should place each gropu
  # in a column

  res[id] <- x

  if(identical(mode, 'sum')) colSums(res, na.rm=TRUE)
  else if(identical(mode, 'mean')) colMeans(res, na.rm=TRUE)
  else stop("Invalid mode")
}
slope2 <- function(x, y, grp) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.len <- grp.rle[['lengths']]
  max.grp <- max(grp.len)

  xo <- x[ord]
  xi <- rep(seq_along(grp.len), grp.len)
  xu <- sum_grp2(xo, grp.len, max.grp, 'mean')
  x_ux <- xo - xu[xi]

  yo <- y[ord]
  yu <- sum_grp2(yo, grp.len, max.grp, 'mean')
  y_uy <- yo - yu[xi]

  x_ux.y_uy <- sum_grp2(x_ux * y_uy, grp.len, max.grp, 'sum')
  x_ux2 <- sum_grp2(x_ux ^ 2, grp.len, max.grp, 'sum')

  x_ux.y_uy / x_ux2
}
slope2a_int <- function(xo, yo, lens) {
  max.grp <- lens[1]

  xi <- rep(seq_along(lens), lens)
  xu <- sum_grp2(xo, lens, max.grp, 'mean')
  x_ux <- xo - xu[xi]

  yu <- sum_grp2(yo, lens, max.grp, 'mean')
  y_uy <- yo - yu[xi]

  x_ux.y_uy <- sum_grp2(x_ux * y_uy, lens, max.grp, 'sum')
  x_ux2 <- sum_grp2(x_ux ^ 2, lens, max.grp, 'sum')

  x_ux.y_uy / x_ux2
}
slope2a <- function(x, y, grp, splits=5) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.len <- grp.rle[['lengths']]

  ord2 <- order(rep(grp.len, grp.len), decreasing=TRUE)
  ordg <- order(grp.len, decreasing=TRUE)
  grp.len.o <- grp.len[ordg]
  len.max <- grp.len.o[1L]  # will break if no groups
  len.min <- grp.len.o[length(grp.len.o)]

  # order the inputs

  ord3 <- ord[ord2]
  xo <- x[ord3]
  yo <- y[ord3]
  go <- grp.ord[ord2]

  # simple initial cut, just cut into equal splits

  cuts <- as.integer(
    round(seq(1L, length(grp.len) + 1L, length.out=splits + 1L))
  )
  grp.len.o.c <- cumsum(c(1L, grp.len.o))
  res <- vector("list", splits)

  for(i in seq_len(splits)){
    # Figure out that starting and ending elements for each group

    start.g <- cuts[i]
    end.g <- cuts[i + 1L]

    start <- grp.len.o.c[start.g]
    end <- grp.len.o.c[end.g] - 1L

    idx.g <- start.g:(end.g - 1L)
    idx <- start:end

    res[[i]] <- slope2a_int(xo[idx], yo[idx], grp.len.o[idx.g])
  }
  # Reorder back in ascending group order instead of group size order

  res.fin <- numeric(length(grp.len))
  res.fin[ordg] <- unlist(res)
  res.fin
}
system.time(slope2(x, y, grp))
RNGversion("3.5.2"); set.seed(42)
x2 <- runif(100)
y2 <- runif(100)
g2 <- sample(1:10, 100, rep=T)
```
```
   user  system elapsed
  2.827   0.932   3.807
  # can't reproduce the earlier timings...
   user  system elapsed
  3.185   1.285   4.783
  # now I can ...
```
```{r eval=FALSE}
system.time(slope2a(x, y, grp))
```
```
   user  system elapsed
  2.998   1.055   4.082
```
```{r eval=FALSE}
dummy <- function(sizes) {
  res <- vector("list", length(sizes))
  for(i in seq_along(sizes)) {
    mx <- matrix(numeric(), nrow=sizes[i], ncol=as.integer(1e6 / length(sizes)))
    res[[i]] <- colSums(mx, na.rm=TRUE)
  }
}
system.time(dummy(c(28, 21, 14, 7, 1)))
```

```{r eval=FALSE}

DT <- copy(DT.raw)
system.time(res.ref <- DT[, sum(x), keyby=grp][['V1']])
#   user  system elapsed
#  1.071   0.134   1.216
system.time(res <- sum_g2(x, grp))
#   user  system elapsed
#  1.286   0.309   1.692
all.equal(res, res.ref, check.attributes=FALSE) # TRUE

system.time(res2 <- sum_g2(x, grp))
system.time(res3 <- sum_g3(x, grp))
system.time(res4 <- rowsum(x, grp))

sum_winvector <- function(DF) {
  odata <- DF[order(DF$grp),,drop=FALSE]
  first_indices <- mark_first_in_each_group(odata, "grp")
  sum_g(odata[['x']], first_indices)
}
```
```
  user  system elapsed
 1.810   0.740   2.651
```
Note on vector size[^vec-size].


# Conclusions

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id='feedback-cont'></div>

# Appendix

[^knowledge-caveat]: Given how long it's taken me to find out about `rowsum` it
is fair to question whether I would know whether there are many other functions
of this kind out there or not.
[^vec-size]: Numeric vectors require 8 bytes per element plus some overhead for
the object meta data.
[^love-r-but]: I love R, but the maddness around [text decoration
conventions][1] is something that I could do without.  Sorry for the rant, but I
was particularly triggered by this example.

[1]: https://twitter.com/BrodieGaslam/status/976616435836510210
[2]: https://en.wikipedia.org/wiki/Hash_table
[3]: https://stackoverflow.com/a/11227902/2725969
[4]: https://www.extremetech.com/extreme/188776-how-l1-and-l2-cpu-caches-work-and-why-theyre-an-essential-part-of-modern-chips
[5]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/unique.c#L1514
