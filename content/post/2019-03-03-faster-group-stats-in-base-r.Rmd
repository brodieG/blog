---
title: Faster Group Stats in Base R
author: ~
date: '2019-03-03'
slug: faster-group-stats-in-base-r
categories: [r]
tags: [optim, rdatatable]
image: /front-img/rock-em-sock-em.png
imagerect: /front-img/rock-em-sock-em-wide.png
imagemrgvt: 0%
imagemrghz: 0%
draft: true
weight: 1
contenttype: article
descriptionlong: "In which scrappy base R takes on the reigning group stats champ
  data.table, with suprising results.  Cache effects, branch prediction, pixie
  dust, IEEE 754, and more!"
---

```{r echo=FALSE}
options(digits=3, crayon.enabled=TRUE)
library(ggplot2)
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
```
```{r echo=FALSE, comment="", results='asis'}
old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)
```
```{r, echo=FALSE}
writeFansi <- function(x) {
  writeLines(
    paste0(
      "<pre></pre><pre><code>",
      paste0(fansi::sgr_to_html(x), collapse="\n"),
      "</code></pre>"
  ) )
}
```

# Outtakes

## Outtakes in Blog Post?

When I first started to write the blog post I was exploring three different
methods for computing group sums.  As is usually the case with R, there are many
ways to do one things.  I include the slower two methods for completeness.

## It's All In the Capitalization

We take our first swing with `base::rowsum`.  That's right, `rowsum`, not
`rowSums`.  Lower case singular makes all the difference in the world.  This is
R, after all, and as any self respecting R user that has read the
seminal paper "Semantics of Capitalization, Punctuation, and Pluralization in R
Function Names" can tell you, the former sums rows within groups, and the latter
sums columns within rows.  You did read the memo, right?  Good.

Even though the function names are self explanatory, let's illustrate for
completeness:

```{r}
mx <- matrix(1:8, 4, byrow=TRUE) * 10 ^ (0:3)
mx
rowsum(mx, group=rep(c('odd', 'even'), 2))
```

`rowsum` preserved the two columns, but collapsed the rows by the `group` value.

```{r eval=FALSE}
rowSums(mx)       # note: output has been edited for clarity
```
```{r echo=FALSE}
matrix(rowSums(mx))
```

`rowSums` collapsed the columns but preserved the rows.  Normally `rowSums`
returns a vector, but here we display it as a one column matrix so the
relationship to the input matrix is clear.




All joking aside, I have known about `rowSums` for a long time, and have seen
the name `rowsum` in passing, but it never occurred to me that the latter might
do something as extraordinary as what it actually does: compute on arbitrary
groups in compiled code.


but very few do both.


`rowsum`
there
I only more recently
discovered `rowsum`.  It probably says more about me than I should admit but I
was downright giddy when I realized _exactly_ what `rowsum` does.  After all,
there are many base R functions that compute on arbitrary groups, and many base
R functions that work directly in compiled code, but as far as I know base R
functions that compute on arbitrary groups in compiled code are
rare[^knowledge-caveat].  This is very useful:

<!--
I have known about rowSums for a long time, and I only more recently discovered rowsum. Imagine the excitement a taxidermist might feel on realizing they were just given the corpse of an albino fox, and not that of a white cat as they initially thought. That should capture the magnitude and significance of my excitement when I realized what rowsum does.

How can a base R function possibly compete for excitement with a dead fox? Well, there are many base R functions that compute on arbitrary groups, and many base R functions that work directly in compiled code, but as far as I know base R functions that compute on arbitrary groups in compiled code are rare1. This is very useful:
-->
```{r eval=FALSE}
sys.time(x.rs <- rowsum(x, grp))
```
```
   user  system elapsed
  2.251   0.058   2.310
```
```{r eval=FALSE}
all.equal(c(x.ref), c(x.rs), check.attributes=FALSE)
```
```
[1] TRUE
```

Over three times faster than the original `tapply` solution, and at least
plausibly in the same conversation as the `data.table` one.

```{r echo=FALSE}
funs <- c('tapply', 'rowsum', 'data.table')
times <- data.frame(
  Function=factor(funs, levels=funs),
  time=c(7.308, 2.310, 0.973)
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col() +
  # facet_wrap(~Function) +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

I've wondered if I am alone in my delayed awareness of `rowsum`, but my attempts
to measure the relative popularity of the two functions via search engine were
fruitless.  Given the glaring semantic cues in `rowsum` vs `rowSums` I would
have thought they would do better...  Thankfully years of using "R" as a search
term have inured me to this type of search disappointment.

## colSums

```{r eval=FALSE}
sum_g3 <- function(x, grp, na.rm=TRUE) {
  ord <- order(grp)
  id.ord <- id[ord]
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  max.grp <- max(grp.rle[['lengths']])

  # this NA handling doesn't work b/c for na.rm=FALSE you still get NAs

  res <- matrix(NA_real_, ncol=length(grp.rle[['lengths']]), nrow=max.grp)

  # each group that isn't as long as the longest group needs padding

  rle.len <- grp.rle[['lengths']]
  grp.pad <- max.grp - rle.len
  id.raw <- rep(1L, length(x))
  id.raw[(cumsum(rle.len) + 1L)[-length(rle.len)]] <-
    grp.pad[-length(rle.len)] + 1L
  id <- cumsum(id.raw)

  res[id] <- x[ord]
  structure(colSums(res, na.rm=na.rm), groups=grp.rle[['values']])
}
system.time(sum_g3(x, grp))
```
```
   user  system elapsed
  1.186   0.374   1.634
```
```{r eval=FALSE}
  # lens: how long each group is
  # maxlen: longest group
sum_grp2 <- function(x, lens, maxlen, mode='sum') {

  res <- matrix(NA_real_, ncol=length(lens), nrow=maxlen)

  # Generate indices that will map to the correct spots in `res` from `x`,
  # which means add whatever padding we need to the index value for the next
  # column

  len_1 <- lens[-length(lens)]
  grp.pad <- (maxlen + 1L) - len_1
  id.raw <- rep(1L, length(x))
  len_1[1L] <- len_1[1L] + 1L
  id.raw[cumsum(len_1)] <- grp.pad
  id <- cumsum(id.raw)

  # Inject the x values according to these indices that should place each gropu
  # in a column

  res[id] <- x

  if(identical(mode, 'sum')) colSums(res, na.rm=TRUE)
  else if(identical(mode, 'mean')) colMeans(res, na.rm=TRUE)
  else stop("Invalid mode")
}
```
```
slope2 <- function(x, y, grp) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.len <- grp.rle[['lengths']]
  max.grp <- max(grp.len)

  xo <- x[ord]
  xi <- rep(seq_along(grp.len), grp.len)
  xu <- sum_grp2(xo, grp.len, max.grp, 'mean')
  x_ux <- xo - xu[xi]

  yo <- y[ord]
  yu <- sum_grp2(yo, grp.len, max.grp, 'mean')
  y_uy <- yo - yu[xi]

  x_ux.y_uy <- sum_grp2(x_ux * y_uy, grp.len, max.grp, 'sum')
  x_ux2 <- sum_grp2(x_ux ^ 2, grp.len, max.grp, 'sum')

  x_ux.y_uy / x_ux2
}
slope2a_int <- function(xo, yo, lens) {
  max.grp <- lens[1]

  xi <- rep(seq_along(lens), lens)
  xu <- sum_grp2(xo, lens, max.grp, 'mean')
  x_ux <- xo - xu[xi]

  yu <- sum_grp2(yo, lens, max.grp, 'mean')
  y_uy <- yo - yu[xi]

  x_ux.y_uy <- sum_grp2(x_ux * y_uy, lens, max.grp, 'sum')
  x_ux2 <- sum_grp2(x_ux ^ 2, lens, max.grp, 'sum')

  x_ux.y_uy / x_ux2
}
slope2a <- function(x, y, grp, splits=5) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.len <- grp.rle[['lengths']]

  ord2 <- order(rep(grp.len, grp.len), decreasing=TRUE)
  ordg <- order(grp.len, decreasing=TRUE)
  grp.len.o <- grp.len[ordg]
  len.max <- grp.len.o[1L]  # will break if no groups
  len.min <- grp.len.o[length(grp.len.o)]

  # order the inputs

  ord3 <- ord[ord2]
  xo <- x[ord3]
  yo <- y[ord3]
  go <- grp.ord[ord2]

  # simple initial cut, just cut into equal splits

  cuts <- as.integer(
    round(seq(1L, length(grp.len) + 1L, length.out=splits + 1L))
  )
  grp.len.o.c <- cumsum(c(1L, grp.len.o))
  res <- vector("list", splits)

  for(i in seq_len(splits)){
    # Figure out that starting and ending elements for each group

    start.g <- cuts[i]
    end.g <- cuts[i + 1L]

    start <- grp.len.o.c[start.g]
    end <- grp.len.o.c[end.g] - 1L

    idx.g <- start.g:(end.g - 1L)
    idx <- start:end

    res[[i]] <- slope2a_int(xo[idx], yo[idx], grp.len.o[idx.g])
  }
  # Reorder back in ascending group order instead of group size order

  res.fin <- numeric(length(grp.len))
  res.fin[ordg] <- unlist(res)
  res.fin
}
system.time(slope2(x, y, grp))
RNGversion("3.5.2"); set.seed(42)
x2 <- runif(100)
y2 <- runif(100)
g2 <- sample(1:10, 100, rep=T)
```
```
   user  system elapsed
  2.827   0.932   3.807
  # can't reproduce the earlier timings...
   user  system elapsed
  3.185   1.285   4.783
  # now I can ...
```
```{r eval=FALSE}
system.time(slope2a(x, y, grp))
```
```
   user  system elapsed
  2.998   1.055   4.082
```
# Appendix

## Acknowledgments

* Matt Dowle and the `data.table` team for contributing their `radix` order to
  R.
* Romain Francois for `seven31` which allowed us to debug double precision
  issues.
* Oleg Sklyar, Duncan Murdoch, Mike Smith, Dirk Eddelbuettel, Romain Francois,
  Karline Soetaert for [`inline`][7].
* Ggplot2
* Ggbeeswarm
* Reshape2

## References

* Ulrich Drepper's [What Every Programmer Should Know About Memory][27]
* [Stuffed Cow post on Ivy Bridge Cache Replacement policies][26]
* Cache timings?
* Nima Honarmand [Memory Prefetching][28], including discussion of stream
  buffers.

### Adjustable Precision Group Sums

```{r group_sum_int3, eval=FALSE}
```





[^knowledge-caveat]: Given how long it's taken me to find out about `rowsum` it
  is fair to question whether I would know whether there are many other
  functions of this kind out there or not.
[^vec-size]: Numeric vectors require 8 bytes per element plus some overhead for
  the object meta data.
[^love-r-but]: I love R, but the madness around [text decoration
  conventions][1], or lack thereof is something that I could do without.  Sorry
  for the rant, but I was particularly triggered by this example.  I also do
  understand that in large collaborative project like this one, things like this
  are bound to happen and in the end it's not a big deal.
[^forgive-me]: I am prone to absurd pedantic turns of phrase when giddy, so
  please forgive me.  I did mention that I am `rowsum` made me giddy, right?
[prior post][8]
[^timing-note]: Timings in the [prior post][8] are slower due to the use of
  `mean` instead of `mean.default` in the [`slope` function](#slope-ex).
[^dt-optim]: In order to unlock the full performance of `data.table` we need to
  engage in some manipulations.  We covered these in the [prior group statistics
  post][9].


1. Read factor level     (int)
2. Read offset in group  (int)xx  Random seek in lvls long vector
3. Read input            (dbl)
4. Read list element     (ptr)xx
5. Read group element    (dbl)x   In most cases should be from after read above
6. Write group elment    (dbl)    Random seek in lvls long vec of vecs
7. Increment offset      (int).
8. Write offset          (int)    Should still be in cache

[1]: https://twitter.com/BrodieGaslam/status/976616435836510210
[3]: https://stackoverflow.com/a/11227902/2725969
[4]: https://www.extremetech.com/extreme/188776-how-l1-and-l2-cpu-caches-work-and-why-theyre-an-essential-part-of-modern-chips
[5]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/unique.c#L1514
[6]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/library/base/R/rowsum.R#L26
[8]: /2019/02/24/a-strategy-for-faster-group-statisitics/#blood-from-a-turnip/
[9]: /2019/02/24/a-strategy-for-faster-group-statisitics/#optim-start/
[12]: https://en.wikipedia.org/wiki/Double-precision_floating-point_format
[13]: https://github.com/wch/r-source/blob/R-3-5-branch/src/main/cum.c#L30
[14]: https://en.wikipedia.org/wiki/Extended_precision#Working_range
[15]: https://github.com/romainfrancois
[16]: https://github.com/ThinkR-open/seven31
[17]: https://github.com/wrathematics/float
[18]: https://github.com/wch/r-source/blob/R-3-3-branch/src/main/summary.c#L434
[19]: https://twitter.com/BrodieGaslam/status/1113783101262585856
[20]: https://www.ibm.com/blogs/research/2018/12/8-bit-precision-training/
[21]: /2019/02/24/a-strategy-for-faster-group-statisitics/
[24]: https://twitter.com/BrodieGaslam/status/1106231241488154626
<!-- 
Analysis of cache replacement policies with experiments.
-->

[26]: http://blog.stuffedcow.net/2013/01/ivb-cache-replacement/
<!-- 
Ulrich Drepper Paper, lots of goodies.

Table 2.2: DDR3 array/bus frequencies (933MHz for DDR3-1866)
Bottom pg 15: Latency numbers for Pentium M, including 240 cycles for main
memory.

Oddly doesn't really talk about interleaved access.
-->

[27]: https://people.freebsd.org/~lstewart/articles/cpumemory.pdf

[28]: https://compas.cs.stonybrook.edu/~nhonarmand/courses/sp16/cse502/slides/13-prefetch.pdf
[34]: https://iis-people.ee.ethz.ch/~gmichi/asocd/addinfo/Out-of-Order_execution.pdf


<!-- Paper that discusses memory page handling, some details on banks, and of
most interest is the idea that you can have a policy that closes pages right
away or leaves them open depending on expectation that prob of next read being
on last page read -->

[36]: https://arxiv.org/pdf/1509.03740.pdf

<!-- 
Very detailed overview of architecture of Core i7 and Xeon 5500 (Circa 2008?),
timings in cycles for cache accesses, and ns for memory 

Includes the 4, 10, 40 cycle access times, plus 100ns for main memory (p8,9,22).

Details about TLB (separate large page TLBs, 2 level TLB, see p9, 64 and 512
entries)
-->

[37]: https://software.intel.com/sites/products/collateral/hpc/vtune/performance_analysis_guide.pdf

<!-- Typical timings in mostly ns-->

[38]: http://norvig.com/21-days.html#answers


<!-- 
Wiki page on DDR3 

Timings for 1866 suggest 11-14 nanoseconds for CAS and Row activation, which
strongly suggests 100ns is way more reasonable than 250 cycles unless the 250
cycles is dominated by bus latency..
-->

[39]: https://en.wikipedia.org/wiki/DDR3_SDRAM#JEDEC_standard_modules

<!--
2014 blogpost with latencies, has main memory at 360 cycles
-->
[40]: http://sgros.blogspot.com/2014/08/memory-access-latencies.html

<!--
More timings, has main memory at 200 cycles, but L1 cache at 0.5ns!  Even at 
possible with 4GHz that's two cycles, seems wrong?  This does tie out with the
100ns number though.  It's possible that in my system the memory access latency
is ironically lower because the clock is so slow, i.e. it always takes about
100ns to get to memory.
-->
https://gist.github.com/jboner/2841832

<!--
CMU paper on DRAM refresh notes typically 8 banks per rank
-->

[43]: https://users.ece.cmu.edu/~omutlu/pub/dram-access-refresh-parallelization_hpca14.pdf

<!-- 
Archives New Zealand photo of coal train 
CC BY 2.0
-->

[44]: https://www.flickr.com/photos/archivesnz/35756097220/in/photolist-WtDkYQ-pFXbm3-EC9kXN-7kXxX1-8U6V9V-8ARY8U-8AS1m3-e2vora-54GqWx-2ejTYGK-24Jjyh2-6VJXqq-Hr5etz-am3WNE-niGFx9-c47ogL-pvnydm-8ANKYR-q8dYGa-GC23Cx-2cwaLQc-22GYq5b-7bdD4a-5J8sM1-a7C8ZL-oeGeph-4qHJ3t-iP2gsi-28A2Rjv-8WetRn-cpRqNW-d4paYu-ppvSpN-UAE5gq-YmgY8J-g1EDcJ-bCWKu5-9fyM4U-YQekTS-sdCeYu-BseS2w-pkMARp-ebgH77-2ekscff-ixN2mt-dXp7D2-a8h1SU-e1d6MP-Ttk3vp-2d4h1cC

<!-- 
Reorder Buffer Size testing

Of interest:
* Mentions the ~200 cycle main memory access time, but the tests themselves
  suggest potentially faster (~150) cycles.
* Shows reorder buffer size likely ~168 for Sandybridge, which is the generation
  prior to Skylake on my system.
-->
[45]: http://blog.stuffedcow.net/2013/05/measuring-rob-capacity/

<!--
Not a reference, but someone that at least seems to have some idea of what's
going on, but no idea if they are right or not.
-->

[46]: https://forums.tomshardware.com/threads/what-exactly-is-a-memory-bank.2919273/#post-18421032

<!--
RIDL and Fallout website, in particular has a great diagram of memory system
architecture, including ROB entry counts, line fill buffer mention, separate L1
and L2 TLBs.
-->

[47]: https://mdsattacks.com/

<!--
Bad concurrency with great but low rez annotated die image
-->

[48]: https://www.flickr.com/search/?text=cpu+die+cache

<!--
Westmere 6 core image
-->
[50]: http://i.imgur.com/AawdBre.jpg


