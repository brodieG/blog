---
title: Faster Group Stats in Base R
author: ~
date: '2019-03-03'
slug: faster-group-stats-in-base-r
categories: []
tags: []
image: /front-img/default.png
imagerect: ~
imagemrgvt: 0%
imagemrghz: 0%
draft: true
weight: 1
contenttype: article
description: Front page summary
---

```{r echo=FALSE}
options(digits=3)
library(ggplot2)
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
```
```{r, echo=FALSE}
writeFansi <- function(x) {
  writeLines(
    paste0(
      "<pre></pre><pre><code>",
      paste0(fansi::sgr_to_html(x), collapse="\n"),
      "</code></pre>"
  ) )
}
```

# blah blah

<!-- this needs to become a shortcode -->
<img
  id='front-img' src='/front-img/default.png'
  class='post-inset-image'
/>

# Baseline

As we saw in the prior
In one corner, the venerable `tapply`:

```{r}
RNGversion("3.5.2"); set.seed(42)
n     <- 1e7
n.grp <- 1e6
grp   <- sample(n.grp, n, replace=TRUE)
x     <- runif(n)
y     <- runif(n)   # we'll use this later
```
```{r eval=FALSE}
system.time(x.ref <- c(tapply(x, grp, sum)))
```
```
   user  system elapsed
  8.539   0.321   9.030
```

In the other, `data.table`.

```{r eval=FALSE}
library(data.table)
DT <- data.table(grp, x)
setDTthreads(1)
system.time(x.dt <- DT[, sum(x), keyby=grp][[2]])
```
```
   user  system elapsed
  0.982   0.059   1.048
```

We use one thread for more stable results, but also because for the final
computation we are planning on doing multi-threaded did not make a huge
difference on my system.  For this particular task multi threading can provide a
substantial boost to data table performance.

# It's All In the Capitalization

Our first solution uses `base::rowsum`.  As is clearly indicated by the lack of
capitalization and pluralization, `base::rowsum` is completely different from
`base::rowSums`.  Further, anyone who has read the "Semantics of Capitalization,
Punctuation, and Pluralization in R Function Names" memo can infer that the
former sums rows within groups, and the latter sums columns within rows.

You did get the memo, right?  Good.

Even though the function names are self explanatory, we will illustrate for
completeness:

```{r}
mx <- matrix(1:8, 4, byrow=TRUE) * 10 ^ (0:3)
mx
rowsum(mx, group=rep(c('odd', 'even'), 2))
```

`rowsum` preserved the two columns, but collapsed the rows by the `group` value.

```{r eval=FALSE}
rowSums(mx)       # note: output has been edited for clarity
```
```{r echo=FALSE}
matrix(rowSums(mx))
```

`rowSums` collapsed the columns but preserved the rows.  Normally `rowSums`
returns a vector, but here we display it as a one column matrix so the
relationship to the input matrix is clear.

I have known about `rowSums` for a long time, and I only more recently
discovered `rowsum`.  It probably says more about me than I should admit but I
was downright giddy when I realized _exactly_ what `rowsum` does.  After all,
there are many base R functions that compute on arbitrary groups, and many base
R functions that work directly in compiled code, but as far as I know base R
functions that compute on arbitrary groups in compiled code are
rare[^knowledge-caveat].  This is very useful:

<!--
I have known about rowSums for a long time, and I only more recently discovered rowsum. Imagine the excitement a taxidermist might feel on realizing they were just given the corpse of an albino fox, and not that of a white cat as they initially thought. That should capture the magnitude and significance of my excitement when I realized what rowsum does.

How can a base R function possibly compete for excitement with a dead fox? Well, there are many base R functions that compute on arbitrary groups, and many base R functions that work directly in compiled code, but as far as I know base R functions that compute on arbitrary groups in compiled code are rare1. This is very useful:
-->
```{r eval=FALSE}
system.time(x.rs <- rowsum(x, grp))
```
```
   user  system elapsed
  2.505   0.096   2.626
```
```{r eval=FALSE}
all.equal(c(x.ref), c(x.rs), check.attributes=FALSE)
```
```
[1] TRUE
```

Almost four times faster than the original `tapply` solution, and not far from
the `data.table` one.

```{r echo=FALSE}
funs <- c('tapply', 'rowsum', 'data.table')
times <- data.frame(
  Function=factor(funs, levels=funs),
  time=c(9.030, 2.626, 1.126)
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col() +
  # facet_wrap(~Function) +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

I've wondered if I am alone in my delayed awareness of `rowsum`, but my attempts
to measure the relative popularity of the two functions via search engine were
fruitless.  Given the glaring semantic cues in `rowsum` vs `rowSums` I would
have thought they would do better...  Thankfully years of using "R" as a search
term have inured me to this type of search disappointment.

# Pixie Dust

Sorting values prior to running `unique` on them makes `unique` faster:

```{r eval=FALSE}
grp.o <- grp[order(grp)]
system.time(unique(grp.o))
```
```
   user  system elapsed
  0.453   0.057   0.514
```
```{r eval=FALSE}
system.time(unique(grp))
```
```
   user  system elapsed
  1.354   0.067   1.436
```

Internally `unique` uses a [hash table][2] to detect duplicate values, so it may
seem odd that the lookup speed is affected by ordering as the hashing time
should not be affected by the input order.  Most likely it is some combination
of caching and [branch prediction][3].  In the sorted version, we will look up
the same key repeatedly, which means the corresponding value will be warm and
snug in the [highest speed cache][4].  Additionally, since all duplicated values
are in sequence, the processor will do a pretty good job predicting branches
just by assuming the next value will be duplicated.  It will get it wrong at
each transition to a new value, but it will be right more often than not if
there is sufficient duplication.  I have no idea if this is actually what is
going on or not, but it seems likely, and I'm tickled pink by the thought that
I'm noticing these effects from R-level for the first time.

Now, for why I'm ranting about this, it turns out that the combined operation of
sorting and `unique`ing is faster than the just `unique`ing:

```{r eval=FALSE}
system.time(unique(grp[order(grp)]))
```
```
   user  system elapsed
  0.909   0.158   1.081
```

This also has a big impact on `rowsum`, as it uses both [`unique`][6] on top of
its own [hash table][5]:<span id=rowsum-ordered></span>

```{r eval=FALSE}
system.time({
  o <- order(grp)
  x.rs2 <- rowsum(x[o], grp[o], reorder=FALSE)
})
```
```
   user  system elapsed
  1.283   0.105   1.430
```

This puts us in striking distance of single-threaded `data.table`:

```{r echo=FALSE}
times <- data.frame(
  Function=c('tapply', 'rowsum', 'rowsum-sorted', 'data.table'),
  time=c(9.030, 2.626, 1.430, 1.048)
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col() +
  # facet_wrap(~Function) +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

Now, you might be thinking, "oh, so maybe `data.table` isn't so special after
all".  Well, the only reason this `order` pixie dust business is even possible
is because `order` is so fast, and `order` is so fast because `data.table`
contributed their pixie dust to R:

<div class=>
```{r echo=FALSE}
blogdown::shortcode('tweet', '1106231241488154626')
```
</div>

<span id=alt-algo></span>
One last thing before we move on: knowing that the data is sorted opens up
different algorithms for many tasks such as `unique` and `rowsum`.  To
illustrate I implemented the alternate [algorithms in C in the
appendix](#sorting-versions).  We wrap those in R functions that order
the data first, and to distinguish them from their base counterparts we
pluralize them.  The trailing "s" indicate these are the sorting versions of the
functions, or maybe that they are the speedy versions.  My dog ate the memo so I
don't know anymore[^love-r-but]...

Here is what happens with `uniques`:<span id=unique-c></span>

```{r eval=FALSE}
uniques <- function(x, res.sorted=FALSE) {
  o <- order(x)
  xo <- x[o]
  i <- .uniques(xo)          # C code: see appendix
  if(!res.sorted) xo[i][order(o[i])] else xo[i]
}
system.time(uniques(grp))
```
```
   user  system elapsed
  0.587   0.018   0.615
```

This is almost twice as fast as the original version with the sorted data, and
it could be made faster if we were okay with getting results back sorted.  Most
of the time is actually spent in the `o <- order(x)` step, so the actually
unique-ing is an order of magnitude faster!

Similar story for `rowsums`:

```{r eval=FALSE}
rowsums <- function(x, group) {
  o <- order(group)
  .rowsums(x[o], group[o])    # C code: see appendix
}
system.time(rowsums(x, grp))
```
```
   user  system elapsed
  0.820   0.022   0.846
```

The implementations are not equivalent, but for these specific inputs they
should be comparable.  Your mileage will vary depending on the degree of
uniqueness of the data.  More details [in the appendix](#sorting-versions).

The main point I'm trying to make here is that it is a **big deal** that `order`
can order fast enough that we can switch the algorithms we use downstream and
get an even bigger performance improvement.  A big thank you to team
`data.table` for sharing the pixie dust.

# So You Think You Can Group-Stat?

Okay, great, we can sum quickly in base R.  One measly stat.  What good is that
if we want to compute something more complex like the slope of a bivariate
regression, as we did in our [prior post][8]?  As a refresher this is what the
calculation looks like:

$$\frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i -
\bar{x})^{2}}$$

The R equivalent is[^slope-mod]:<span id='slope-ex'></a>

```{r eval=FALSE}
slope <- function(x, y) {
  x_ux <- x - mean.default(x)
  y_uy <- y - mean.default(y)
  sum(x_ux * y_uy) / sum(x_ux ^ 2)
}
```

We can see that `sum` shows up explicitly, and somewhat implicitly via
`mean`[^not-quite-sum].  There are many statistics that essentially boil down to
adding things together, so we can use `rowsum` as the wedge to breach the
heretofore impenetrable barrier to fast grouped statistics in R[^forgive-me]:
<span id=group-slope-rs></span>

```{r eval=FALSE}
group_slope <- function(x, y, grp) {
  ## order inputs by group
  o <- order(grp)
  go <- grp[o]
  xo <- x[o]
  yo <- y[o]

  ## group sizes and group indices
  grle <- rle(go)
  gn <- grle[['lengths']]
  gi <- rep(seq_along(gn), gn)

  ## compute mean(x) and mean(y), and recycle them
  ## to each element of `x` and `y`:
  sx <- rowsum(xo, go)     # sum(x)
  ux <- (sx/gn)[gi]        # mean(x), recycled to original vec length
  sy <- rowsum(yo, go)
  uy <- (sy/gn)[gi]

  ## (x - mean(x)) and (y - mean(y))
  x_ux <- xo - ux
  y_uy <- yo - uy

  ## Slopes!
  setNames(rowsum(x_ux * y_uy, go) / rowsum(x_ux ^ 2, go), grle[['values']])
}
```

The non-obvious steps involve `gn` and `gi`:

```{r eval=FALSE}
grle <- rle(go)
gn <- grle[['lengths']]
gi <- rep(seq_along(gn), gn)
```

`base::rle` computes "Run Lengths" a.k.a. the length of sequences of repeated
values.  With our ordered groups this corresponds to the size of the groups.
We can illustrate in simplified form:

```{r}
(xo <- 2:6)                              # some values
(go <- c(3, 3, 5, 5, 5))                 # their groups
(gn <- rle(go)[['lengths']])             # the size of the groups
```

`gn` tells us how many elements there are in each group.  This allows us to
compute the `$\bar{x}$` values:

```{r}
(sx <- rowsum(xo, go))                   # sum of each group
(ux <- sx / gn)                          # mean of each group
```

But we need to compute `$x - \bar{x}$`, which means we need to recycle each
group's `$\bar{x}$` value for each `$x$`.  This is what `gi` does:

```{r}
(gi <- rep(seq_along(gn), gn))
cbind(x=xo, ux=ux[gi], g=go)
```

For each original `$x$` value, we have associated the corresponding `$\bar{x}$`
value.  In `group_slope` we combined the mean calculation and recycling into one
step: `xu <- (xs/gn)[gi]`.

This is pretty fast with our original 10MM record data set:

```{r eval=FALSE}
system.time(slope.rs <- group_slope(x, y, grp))
```
```
   user  system elapsed
  4.110   0.785   4.915
```

Compare to the `vapply` version from the [prior grouped statistics post][8]:

```{r eval=FALSE}
system.time({
  id <- seq_along(grp)
  id.split <- split(id, grp)
  slope.ply <- vapply(id.split, function(id) slope(x[id], y[id]), 0)
})
```
```
   user  system elapsed
 13.817   0.216  14.184
```
```{r}
all.equal(slope.ply, c(slope.rs), check.attributes=FALSE)
```
```
[1] TRUE
```

And a summary of all the timings including `data.table`:

```{r rowsum-timings-all, echo=FALSE}
funs <- c('*pply', '*pply', 'data.table', 'data.table', 'rowsum')
times <- data.frame(
  Function=factor(funs, levels=unique(funs)),
  Version=c('normal', 'ordered', 'normal', 'optim', 'normal'),
  time=c(14.184, 9.867 , 7.959, 3.697, 4.915)
)
ggplot(times, aes(x=Version, y=time)) +
  geom_col() +
  facet_grid(.~Function, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

`rowsum` is ~3x faster than even basic `*pply` version, and faster than the
unoptimized `data.table` [^timing-note].  But `data.table` keeps the performance
crown, albeit with a little help[^dt-optim].  More details for the benchmarks
are [in the appendix](#other-benchmarks).

So like Rocky Balboa we walk off into the sunset, without the title but
satisfied we put up a good fight.  Except...

# Round II

Remember how we saw earlier that a fast order opened up opportunities for [new
algorithms](#alt-algo)?  [John Mount][10], shows how we can compute group sums
[using `cumsum`][11] on group-ordered data[^prior-art].  With a little work we
can generalize it.

The concept is simple: order by group, compute cumulative sum, pull out the last
value for each group, and take their differences.  Visually:

```{r cumsum-ex, echo=FALSE}
library(ggbg)
RNGversion("3.5.2"); set.seed(42)
n1 <- 7
x1 <- seq_len(n1)
y1 <- runif(n1);
colors <- c('#3333ee', '#33ee33', '#eeee33')
g1 <- sample(1:3, n1, replace=TRUE)
steps <- c(
  '1 - Start', '2 - Sort By Group', '3 - Cumulative Sum',
  '4 - Last Value in Group', '5 - Take Differences', '6 - Group Sums!'
)
steps <- factor(steps, levels=steps)
df1 <- data.frame(
  x1, y1, g1=as.character(g1), step=steps[[1]], stringsAsFactors=FALSE
)
df2 <- df1[order(g1),]
df2[['x1']] <- x1
df2[['step']] <- steps[[2]]
df3 <- df2
df3 <- transform(
  df3, yc=cumsum(y1), step=steps[[3]],
  last=c(head(g1, -1) != tail(g1, -1), TRUE)
)
df4 <- transform(df3, step=steps[[4]])
df5 <- transform(
  subset(df4, last), x1=3:5, y1=c(yc[[1]], diff(yc)), step=steps[[5]]
)
df6 <- transform(df5, step=steps[[6]])

plot.extra <- list(
  facet_wrap(~step, ncol=2),
  ylab(NULL), xlab(NULL),
  theme(
    axis.text.x=element_blank(), axis.text.y=element_blank(),
    panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
    axis.ticks=element_blank()
  ),
  scale_fill_manual(values=setNames(colors, 1:3), guide=FALSE)
)

ggplot(mapping=aes(x=x1, y=y1, fill=g1)) +
  geom_col(data=df1) +
  geom_col(data=df2) +
  geom_col(data=df3, mapping=aes(y=yc)) +
  geom_col(
    data=df4, mapping=aes(y=yc, alpha=I(ifelse(last, 1, .15))),
    width=0.9
  ) +
  geom_col(data=df5, width=0.9, position='waterfall') +
  geom_col(data=df6, width=0.9) +
  plot.extra
```

This is the data we used for the visualization:

```{r echo=FALSE}
x1 <- y1 # a bit confusing with x/y for ggplot
```
```{r}
g1
x1
```

The first three steps are obvious:

```{r}
ord <- order(g1)
go <- g1[ord]
xo <- x1[ord]
xc <- cumsum(xo)
```

Picking the last value from each group is a little harder, but we can do so with
the help of `base::rle`.  `rle` returns the lengths of repeated-value
sequences within a vector.  In a vector of ordered group ids, we can use it to
compute the lengths of each group:

```{r}
go
(grle <- rle(go))
(gn <- grle[['lengths']])
```

This tells us the first group has two elements, the second also two, and the
last three.  We can translate this into indices of the original vector with
`cumsum`, and use it to pull out the relevant values from the cumulative sum of
the `x` values:

```{r}
(gnc <- cumsum(gn))
(xc.last <- xc[gnc])
```

To finish we just take the differences:

```{r}
diff(c(0, xc.last))
```

I wrapped the whole thing into the [`group_sum` function](#group_sum) you can
see in the appendix:

```{r "group_sum-def", echo=FALSE}
group_sum <- function(x, grp) {
  ## Order groups and values
  ord <- order(grp)
  go <- grp[ord]
  xo <- x[ord]

  ## Last values
  grle <- rle(go)
  gnc <- cumsum(grle[['lengths']])
  xc <- cumsum(xo)
  xc.last <- xc[gnc]

  ## Take diffs and return
  gs <- diff(c(0, xc.last))
  setNames(gs, grle[['values']])
}
```
```{r}
group_sum(x1, g1)
```

Every step of `group_sum` is internally vectorized[^int-vec], so the function is
fast.  We demonstrate here with the original 10MM data set:

```{r eval=FALSE}
system.time(x.grpsum <- group_sum(x, grp))
```
```
   user  system elapsed
  1.098   0.244   1.344
```
```{r eval=FALSE}
all.equal(x.grpsum, c(x.ref), check.attributes=FALSE)
```
```
[1] TRUE
```

This is slightly faster than [pre-ordered `rowsum`](#rowsum-ordered)
calculation, but as we will see shortly it has the additional advantage that we
can capture and re-use intermediate steps.  I wrote a [variation](#cumulative
group-sum-with-na-and-inf) that handles `NAs` and `Inf` values to show it can be
done, but we will ignore than wrinkle going forward for clarity's sake.

Now for the real fun: let's re-implement the [initial `rowsum` group slope
function](#group-slope-rs) with this method:

```{r eval=FALSE}
.group_sum_int <- function(x, last.in.group) {
  xgc <- cumsum(x)[last.in.group]
  diff(c(0, xgc))
}
group_slope2 <- function(x, y, grp, group_sum=.group_sum_int) {
  ## order inputs by group
  o <- order(grp)
  go <- grp[o]
  xo <- x[o]
  yo <- y[o]

  ## group sizes and group indices
  grle <- rle(go)
  gn <- grle[['lengths']]
  gi <- rep(seq_along(gn), gn)
  gnc <- cumsum(gn)    # Last index in each group

  ## compute mean(x) and mean(y), and recycle them
  ## to each element of `x` and `y`:
  sx <- group_sum(xo, gnc)
  ux <- (sx/gn)[gi]
  sy <- group_sum(yo, gnc)
  uy <- (sy/gn)[gi]

  ## (x - mean(x)) and (y - mean(y))
  x_ux <- xo - ux
  y_uy <- yo - uy

  ## Slopes!
  x_ux.y_uy <- group_sum(x_ux * y_uy, gnc)
  x_ux2 <- group_sum(x_ux ^ 2, gnc)
  setNames(x_ux.y_uy / x_ux2, grle[['values']])
}
system.time(slope.gs <- group_slope2(x, y, grp))
```
```
   user  system elapsed
  1.952   0.605   2.573
# can't reproduce above one right now?
   user  system elapsed 
  2.032   0.673   2.715 
   user  system elapsed 
  2.106   0.743   2.876 
```

Oh snap, 40% faster than `data.table`!

```{r slope-timings-2, echo=FALSE}
funs <- c('*pply', '*pply', 'data.table', 'data.table', 'rowsum', 'cumsum')
times <- data.frame(
  Function=factor(funs, levels=unique(funs)),
  Version=c('normal', 'ordered', 'normal', 'optim', 'normal', 'normal'),
  time=c(14.184, 9.867 , 7.959, 3.697, 4.915, 2.573)
)
ggplot(times, aes(x=Version, y=time)) +
  geom_col() +
  facet_grid(.~Function, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

The key advantage `group_slope2` has over the `rowsum` version is that it can
re-use `gnc`, the vector of indices to the last value in each group.
Computing `gnc` is the expensive part of the `cumsum` group sum calculation:

```{r}
o <- order(grp)
go <- grp[o]
xo <- x[o]
gn <- rle(go)[['lengths']]
gi <- rep(seq_along(gn), gn)
gnc <- cumsum(gn)
```
```{r eval=FALSE}
system.time(.group_sum_int(xo, gnc))
```
```
   user  system elapsed
  0.042   0.008   0.050
```
```{r eval=FALSE}
system.time(group_sum(xo, go))
```
```
   user  system elapsed
  0.567   0.055   0.622
```

This is an order of magnitude difference for the grouped sum step, which we four
times when computing the slope.

# Precision Schmecision

## A Disclaimer

I have no special knowledge of floating point precision issues.  Everything I
know is from reading the [wiki page][12], random web research, and
experimentation.  If your billion dollar Mars lander burns up on entry because
you relied on information in this post, you only have yourself to blame.

## Oh the Horror!

The skeptical reader might have noticed that we did not compare the results of
`group_slope2` with reference values.  It turns out that our blazing fast
benchmark hero is cutting some corners:

```{r eval=FALSE}
all.equal(slope.gs, c(slope.rs), check.attributes=FALSE)
```
```
[1] "Mean relative difference: 0.001404195"
```
With a generous tolerance we find equality:
```{r eval=FALSE}
all.equal(slope.gs, c(slope.rs), check.attributes=FALSE, tolerance=2e-3)
```
```
[1] TRUE
```

Let's find the worst offender:<span id=prec-error></span>

```{r eval=FALSE}
bad.el <- which.max(abs(slope.gs / slope.rs - 1))
slope.gs[bad.el]
```
```
   616826
-2511.833
```
```{r echo=FALSE}
# hard coding the above value so that rest of stuff that we actually
# run can use it
bad.el <- 616793L
bad.grp <- 616826L
```
```{r eval=FALSE}
slope.rs[bad.el]
```
```
   616826
-2977.281
```

Ugh, that's pretty bad.  These are the values responsible:

```{r eval=FALSE}
bad.grp <- as.integer(names(slope.gs[bad.el]))
```
```{r}
(bad.x <- x[grp == bad.grp])
```
```
[1] 0.4239786 0.4239543
```
```{r}
(bad.y <- y[grp == bad.grp])
```
```
[1] 0.7647899 0.8370645
```

The `x` values are almost certainly the cause of the problem, as `$x
- \bar{x}$` produces some small numbers.

```{r}
old.opt <- options(digits=22, scipen=100) # show more digits
```
```{r eval=FALSE}
bad.x - mean(bad.x)
```
```
[1]  0.00001213769 -0.00001213769
```

But these are well within the range of values that should be manageable
for [double precision floats][12] ("doubles" henceforth) which are what R
"numeric" values are stored as.  So what's going on?  It turns out that the
`cumsum` approach severely amplifies precision issues.  Typical double precision
floating point numbers have about 15-16 digits of precision.  In order to
mitigate precision issues, [R internally uses a representation][13] that allows
[18-19 digits of precision][14] for `cumsum`[^extended-precision].  Yet that is
not enough here.  To see why we need to review our algorithm:

```{r cumsum-review, echo=FALSE}
ggplot(mapping=aes(x=x1, y=y1, fill=g1)) +
  geom_col(
    data=df4, mapping=aes(y=yc, alpha=I(ifelse(last, 1, .15))),
    width=0.9
  ) +
  geom_col(data=df5, width=0.9, position='waterfall') +
  plot.extra
```

While the overall cumulative value is going to be reasonably precise, the
_differences_ between adjacent cumulative values will cause us to lose some
precision.  Let's look at the actual values involved, starting with what the
accumulated sum of all the prior ordered `x` values is by the time we reach our
bad group:

```{r}
bad.grp.idx <- which(go == bad.grp)
bad.grp.start <- head(bad.grp.idx, 1)
bad.grp.end <- tail(bad.grp.idx, 1)
(x.cum.prev <- sum(xo[1:(bad.grp.start-1)]))  # end val of prior group
```
```
[1] 3084341.120945733506233
```
And the cumulative sum at the end of our group:

```{r eval=FALSE}
(x.cum <- sum(xo[1:bad.grp.end]))             # end val of bad group
```
```
[1] 3084341.968878706917167
```

In our algorithm we take the difference of these two numbers to get the group
sum.  Here we show the two numbers and their difference highlighting the
precision digits in green[^precision-digits]:

```{r eval=FALSE}
group.sum <- x.cum - x.cum.pre
rbind(x.cum.pre, x.accum.post, group.sum)
```
```{r echo=FALSE, results='asis'}
val <- "                                       [,1]
x.accum.pre  \033[42m3084341.120945733\033[m5062325000763
x.accum.post \033[42m3084341.968878706\033[m9171667098999
group.sum          0.\033[42m847932973\033[m4109342098236"
writeFansi(val)
```

Our group sum is small, so the group ends are near each other.  By taking the
difference we lose all the precision that was contributed by the first seven
digits of the cumulative numbers, leaving us with about nine
digits[^precision-digits].

Normally this should still be plenty of precision, but the next step `$x -
\bar{x}$` peels off even more, as shown here with the first value of `$x$` in
the group:

```{r eval=FALSE}
x1 <- xo[bad.grp.start]
ux <- group.sum / length(bad.grp.idx)
x1_ux <- x1 - ux
rbind(x1, ux, x1_ux)
```
```{r echo=FALSE, results='asis'}
val <- "                              [,1]
x1    0.\033[42m423978624399751\033[m42478942871
ux    0.\033[42m423966486\033[m70546710491180420
x1_ux 0.0000\033[42m12137\033[m69428431987762451"
writeFansi(val)
```

More generally, any time we subtract two numbers that are very similar we lose
precision as in this example and the prior one.  Additionally, as also shown in
this example, we will lose precision anytime add or subtract numbers of
mismatched precisions[^prec-mismatch].

Still, this loss of precision can't account for the [~15% error we
observed](#prec-error).  Where the wheels really fall off is in the `$\sum(x -
\bar{x})^2$` step.  This is the code from the `group_slope2` function that gets
us to `$(x - \bar{x})^2$`, edited to remove irrelavant parts:

```{r eval=FALSE}
grle <- rle(go)
gn <- grle[['lengths']]
gi <- rep(seq_along(gn), gn)
gnc <- cumsum(gn)    # Last index in each group

sx <- .group_sum_int(xo, gnc)
ux <- (sx/gn)[gi]
x_ux <- xo - ux
x_ux2 <- (x_ux ^ 2)
```

Already we were at only about five digits of precision and now we're going to
take the difference between these two numbers:

```{r eval=FALSE}
(x_ux2.cum.prev <- cumsum(x_ux2)[bad.grp.start - 1])
```
```
[1] 462818.5549556238111109
```
```{r eval=FALSE}
(x_ux2.cum <- cumsum(x_ux2)[bad.grp.end])
```
```
[1] 462818.5549556241603568
```

This is the computing equivalent of walking down the steps into a dark basement
with ominous background music in a horror movie.  To leave no doubt about the
carnage that is about to unfold (viewer discretion advised):<span
id=carnage></span>

```{r eval=FALSE}
x_ux2.diff <- x_ux2.cum - x_ux2.cum.prev
rbind(x_ux2.cum.prev, x_ux2.cum, x_ux2.diff)
```
```{r echo=FALSE, results='asis'}
val <- "                                                 [,1]
x_ux2.cum.prev \033[42m46281\033[m8.5549556238111108541488647460938
x_ux2.cum      \033[42m46281\033[m8.5549556241603568196296691894531
x_ux2.diff          0.0000000003492459654808044433594"
writeFansi(val)
```

Zilch precision left[^mag-prec].  I will not transcribe the scream of horror and
agony for the sake of the children, but I will show you the aftermath:

```{r eval=FALSE}
seven31::reveal(diff)
```
```{r echo=FALSE, results='asis'}
val <- "\033[31m0\033[m \033[34m01111011111\033[m (-32) 1000000000000000000000000000000000000000000000000000 : diff"
writeFansi(val)
```

[Romain François's][15] [`seven31`][16] allows us to inspect the internal
representation of floating point numbers.  These have three components that
together account for the 64 bits used in encoding the number.  In order they
are:

* Sign (1 bit: <code style='color: red !important;'>0</code>), which tells us
  if the number is positive or negative.
* Exponent (11 bits: <code style='color: blue !important;'>01111011111</code>,
  also shown in decimal form as `(-32)`) which gives the magnitude of the
  number.
* Normalized value (52 bits: <code>100000000000000...</code>), which is  the
  part that matters for precision.  There is an additional implicit leading bit
  set to `1` that is not shown.

This suggests there are essentially two bits of precision: the only `1` visible
in the "Normalized Value" section, and implicitly, the leading `1` of that same
value that is not shown.  Check this out:

```{r}
(2^-32) *         # exponent
  (2^0 + 2^-1)    # normalized value
```
```{r eval=FALSE}
xux2.diff
```

We exactly reproduced what we got from subtracting the [cumulative values
earlier](#carnage). The `2^0` corresponds to the implicit value bit that isn't
shown, and `2^-1` to the one `1` at the first position in the "normalized value"
portion of the double.  Each bit in the "normalized value" section is
represented by `$2^{-n}$` where `n` is the position in the bit field.

I hope you will forgive some of the hand-waving I indulged in above.  After all
I started off claiming all precision was lost, then noted that the bit
representation suggests two bits of precision.  In reality, the strict "bit"
precision of the number is higher than that as some of the zero bits are
technically significant[^tech-sig], but this is irrelevant as it arises from the
combination of numbers of different magnitudes[^mag-diff].  So the above example
is a situation of two wrongs making one approximately correct "Aha" result, so I
included it because it is a good demonstration of how misleading decimal
"display" precision is.

## A New Hope

Before we give up it is worth pointing out that precision loss in some cases can
be considered a feature.  For example, [Drew Schmidt's `float`][17] package
implements single precision numerics for R as an explicit trade-off of precision
for memory and speed.  In a sense, we are doing something similar by using
`cumsum`, although not explicitly.

Still, it would be nice if we could make a version of this method that
doesn't suffer from this precision infirmity.  And it turns out we can!  The
main source of precision loss is due to the accumulated sum eventually growing
much larger than any given individual group in size.  This became particularly
bad for [cumulative sum of `$(x - \bar{x})^2$`](#carnage) due to the squaring
further exacerbating relative magnitude differences.  To compensate for this
we're going to adapt a strategy from `base::mean`.  Here is an snippet from the
[C code in R3.3][18] altered for clarity:

```{c eval=FALSE}
LDOUBLE s = 0., t = 0.;                // extended precision floats
for (i = 0; i < n; i++) s += x[i];     // sum all values i->n
s /= n;                                // turn `s` into mean by dividing by `n`
if(R_FINITE((double)s)) {
    for (i = 0; i < n; i++)            // second pass
        t += (x[i] - s);               // <- KEY STEP
    s += t/n;                          // update mean
}
ans[0] = (double) s;
```

In the "key step" we are subtracting the mean from each value before adding it
to the secondary accumulator `t`.  The expected value of `(x[i] - s)` is zero,
so `t` should remain close to that throughout, absent pathological
inputs[^mean-pathological].  The end value of `t` will be the sum of the
incremental precision error from first-pass mean.  This does not guarantee
perfect precision, but it should always improve precision.

A similar adjustment for `cumsum` based group sums is:

* Compute group sums.
* Insert the negative of the group sums after each group in the original data.
* Recompute the group sums.

If there was no precision loss at all the result of step 3 above should be zero
for every group.  Visually:

```{r echo=FALSE}
steps <- c(
  '2a - Sort By Group', '3a - Insert Neg Group Sum', '4a - Cumulative Sum',
  '5a - Last Val Is Error'
)
df2a <- transform(df2, step=factor(steps[1], levels=steps))
df3 <- rbind(
  transform(df2a, x1=x1+cumsum(c(0, diff(as.integer(g1))))),
  with(
    df2,
    data.frame(
      x1=c(3L,6L,10L), 
      y1=-rowsum(y1,g1)+(runif(3)-.5)*.05, 
      g1=as.character(1:3),
      step=step[1]
) ) )
df3 <- df3[order(df3[['x1']]),]
df3[['step']] <- steps[2]
df4 <- transform(df3, y1=cumsum(y1), step=steps[3])
df5 <-
  transform(df4, last=c(head(g1, -1) != tail(g1, -1), TRUE), step=steps[4])
df5a <- transform(subset(df5, last))

ggplot(mapping=aes(x=x1, y=y1, fill=g1)) +
  geom_col(data=df2) +
  geom_col(data=df3) +
  geom_col(data=df4) +
  geom_col(data=df5, mapping=aes(alpha=I(ifelse(last, 1, .15)))) +
  geom_tile(
    data=df5a, mapping=aes(fill=NA, color=g1), alpha=1,
    height=.6, width=1.2, size=0.5
  ) +
  scale_colour_manual(
    values=setNames(c(colors[1:2], 'yellow'), 1:3), guide=FALSE
  ) +
  plot.extra
```

If the first pass group sum was fully precise, the values inside the
highlighting boxes should be zero. The small values we see inside the boxes
represent the errors of each group computation[^for-effect].  Steps that we are
not showing above is collecting these values, taking their differences and
adding them back to the first pass values.

`.group_sum_int2` below embodies this approach.  One slight modification is that
we subtract the first pass group value from the last value in the group instead
of adding it as it own stand-alone value in order to save a new vector
allocation:

```{r}
.group_sum_int2 <- function(x, last.in.group){
  x.grp <- .group_sum_int(x, last.in.group)     # imprecise pass
  x[last.in.group] <- x[last.in.group] - x.grp  # subtract from each group
  x.grp + .group_sum_int(x, last.in.group)      # compute errors and add
}
```

```{r echo=FALSE}
system.time(slope.gs2 <- group_slope2(x, y, grp, group_sum=.group_sum_int2))
```
```
   user  system elapsed 
  2.624   0.918   3.562 
# this one seems to have been a fluke?  Maybe system not
# busy and able to give mem real quick? Not a fluke once we killed
# FF and all other random processes
   user  system elapsed 
  2.446   0.772   3.246 

   user  system elapsed 
  2.543   0.754   3.312 
```

Updated reference times for `data.table`.  This is after we killed all
extraneous processes, including firefox.

```{r eval=FALSE}
setDTthreads(1)
DT <- data.table(grp, x, y)
system.time({
  setkey(DT, grp)
  DTsum <- DT[, .(ux=mean(x), uy=mean(y)), keyby=grp]
  DT[DTsum, `:=`(x_ux=x - ux, y_uy=y - uy)]
  DT[, `:=`(x_ux.y_uy=x_ux * y_uy, x_ux2=x_ux^2)]
  DTsum <- DT[, .(x_ux.y_uy=sum(x_ux.y_uy), x_ux2=sum(x_ux2)), keyby=grp]
  res.slope.dt2 <- DTsum[, .(grp, V1=x_ux.y_uy / x_ux2)]
})
```
```
# Bah, it was with four threads.  Still, useful number
   user  system elapsed 
  7.691   0.815   2.828 
```

We can do a little better though since we don't need the full precision.
* Get max cumsum value
* Compare to smallest group
* But one big problem with this is we don't actually know for sure that the
  cumulative sum is growing throughout, so last value is not necessarily the
  largest in magnitude.  Shortcut might be taking the max and min of the
  cumulative sum?  And to find the smallest value?  Can we do it without taking
  the abs and copying the whole group?

```{r}
.group_sum_int3 <- function(x, last.in.group, p.bits=53){
  x.grp <- .group_sum_int(x, last.in.group)
  x[last.in.group] <- x[last.in.group] - x.grp
  x.grp + .group_sum_int(x, last.in.group)
}

.group_sum_intb <- function(x, last.in.group, p.bits=53) {
  xgc <- cumsum(x)[last.in.group]
  gmax <- log2(max(abs(range(xgc))))
  gs <- diff(c(0, xgc))
  gsabs <- abs(gs)
  gmin <- min(log2(gsabs[gsabs > 0]))
  precision <- floor(53 + (gmin - gmax))
  if(precision < p.bits) {
    x[last.in.group] <- x[last.in.group] - gs
    gs <- gs + .group_sum_int(x, last.in.group)
  }
  structure(gs, precision=precision, precision.mitigation=precision < p.bits)
}
gsfun <- function(x, last.in.group) .group_sum_intb(x, last.in.group, 16)
system.time(slope.gs3 <- group_slope2(x, y, grp, group_sum=gsfun))

system.time(slope.gs2 <- group_slope2(x, y, grp, group_sum=.group_sum_int2))

system.time(.group_sum_intb(xo, gnc, 50))
system.time(.group_sum_int(xo, gnc))
system.time(.group_sum_intb(xo, gnc, 50))

all.equal(
  slope.gs3[!is.na(slope.rs)], slope.rs[!is.na(slope.rs)], 
  check.attributes=FALSE
)

```


## testing whether we're better off with different method
.group_sum_int3 <- function(x, last.in.group){
  x.grp <- .group_sum_int(x, last.in.group)     # imprecise pass
  zz <- numeric(length(x) + length(last.in.group))
  zz[last.in.group] <- last.in.group
  x.grp
}

```
sy <- .group_sum_int(yo, gnc)
uy <- (sy/gn)[gi]

## (x - mean(x)) and (y - mean(y))
y_uy <- yo - uy

ux0 <- sum(xo[bad.grp.idx]) / length(bad.grp.idx)
x_ux0 <- xo[bad.grp.idx] - ux0

uy0 <- sum(yo[bad.grp.idx]) / length(bad.grp.idx)
y_uy0 <- yo[bad.grp.idx] - uy0

xoc <- cumsum(xo)
ux1 <- (xoc[bad.grp.end] - xoc[bad.grp.start - 1]) / length(bad.grp.idx)
x_ux1 <- xo[bad.grp.idx] - ux1

yoc <- cumsum(yo)
uy1 <- (yoc[bad.grp.end] - yoc[bad.grp.start - 1]) / length(bad.grp.idx)
y_uy1 <- yo[bad.grp.idx] - uy1


x_uxc1 <-

sum(x_ux1 * y_uy1) / sum(x_ux1 ^2)
sum(x_ux0 * y_uy0) / sum(x_ux0 ^2)

## group sizes and group indices
grle <- rle(go)
gn <- grle[['lengths']]
gi <- rep(seq_along(gn), gn)
gnc <- cumsum(gn)    # Last index in each group

## compute mean(x) and mean(y), and recycle them
## to each element of `x` and `y`:
sx <- .group_sum_int(xo, gnc)
ux <- (sx/gn)[gi]
sy <- .group_sum_int(yo, gnc)
uy <- (sy/gn)[gi]

## (x - mean(x)) and (y - mean(y))
x_ux <- xo - ux
y_uy <- yo - uy

## Slopes!
x_ux.y_uy <- .group_sum_int(x_ux * y_uy, gnc)
x_ux2 <- .group_sum_int(x_ux ^ 2, gnc)



```


```
.group_sum_int2 <- function(x, last.in.group, precise=FALSE){
  x.grp <- sum_grp1_int(x, last.in.group)
  if(precise) {
    x[last.in.group] <- x[last.in.group] - x.grp
    x.grp + sum_grp1_int(x, last.in.group)
  } else x.grp
}
```


But we can make it precise
```{r eval=FALSE}
system.time(slope1(x, y, grp, precise=TRUE))
```
```
   user  system elapsed
  2.481   1.087   3.754
  # second faster timing was from a fresh session, repeated several times
   user  system elapsed
  2.399   0.933   3.348
```

```{r eval=FALSE}
sum_g3 <- function(x, grp, na.rm=TRUE) {
  ord <- order(grp)
  id.ord <- id[ord]
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  max.grp <- max(grp.rle[['lengths']])

  # this NA handling doesn't work b/c for na.rm=FALSE you still get NAs

  res <- matrix(NA_real_, ncol=length(grp.rle[['lengths']]), nrow=max.grp)

  # each group that isn't as long as the longest group needs padding

  rle.len <- grp.rle[['lengths']]
  grp.pad <- max.grp - rle.len
  id.raw <- rep(1L, length(x))
  id.raw[(cumsum(rle.len) + 1L)[-length(rle.len)]] <-
    grp.pad[-length(rle.len)] + 1L
  id <- cumsum(id.raw)

  res[id] <- x[ord]
  structure(colSums(res, na.rm=na.rm), groups=grp.rle[['values']])
}
system.time(sum_g3(x, grp))
```
```
   user  system elapsed
  1.186   0.374   1.634
```
```{r eval=FALSE}
# lens: how long each group is
# maxlen: longest group
sum_grp2 <- function(x, lens, maxlen, mode='sum') {

  res <- matrix(NA_real_, ncol=length(lens), nrow=maxlen)

  # Generate indices that will map to the correct spots in `res` from `x`,
  # which means add whatever padding we need to the index value for the next
  # column

  len_1 <- lens[-length(lens)]
  grp.pad <- (maxlen + 1L) - len_1
  id.raw <- rep(1L, length(x))
  len_1[1L] <- len_1[1L] + 1L
  id.raw[cumsum(len_1)] <- grp.pad
  id <- cumsum(id.raw)

  # Inject the x values according to these indices that should place each gropu
  # in a column

  res[id] <- x

  if(identical(mode, 'sum')) colSums(res, na.rm=TRUE)
  else if(identical(mode, 'mean')) colMeans(res, na.rm=TRUE)
  else stop("Invalid mode")
}
slope2 <- function(x, y, grp) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.len <- grp.rle[['lengths']]
  max.grp <- max(grp.len)

  xo <- x[ord]
  xi <- rep(seq_along(grp.len), grp.len)
  xu <- sum_grp2(xo, grp.len, max.grp, 'mean')
  x_ux <- xo - xu[xi]

  yo <- y[ord]
  yu <- sum_grp2(yo, grp.len, max.grp, 'mean')
  y_uy <- yo - yu[xi]

  x_ux.y_uy <- sum_grp2(x_ux * y_uy, grp.len, max.grp, 'sum')
  x_ux2 <- sum_grp2(x_ux ^ 2, grp.len, max.grp, 'sum')

  x_ux.y_uy / x_ux2
}
slope2a_int <- function(xo, yo, lens) {
  max.grp <- lens[1]

  xi <- rep(seq_along(lens), lens)
  xu <- sum_grp2(xo, lens, max.grp, 'mean')
  x_ux <- xo - xu[xi]

  yu <- sum_grp2(yo, lens, max.grp, 'mean')
  y_uy <- yo - yu[xi]

  x_ux.y_uy <- sum_grp2(x_ux * y_uy, lens, max.grp, 'sum')
  x_ux2 <- sum_grp2(x_ux ^ 2, lens, max.grp, 'sum')

  x_ux.y_uy / x_ux2
}
slope2a <- function(x, y, grp, splits=5) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.len <- grp.rle[['lengths']]

  ord2 <- order(rep(grp.len, grp.len), decreasing=TRUE)
  ordg <- order(grp.len, decreasing=TRUE)
  grp.len.o <- grp.len[ordg]
  len.max <- grp.len.o[1L]  # will break if no groups
  len.min <- grp.len.o[length(grp.len.o)]

  # order the inputs

  ord3 <- ord[ord2]
  xo <- x[ord3]
  yo <- y[ord3]
  go <- grp.ord[ord2]

  # simple initial cut, just cut into equal splits

  cuts <- as.integer(
    round(seq(1L, length(grp.len) + 1L, length.out=splits + 1L))
  )
  grp.len.o.c <- cumsum(c(1L, grp.len.o))
  res <- vector("list", splits)

  for(i in seq_len(splits)){
    # Figure out that starting and ending elements for each group

    start.g <- cuts[i]
    end.g <- cuts[i + 1L]

    start <- grp.len.o.c[start.g]
    end <- grp.len.o.c[end.g] - 1L

    idx.g <- start.g:(end.g - 1L)
    idx <- start:end

    res[[i]] <- slope2a_int(xo[idx], yo[idx], grp.len.o[idx.g])
  }
  # Reorder back in ascending group order instead of group size order

  res.fin <- numeric(length(grp.len))
  res.fin[ordg] <- unlist(res)
  res.fin
}
system.time(slope2(x, y, grp))
RNGversion("3.5.2"); set.seed(42)
x2 <- runif(100)
y2 <- runif(100)
g2 <- sample(1:10, 100, rep=T)
```
```
   user  system elapsed
  2.827   0.932   3.807
  # can't reproduce the earlier timings...
   user  system elapsed
  3.185   1.285   4.783
  # now I can ...
```
```{r eval=FALSE}
system.time(slope2a(x, y, grp))
```
```
   user  system elapsed
  2.998   1.055   4.082
```
```{r eval=FALSE}
dummy <- function(sizes) {
  res <- vector("list", length(sizes))
  for(i in seq_along(sizes)) {
    mx <- matrix(numeric(), nrow=sizes[i], ncol=as.integer(1e6 / length(sizes)))
    res[[i]] <- colSums(mx, na.rm=TRUE)
  }
}
system.time(dummy(c(28, 21, 14, 7, 1)))
```

```{r eval=FALSE}

DT <- copy(DT.raw)
system.time(res.ref <- DT[, sum(x), keyby=grp][['V1']])
#   user  system elapsed
#  1.071   0.134   1.216
system.time(res <- sum_g2(x, grp))
#   user  system elapsed
#  1.286   0.309   1.692
all.equal(res, res.ref, check.attributes=FALSE) # TRUE

system.time(res2 <- sum_g2(x, grp))
system.time(res3 <- sum_g3(x, grp))
system.time(res4 <- rowsum(x, grp))

sum_winvector <- function(DF) {
  odata <- DF[order(DF$grp),,drop=FALSE]
  first_indices <- mark_first_in_each_group(odata, "grp")
  sum_g(odata[['x']], first_indices)
}
```
```
  user  system elapsed
 1.810   0.740   2.651
```
Note on vector size[^vec-size].


# Conclusions

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id='feedback-cont'></div>

# Appendix

## Acknowledgments

* Matt Dowle and the `data.table` team for contributing their `radix` order to
  R.
* Romain Francois for `seven31` which allowed us to debug the double precision
  precision issues.
* Oleg Sklyar, Duncan Murdoch, Mike Smith, Dirk Eddelbuettel, Romain Francois,
  Karline Soetaert for [`inline`][7].

## Sorting Versions

I quickly threw together C functions that `unique` and `rowsum` on sorted data.
They support a very narrow set of inputs, but for those inputs the timing
comparisons relative to the base equivalents should be fair.  `.rowsums` does
benefit from being able to assume the input will be a vector and not a matrix,
but that should be a small difference.

I tried two variations from the one we looked at in the [main body](#unique-c)
of the post that generally support the timings against the base functions with
input sorting:

* 1e8 vector with ~10 sized groups
* 1e7 vector with ~1 sized groups

`rowsums` preserves the ~2x advantage in both scenarios.  `uniques` maintains
the advantage for the first scenario, but in the second is at parity.  This is
because for `uniques` we need to unsort the result and when there are no
duplicates the vector to unsort is large.  If we were allowed to return the
results sorted then `uniques` is faster, but by a smaller margin than with the
original example.

```{r eval=FALSE}
.uniques <- inline::cfunction(sig=c(x='integer'), body="
  R_xlen_t len, i, len_u = 1;
  SEXP res;
  int *xi = INTEGER(x);
  len = XLENGTH(x);

  if(len > 1) {
    // count uniques
    for(i = 1; i < len; ++i) {
      if(xi[i - 1] != xi[i]) {
        ++len_u;
    } }
    // allocate and record uniques
    res = PROTECT(allocVector(INTSXP, len_u));
    int *resi = INTEGER(res);
    *resi = 1;
    R_xlen_t j = 1;
    for(i = 1; i < len; ++i) {
      if(xi[i - 1] != xi[i]) {
        resi[j++] = i + 1;  // 1 based indexing
    } }
    UNPROTECT(1);
  } else {
    res = x;
  }
  return res;
")

.rowsums <- inline::cfunction(
  sig=c(x='numeric', g='integer'),
  body="
  R_xlen_t len, i, len_u = 1;
  SEXP res, res_x, res_g;
  int *gi = INTEGER(g);
  double *xi = REAL(x);
  len = XLENGTH(g);
  if(len != XLENGTH(x)) error(\"Unequal Length Vectors\");
  res = PROTECT(allocVector(VECSXP, 2));

  if(len > 1) {
    // count uniques
    for(i = 1; i < len; ++i) {
      if(gi[i - 1] != gi[i]) {
        ++len_u;
    } }
    // allocate and record uniques
    res_x = PROTECT(allocVector(REALSXP, len_u));
    res_g = PROTECT(allocVector(INTSXP, len_u));

    double *res_xi = REAL(res_x);
    int *res_gi = INTEGER(res_g);
    R_xlen_t j = 0;

    res_xi[0] = 0;
    for(i = 1; i < len; ++i) {
      res_xi[j] += xi[i - 1]; // we don't check for double overflow...
      if(gi[i - 1] != gi[i]) {
        res_gi[j] = gi[i - 1];
        ++j;
        res_xi[j] = 0;
    } }
    res_xi[j] += xi[i - 1];
    res_gi[j] = gi[i - 1];

    SET_VECTOR_ELT(res, 0, res_x);
    SET_VECTOR_ELT(res, 1, res_g);
    UNPROTECT(2);
  } else {
    // Don't seem to need to duplicate x/g
    SET_VECTOR_ELT(res, 0, x);
    SET_VECTOR_ELT(res, 1, g);
  }
  UNPROTECT(1);
  return res;
")
```

## Other Benchmarks


Ordered version:

```{r eval=FALSE}
system.time({
  o <- order(grp)
  go <- grp[o]
  id <- seq_along(grp)[o]
  id.split <- split(id, go)
  slope.ply <- vapply(id.split, function(id) slope(x[id], y[id]), 0)
})
```
```
   user  system elapsed
  9.565   0.203   9.867
```
```{r eval=FALSE}
all.equal(slope.ply, c(slope.rs), check.attributes=FALSE)
```
```
[1] TRUE
```

`data.table`, not optimized:

```{r eval=FALSE}
setDTthreads(1)
DT <- data.table(grp, x, y)
system.time(DT[, slope(x, y), grp])
```
```
   user  system elapsed
  7.798   0.082   7.959
```

## Function Definitions

### group_sum

```{r group_sum-def, eval=FALSE}
```

### Cumulative Group Sum With NA and Inf

A correct implementation of the single pass `cumsum` based `group_sum` requires
a bit of work to correctly handle both `NA` and `Inf` values.  Both of these
need to be pulled out of the data ahead of the cumulative step otherwise they
would wreck all subsequent calculations.  The rub is they need to be re-injected
into the results, and with `Inf` we need to account for groups computing to
`Inf`, `-Inf`, and even `NaN`.

I implemented a version of `group_sum` that handles these for illustrative
purposes.  It is lightly tested so you should not consider it to be generally
robust.

```{r eval=FALSE}
group_sum3 <- function(x, grp, na.rm=FALSE) {
  if(length(x) != length(grp)) stop("Unequal length args")
  if(!(is.atomic(x) && is.atomic(y))) stop("Non-atomic args")
  if(anyNA(grp)) stop("NA vals not supported in `grp`")

  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.rle.c <- cumsum(grp.rle[['lengths']])
  x.ord <- x[ord]

  # NA and Inf handling. Checking inf makes this 5% slower, but
  # doesn't seem worth adding special handling for cases w/o Infs

  has.na <- anyNA(x)
  if(has.na) {
    na.x <- which(is.na(x.ord))
    x.ord[na.x] <- 0
  } else na.x <- integer()
  inf.x <- which(is.infinite(x.ord))
  any.inf <- length(inf.x) > 0
  if(any.inf) {
    inf.vals <- x.ord[inf.x]
    x.ord[inf.x] <- 0
  }
  x.grp.c <- cumsum(x.ord)[grp.rle.c]
  x.grp.c[-1L] <- x.grp.c[-1L] - x.grp.c[-length(x.grp.c)]

  # Re-inject NAs and Infs as needed

  if(any.inf) {
    inf.grps <- findInterval(inf.x, grp.rle.c, left.open=TRUE) + 1L
    inf.rle <- rle(inf.grps)
    inf.res <- rep(Inf, length(inf.rle[['lengths']]))
    inf.neg <- inf.vals < 0

    # If more than one Inf val in group, need to make sure we don't have
    # Infs of different signs as those add up to NaN
    if(any(inf.long <- (inf.rle[['lengths']] > 1L))) {
      inf.pos.g <- group_sum2(!inf.neg, inf.grps)
      inf.neg.g <- group_sum2(inf.neg, inf.grps)
      inf.res[inf.neg.g > 0] <- -Inf
      inf.res[inf.pos.g & inf.neg.g] <- NaN
    } else {
      inf.res[inf.neg] <- -Inf
    }
    x.grp.c[inf.rle[['values']]] <- inf.res
  }
  if(!na.rm && has.na)
    x.grp.c[findInterval(na.x, grp.rle.c, left.open=TRUE) + 1L] <- NA

  structure(x.grp.c, groups=grp.rle[['values']], n=grp.rle[['lengths']])
}
system.time(x.grpsum3 <- group_sum3(x, grp))
```
```
   user  system elapsed
  1.189   0.346   1.540
```
```{r echo=FALSE}
all.equal(x.grpsum, x.grpsum3)
```
```
[1] TRUE
```
```{r echo=FALSE, eval=FALSE}
## some tests
RNGversion("3.5.2"); set.seed(42)
g20 <- rep(c(1, 3, 8, 9), c(2, 3, 4, 1))
x20 <- runif(length(g2))
rbind(g20, x20)
o20 <- sample(length(g2))
oo20 <- order(o20)
g2 <- g20[o20]

xs <- replicate(4, x20[o20], simplify=FALSE)
xs[[1]][oo20][c(2, 6)] <- Inf
xs[[2]][oo20][c(2, 6)] <- NA
xs[[3]] <- xs[[2]]
xs[[4]][oo20][c(1, 2, 3, 4, 7, 8, 10)] <- c(Inf, -Inf, Inf, Inf, NA, Inf, -Inf)

mapply(
  function(x, g, na.rm) {
    all.equal(
      group_sum3(x, g, na.rm=na.rm),
      c(rowsum(x, g, na.rm=na.rm)),
      check.attributes=FALSE
    )
  },
  xs, list(g2), c(FALSE, FALSE, TRUE, FALSE)
)
```

[^knowledge-caveat]: Given how long it's taken me to find out about `rowsum` it
  is fair to question whether I would know whether there are many other
  functions of this kind out there or not.
[^vec-size]: Numeric vectors require 8 bytes per element plus some overhead for
  the object meta data.
[^int-vec]: R code that carries out looped operations over vectors in compiled
  code rather than in R-level code.
[^love-r-but]: I love R, but the madness around [text decoration
  conventions][1], or lack thereof is something that I could do without.  Sorry
  for the rant, but I was particularly triggered by this example.  I also do
  understand that in large collaborative project like this one, things like this
  are bound to happen and in the end it's not a big deal.
[^not-quite-sum]: `mean(x)` in R is not exactly equivalent to
  `sum(x) / length(x)` because `mean` has a two pass precision improvement
  algorithm.
[^forgive-me]: I am prone to absurd pedantic turns of phrase when giddy, so
  please forgive me.  I did mention that I am `rowsum` made me giddy, right?
[prior post][8]
[^slope-mod]: This is a slightly modified version of the original from [prior
  post][8] that is faster because it uses `mean.default` instead of `mean`.
[^timing-note]: Timings in the [prior post][8] are slower due to the use of
  `mean` instead of `mean.default` in the [`slope` function](#slope-ex).
[^dt-optim]: In order to unlock the full performance of `data.table` we need to
  engage in some manipulations.  We covered these in the [prior group statistics
  post][9].
[^prior-art]: I did some light research but did not find other
  obvious uses of this method.  Since this approach was not really practical
  until `data.table` radix sorting was added to base R in version 3.3.0, its
  plausible it is somewhat rare.  Send me links if there are other examples.
[^extended-precision]: The additional precision is only available internally;
  the return value is truncated to the regular double precision.
[^precision-digits]: Double precision is roughly `log10(2^53)`, or 15.95
  in decimal, but precision can really only be measured in binary "digits", so
  you cannot directly compute how much precision is lost by comparing two
  decimal numbers.  This particular subtraction loses us 21 bits of precision,
  or about 6.32 digits of decimal precision.
[^tech-sig]: This would be apparent if we inspected the bit representation of
  the `x_ux.cum*` values as they are only identical in the first 44 bits.
[^mag-diff]: Adding numbers with low precision but very different magnitudes
  can create a bit pattern in the double representation that suggests high
  precision, but that is just an artifact of the computation.  The actual
  precision is no higher than the precision of the highest magnitude number.
[^mean-pathological]: Incidentally, this implies that you will get less
  precise answers if your vector is [sorted by value][19] instead of even just
  randomly.
[^mag-prec]: There is still the general order of magnitude left, and since this
  is a base 2 order of magnitude you should still be at +-50% accuracy (i.e.
  value is between x and 2x).
[^prec-mismatch]: Precision mismatch can arise when adding or subtracting
  numbers different magnitudes, different precisions, or some combination of the
  two.  Only the most significant digits of the larger magnitude number that are
  either larger than the largest digit of the other number or overlap with a
  "precise" digit of the smaller number survive.  As always all these
  determinations should be made on binary digits, although it should hold
  roughly true for decimal digits.
[^for-effect]: We exaggerated the errors for expository purposes.

[1]: https://twitter.com/BrodieGaslam/status/976616435836510210
[2]: https://en.wikipedia.org/wiki/Hash_table
[3]: https://stackoverflow.com/a/11227902/2725969
[4]: https://www.extremetech.com/extreme/188776-how-l1-and-l2-cpu-caches-work-and-why-theyre-an-essential-part-of-modern-chips
[5]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/unique.c#L1514
[6]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/library/base/R/rowsum.R#L26
[7]: https://cran.r-project.org/web/packages/inline/index.html
[8]: /2019/02/24/a-strategy-for-faster-group-statisitics/#blood-from-a-turnip/
[9]: /2019/02/24/a-strategy-for-faster-group-statisitics/#optim-start/
[10]: https://github.com/JohnMount
[11]: https://github.com/WinVector/FastBaseR/blob/f4d4236/R/cumsum.R#L105
[12]: https://en.wikipedia.org/wiki/Double-precision_floating-point_format
[13]: https://github.com/wch/r-source/blob/R-3-5-branch/src/main/cum.c#L30
[14]: https://en.wikipedia.org/wiki/Extended_precision#Working_range
[15]: https://github.com/romainfrancois
[16]: https://github.com/ThinkR-open/seven31
[17]: https://github.com/wrathematics/float
[18]: https://github.com/wch/r-source/blob/R-3-3-branch/src/main/summary.c#L434
[19]: https://twitter.com/BrodieGaslam/status/1113783101262585856
