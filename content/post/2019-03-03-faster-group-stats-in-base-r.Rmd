---
title: Faster Group Stats in Base R
author: ~
date: '2019-03-03'
slug: faster-group-stats-in-base-r
categories: []
tags: []
image: /front-img/default.png
imagerect: ~
imagemrgvt: 0%
imagemrghz: 0%
draft: true
weight: 1
contenttype: article
description: Front page summary
---

```{r echo=FALSE}
options(digits=3)
library(ggplot2)
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
```

# blah blah

<!-- this needs to become a shortcode -->
<img
  id='front-img' src='/front-img/default.png'
  class='post-inset-image'
/>

# Baseline

As we saw in the prior
In one corner, the venerable `tapply`:

```{r eval=FALSE}
RNGversion("3.5.2"); set.seed(42)
n     <- 1e7
n.grp <- 1e6
grp   <- sample(n.grp, n, replace=TRUE)
x     <- runif(n)
y     <- runif(n)

system.time(x.ref <- c(tapply(x, grp, sum)))
```
```
   user  system elapsed
  8.539   0.321   9.030
```

In the other, `data.table`.

```{r}
library(data.table)
DT <- data.table(grp, x)
setDTthreads(1)
system.time(x.dt <- DT[, sum(x), keyby=grp][[2]])
```
```
   user  system elapsed
  0.982   0.059   1.048
```

We use one thread for more stable results, but also because for the final
computation we are planning on doing multi-threaded did not make a huge
difference on my system.  For this particular task multi threading can provide a
substantial boost to data table performance.

# It's All In the Capitalization

Our first solution uses `base::rowsum`.  As is clearly indicated by the lack of
capitalization and pluralization, `base::rowsum` is completely different from
`base::rowSums`.  Further, anyone who has read the "Semantics of Capitalization,
Punctuation, and Pluralization in R Function Names" memo can infer that the
former sums rows within groups, and the latter sums columns within rows.

You did get the memo, right?  Good.

Even though the function names are self explanatory, we will illustrate for
completeness:

```{r}
mx <- matrix(1:8, 4, byrow=TRUE) * 10 ^ (0:3)
mx
rowsum(mx, group=rep(c('odd', 'even'), 2))
```

`rowsum` preserved the two columns, but collapsed the rows by the `group` value.

```{r eval=FALSE}
rowSums(mx)       # note: output has been edited for clarity
```
```{r echo=FALSE}
matrix(rowSums(mx))
```

`rowSums` collapsed the columns but preserved the rows.  Normally `rowSums`
returns a vector, but here we display it as a one column matrix so the
relationship to the input matrix is clear.

I have known about `rowSums` for a long time, and I only more recently
discovered `rowsum`.  It probably says more about me than I should admit but I
was downright giddy when I realized _exactly_ what `rowsum` does.  After all,
there are many base R functions that compute on arbitrary groups, and many base
R functions that work directly in compiled code, but as far as I know base R
functions that compute on arbitrary groups in compiled code are
rare[^knowledge-caveat].  This is very useful:

<!--
I have known about rowSums for a long time, and I only more recently discovered rowsum. Imagine the excitement a taxidermist might feel on realizing they were just given the corpse of an albino fox, and not that of a white cat as they initially thought. That should capture the magnitude and significance of my excitement when I realized what rowsum does.

How can a base R function possibly compete for excitement with a dead fox? Well, there are many base R functions that compute on arbitrary groups, and many base R functions that work directly in compiled code, but as far as I know base R functions that compute on arbitrary groups in compiled code are rare1. This is very useful:
-->
```{r eval=FALSE}
system.time(x.rs <- rowsum(x, grp))
```
```
   user  system elapsed
  2.505   0.096   2.626
```
```{r eval=FALSE}
all.equal(c(x.ref), c(x.rs), check.attributes=FALSE)
```
```
[1] TRUE
```

Almost four times faster than the original `tapply` solution, and not far from
the `data.table` one.

```{r echo=FALSE}
funs <- c('tapply', 'rowsum', 'data.table')
times <- data.frame(
  Function=factor(funs, levels=funs),
  time=c(9.030, 2.626, 1.126)
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col() +
  # facet_wrap(~Function) +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

I've wondered if I am alone in my delayed awareness of `rowsum`, but my attempts
to measure the relative popularity of the two functions via search engine were
fruitless.  Given the glaring semantic cues in `rowsum` vs `rowSums` I would
have thought they would do better...  Thankfully years of using "R" as a search
term have inured me to this type of search disappointment.

# Pixie Dust

Sorting values prior to running `unique` on them makes `unique` faster:

```{r eval=FALSE}
grp.o <- grp[order(grp)]
system.time(unique(grp.o))
```
```
   user  system elapsed
  0.453   0.057   0.514
```
```{r eval=FALSE}
system.time(unique(grp))
```
```
   user  system elapsed
  1.354   0.067   1.436
```

Internally `unique` uses a [hash table][2] to detect duplicate values, so it may
seem odd that the lookup speed is affected by ordering as the hashing time
should not be affected by the input order.  Most likely it is some combination
of caching and [branch prediction][3].  In the sorted version, we will look up
the same key repeatedly, which means the corresponding value will be warm and
snug in the [highest speed cache][4].  Additionally, since all duplicated values
are in sequence, the processor will do a pretty good job predicting branches
just by assuming the next value will be duplicated.  It will get it wrong at
each transition to a new value, but it will be right more often than not if
there is sufficient duplication.  I have no idea if this is actually what is
going on or not, but it seems likely, and I'm tickled pink by the thought that
I'm noticing these effects from R-level for the first time.

Now, for why I'm ranting about this, it turns out that the combined operation of
sorting and `unique`ing is faster than the just `unique`ing:

```{r eval=FALSE}
system.time(unique(grp[order(grp)]))
```
```
   user  system elapsed
  0.909   0.158   1.081
```

This also has a big impact on `rowsum`, as it uses both [`unique`][6] on top of
its own [hash table][5]:<span id=rowsum-ordered></span>

```{r eval=FALSE}
system.time({
  o <- order(grp)
  x.rs2 <- rowsum(x[o], grp[o], reorder=FALSE)
})
```
```
   user  system elapsed
  1.283   0.105   1.430
```

This puts us in striking distance of single-threaded `data.table`:

```{r echo=FALSE}
times <- data.frame(
  Function=c('tapply', 'rowsum', 'rowsum-sorted', 'data.table'),
  time=c(9.030, 2.626, 1.430, 1.048)
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col() +
  # facet_wrap(~Function) +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

Now, you might be thinking, "oh, so maybe `data.table` isn't so special after
all".  Well, the only reason this `order` pixie dust business is even possible
is because `order` is so fast, and `order` is so fast because `data.table`
contributed their pixie dust to R:

<div class=>
```{r echo=FALSE}
blogdown::shortcode('tweet', '1106231241488154626')
```
</div>

<span id=alt-algo></span>
One last thing before we move on: knowing that the data is sorted opens up
different algorithms for many tasks such as `unique` and `rowsum`.  To
illustrate I implemented the alternate [algorithms in C in the
appendix](#sorting-versions).  We wrap those in R functions that order
the data first, and to distinguish them from their base counterparts we
pluralize them.  The trailing "s" indicate these are the sorting versions of the
functions, or maybe that they are the speedy versions.  My dog ate the memo so I
don't know anymore[^love-r-but]...

Here is what happens with `uniques`:<span id=unique-c></span>

```{r eval=FALSE}
uniques <- function(x, res.sorted=FALSE) {
  o <- order(x)
  xo <- x[o]
  i <- .uniques(xo)          # C code: see appendix
  if(!res.sorted) xo[i][order(o[i])] else xo[i]
}
system.time(uniques(grp))
```
```
   user  system elapsed
  0.587   0.018   0.615
```

This is almost twice as fast as the original version with the sorted data, and
it could be made faster if we were okay with getting results back sorted.  Most
of the time is actually spent in the `o <- order(x)` step, so the actually
unique-ing is an order of magnitude faster!

Similar story for `rowsums`:

```{r eval=FALSE}
rowsums <- function(x, group) {
  o <- order(group)
  .rowsums(x[o], group[o])    # C code: see appendix
}
system.time(rowsums(x, grp))
```
```
   user  system elapsed
  0.820   0.022   0.846
```

The implementations are not equivalent, but for these specific inputs they
should be comparable.  Your mileage will vary depending on the degree of
uniqueness of the data.  More details [in the appendix](#sorting-versions).

The main point I'm trying to make here is that it is a **big deal** that `order`
can order fast enough that we can switch the algorithms we use downstream and
get an even bigger performance improvement.  A big thank you to team
`data.table` for sharing the pixie dust.

# So You Think You Can Group-Stat?

Okay, great, we can sum quickly in base R.  One measly stat.  What good is that
if we want to compute something more complex like the slope of a bivariate
regression, as we did in our [prior post][8]?  As a refresher this is what the
calculation looks like:

$$\frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i -
\bar{x})^{2}}$$

The R equivalent is[^slope-mod]:<span id='slope-ex'></a>

```{r eval=FALSE}
slope <- function(x, y) {
  x_ux <- x - mean.default(x)
  y_uy <- y - mean.default(y)
  sum(x_ux * y_uy) / sum(x_ux ^ 2)
}
```

We can see that `sum` shows up explicitly, and somewhat implicitly via
`mean`[^not-quite-sum].  There are many statistics that essentially boil down to
adding things together, so we can use `rowsum` as the wedge to breach the
heretofore impenetrable barrier to fast grouped statistics in R[^forgive-me]:
<span id=group-slope-rs></span>

```{r eval=FALSE}
group_slope <- function(x, y, grp) {
  ## order inputs by group
  o <- order(grp)
  go <- grp[o]
  xo <- x[o]
  yo <- y[o]

  ## group sizes and group indices
  grle <- rle(go)
  gn <- grle[['lengths']]
  gi <- rep(seq_along(gn), gn)

  ## compute mean(x) and mean(y), and recycle them
  ## to each element of `x` and `y`:
  sx <- rowsum(xo, go)     # sum(x)
  ux <- (sx/gn)[gi]        # mean(x), recycled to original vec length
  sy <- rowsum(yo, go)
  uy <- (sy/gn)[gi]

  ## (x - mean(x)) and (y - mean(y))
  x_ux <- xo - ux
  y_uy <- yo - uy

  ## Slopes!
  setNames(rowsum(x_ux * y_uy, go) / rowsum(x_ux ^ 2, go), grle[['values']])
}
```

The non-obvious steps involve `gn` and `gi`:

```{r eval=FALSE}
grle <- rle(go)
gn <- grle[['lengths']]
gi <- rep(seq_along(gn), gn)
```

`base::rle` computes "Run Lengths" a.k.a. the length of sequences of repeated
values.  With our ordered groups this corresponds to the size of the groups.
We can illustrate in simplified form:

```{r}
(xo <- 2:6)                              # some values
(go <- c(3, 3, 5, 5, 5))                 # their groups
(gn <- rle(go)[['lengths']])             # the size of the groups
```

`gn` tells us how many elements there are in each group.  This allows us to
compute the `$\bar{x}$` values:

```{r}
(sx <- rowsum(xo, go))                   # sum of each group
(ux <- sx / gn)                          # mean of each group
```

But we need to compute `$x - \bar{x}$`, which means we need to recycle each
group's `$\bar{x}$` value for each `$x$`.  This is what `gi` does:

```{r}
(gi <- rep(seq_along(gn), gn))
cbind(x=xo, ux=ux[gi], g=go)
```

For each original `$x$` value, we have associated the corresponding `$\bar{x}$`
value.  In `group_slope` we combined the mean calculation and recycling into one
step: `xu <- (xs/gn)[gi]`.

This is pretty fast with our original 10MM record data set:

```{r eval=FALSE}
system.time(slope.rs <- group_slope(x, y, grp))
```
```
   user  system elapsed
  4.110   0.785   4.915
```

Compare to the `vapply` version from the [prior grouped statistics post][8]:

```{r eval=FALSE}
system.time({
  id <- seq_along(grp)
  id.split <- split(id, grp)
  slope.ply <- vapply(id.split, function(id) slope(x[id], y[id]), 0)
})
```
```
   user  system elapsed
 13.817   0.216  14.184
```
```{r}
all.equal(slope.ply, c(slope.rs), check.attributes=FALSE)
```
```
[1] TRUE
```

And a summary of all the timings including `data.table`:

```{r rowsum-timings-all, echo=FALSE}
funs <- c('*pply', '*pply', 'data.table', 'data.table', 'rowsum')
times <- data.frame(
  Function=factor(funs, levels=unique(funs)),
  Version=c('normal', 'ordered', 'normal', 'optim', 'normal'),
  time=c(14.184, 9.867 , 7.959, 3.697, 4.915)
)
ggplot(times, aes(x=Version, y=time)) +
  geom_col() +
  facet_grid(.~Function, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

`rowsum` is ~3x faster than even basic `*pply` version, and faster than the
unoptimized `data.table` [^timing-note].  But `data.table` keeps the performance
crown, albeit with a little help[^dt-optim].  More details for the benchmarks
are [in the appendix](#other-benchmarks).

So like Rocky Balboa we walk off into the sunset, without the title but
satisfied we put up a good fight.  Except...

# Round II

Remember how we saw earlier that a fast order opened up opportunities for [new
algorithms](#alt-algo)?  [John Mount][10], shows how we can compute group sums
[using `cumsum`][11] on group-ordered data[^prior-art].  With a little work we
can generalize it.

The concept is simple: order by group, compute cumulative sum, pull out the last
value for each group, and take their differences.  Visually:

```{r cumsum-ex, echo=FALSE}
library(ggbg)
RNGversion("3.5.2"); set.seed(42)
n1 <- 7
x1 <- seq_len(n1)
y1 <- runif(n1);
colors <- c('#3333ee', '#33ee33', '#eeee33')
g1 <- sample(1:3, n1, replace=TRUE)
steps <- c(
  '1 - Start', '2 - Sort By Group', '3 - Cumulative Sum',
  '4 - Last Value in Group', '5 - Take Differences', '6 - Group Sums!'
)
steps <- factor(steps, levels=steps)
df1 <- data.frame(
  x1, y1, g1=as.character(g1), step=steps[[1]], stringsAsFactors=FALSE
)
df2 <- df1[order(g1),]
df2[['x1']] <- x1
df2[['step']] <- steps[[2]]
df3 <- df2
df3 <- transform(
  df3, yc=cumsum(y1), step=steps[[3]],
  last=c(head(g1, -1) != tail(g1, -1), TRUE)
)
df4 <- transform(df3, step=steps[[4]])
df5 <- transform(
  subset(df4, last), x1=3:5, y1=c(yc[[1]], diff(yc)), step=steps[[5]]
)
df6 <- transform(df5, step=steps[[6]])

ggplot(mapping=aes(x=x1, y=y1, fill=g1)) +
  geom_col(data=df1) +
  geom_col(data=df2) +
  geom_col(data=df3, mapping=aes(y=yc)) +
  geom_col(
    data=df4, mapping=aes(y=yc, alpha=I(ifelse(last, 1, .15))),
    width=0.9
  ) +
  geom_col(data=df5, width=0.9, position='waterfall') +
  geom_col(data=df6, width=0.9) +
  facet_wrap(~step, ncol=2) +
  ylab(NULL) + xlab(NULL) +
  theme(
    axis.text.x=element_blank(), axis.text.y=element_blank(),
    panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
    axis.ticks=element_blank()
  ) +
  scale_fill_manual(values=setNames(colors, 1:3), guide=FALSE)
```

This is the data we used for the visualization:

```{r echo=FALSE}
x1 <- y1 # a bit confusing with x/y for ggplot
```
```{r}
g1
x1
```

The first three steps are obvious:

```{r}
ord <- order(g1)
go <- g1[ord]
xo <- x1[ord]
xc <- cumsum(xo)
```

Picking the last value from each group is a little harder, but we can do so with
the help of `base::rle`.  `rle` returns the lengths of repeated-value
sequences within a vector.  In a vector of ordered group ids, we can use it to
compute the lengths of each group:

```{r}
go
(grle <- rle(go))
(gn <- grle[['lengths']])
```

This tells us the first group has two elements, the second also two, and the
last three.  We can translate this into indices of the original vector with
`cumsum`, and use it to pull out the relevant values from the cumulative sum of
the `x` values:

```{r}
(gnc <- cumsum(gn))
(xc.last <- xc[gnc])
```

To finish we just take the differences:

```{r}
diff(c(0, xc.last))
```

I wrapped the whole thing into the [`group_sum` function](#group_sum) you can
see in the appendix:

```{r "group_sum-def", echo=FALSE}
group_sum <- function(x, grp) {
  ## Order groups and values
  ord <- order(grp)
  go <- grp[ord]
  xo <- x[ord]

  ## Last values
  grle <- rle(go)
  gnc <- cumsum(grle[['lengths']])
  xc <- cumsum(xo)
  xc.last <- xc[gnc]

  ## Take diffs and return
  gs <- diff(c(0, xc.last))
  setNames(gs, grle[['values']])
}
```
```{r}
group_sum(x1, g1)
```

Every step of `group_sum` is internally vectorized[^int-vec], so the function is
fast.  We demonstrate here with the original 10MM data set:

```{r eval=FALSE}
system.time(x.grpsum <- group_sum(x, grp))
```
```
   user  system elapsed
  1.098   0.244   1.344
```
```{r eval=FALSE}
all.equal(x.grpsum, c(x.ref), check.attributes=FALSE)
```
```
[1] TRUE
```

This is slightly faster than [pre-ordered `rowsum`](#rowsum-ordered)
calculation, but as we will see shortly it has the additional advantage that we
can capture and re-use intermediate steps.  I wrote a [variation](#cumulative
group-sum-with-na-and-inf) that handles `NAs` and `Inf` values to show it can be
done, but we will ignore than wrinkle going forward for clarity's sake.

Now for the real fun: let's re-implement the [initial `rowsum` group slope
function](#group-slope-rs) with this method:

```{r eval=FALSE}
.group_sum_int <- function(x, last.in.group) {
  xgc <- cumsum(x)[last.in.group]
  diff(c(0, xgc))
}
group_slope2 <- function(x, y, grp) {
  ## order inputs by group
  o <- order(grp)
  go <- grp[o]
  xo <- x[o]
  yo <- y[o]

  ## group sizes and group indices
  grle <- rle(go)
  gn <- grle[['lengths']]
  gi <- rep(seq_along(gn), gn)
  gnc <- cumsum(gn)    # Last index in each group

  ## compute mean(x) and mean(y), and recycle them
  ## to each element of `x` and `y`:
  sx <- .group_sum_int(xo, gnc)
  ux <- (sx/gn)[gi]
  sy <- .group_sum_int(yo, gnc)
  uy <- (sy/gn)[gi]

  ## (x - mean(x)) and (y - mean(y))
  x_ux <- xo - ux
  y_uy <- yo - uy

  ## Slopes!
  x_ux.y_uy <- .group_sum_int(x_ux * y_uy, gnc)
  x_ux2 <- .group_sum_int(x_ux ^ 2, gnc)
  setNames(x_ux.y_uy / x_ux2, grle[['values']])
}
system.time(slope.gs <- group_slope2(x, y, grp))
```
```
   user  system elapsed
  1.952   0.605   2.573
```

Oh snap, 40% faster than `data.table`!

```{r slope-timings-2, echo=FALSE}
funs <- c('*pply', '*pply', 'data.table', 'data.table', 'rowsum', 'cumsum')
times <- data.frame(
  Function=factor(funs, levels=unique(funs)),
  Version=c('normal', 'ordered', 'normal', 'optim', 'normal', 'normal'),
  time=c(14.184, 9.867 , 7.959, 3.697, 4.915, 2.573)
)
ggplot(times, aes(x=Version, y=time)) +
  geom_col() +
  facet_grid(.~Function, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

The key advantage `group_slope2` has over the `rowsum` version is that it can
re-use `gnc`, the vector of indices to the last value in each group.
Computing `gnc` is the expensive part of the `cumsum` group sum calculation:

```{r eval=FALSE}
o <- order(grp)
go <- grp[o]
xo <- x[o]
gn <- rle(go)[['lengths']]
gi <- rep(seq_along(gn), gn)
gnc <- cumsum(gn)

system.time(.group_sum_int(xo, gnc))
```
```
   user  system elapsed
  0.042   0.008   0.050
```
```{r eval=FALSE}
system.time(group_sum(xo, go))
```
```
   user  system elapsed
  0.567   0.055   0.622
```

This is an order of magnitude difference for the grouped sum step, which we four
times when computing the slope.

# Precision Schmecision

The skeptical reader might have noticed that we did not compare the results of
`group_slope2` with reference values.  It turns out that our blazing fast
benchmark hero is cutting some corners:

```{r}
all.equal(slope.gs, c(slope.rs), check.attributes=FALSE)
```
```
[1] "Mean relative difference: 0.001404195"
```
With a generous tolerance we find equality:
```{r}
all.equal(slope.gs, c(slope.rs), check.attributes=FALSE, tolerance=2e-3)
```
```
[1] TRUE
```

Let's find the worst offender:

```{r eval=FALSE}
bad.el <- which.max(abs(slope.gs / slope.rs - 1))
slope.gs[bad.el]
```
```
   616826 
-2511.833 
```
```{r eval=FALSE}
slope.rs[bad.el]
```
```
   616826 
-2977.281 
```

Ugh, that's pretty bad.  These are the values responsible:

```{r eval=FALSE}
bad.grp <- as.integer(names(slope.gs[bad.el]))
(bad.x <- x[grp == mxgr])
```
```
[1] 0.4239786 0.4239543
```
```{r eval=FALSE}
(bad.y <- y[grp == mxgr])
```
```
[1] 0.7647899 0.8370645
```

The `x` values are almost certainly the cause of the problem, as `$x
- \bar{x}$` produces some small numbers.

```{r eval=FALSE}
bad.x - mean(bad.x)
```
```
[1]  0.00001213769 -0.00001213769
```

But these are well within the range of values that should be manageable
for [double precision floats][12] which are what R "numeric" values are stored
as.  So what's going on?  It turns out that the `cumsum` approach severely
amplifies precision issues.  Typical double precision floating point numbers
have about 15-16 digits of precision.  In order to mitigate precision issues, [R
internally uses a representation][13] that allows [18-19 digits of
precision][14] for `cumsum`.  Yet that is not enough here.  To see why we need
to review our algorithm:

```{r cumsum-review, echo=FALSE}
ggplot(mapping=aes(x=x1, y=y1, fill=g1)) +
  geom_col(
    data=df4, mapping=aes(y=yc, alpha=I(ifelse(last, 1, .15))),
    width=0.9
  ) +
  geom_col(data=df5, width=0.9, position='waterfall') +
  facet_wrap(~step, ncol=2) +
  ylab(NULL) + xlab(NULL) +
  theme(
    axis.text.x=element_blank(), axis.text.y=element_blank(),
    panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
    axis.ticks=element_blank()
  ) +
  scale_fill_manual(values=setNames(colors, 1:3), guide=FALSE)

```


```
.group_sum2 <- function(x, last.in.group, precise=FALSE){
  x.grp <- sum_grp1_int(x, last.in.group)
  if(precise) {
    x[last.in.group] <- x[last.in.group] - x.grp
    x.grp + sum_grp1_int(x, last.in.group)
  } else x.grp
}
```


But we can make it precise
```{r eval=FALSE}
system.time(slope1(x, y, grp, precise=TRUE))
```
```
   user  system elapsed
  2.481   1.087   3.754
  # second faster timing was from a fresh session, repeated several times
   user  system elapsed
  2.399   0.933   3.348
```

```{r eval=FALSE}
sum_g3 <- function(x, grp, na.rm=TRUE) {
  ord <- order(grp)
  id.ord <- id[ord]
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  max.grp <- max(grp.rle[['lengths']])

  # this NA handling doesn't work b/c for na.rm=FALSE you still get NAs

  res <- matrix(NA_real_, ncol=length(grp.rle[['lengths']]), nrow=max.grp)

  # each group that isn't as long as the longest group needs padding

  rle.len <- grp.rle[['lengths']]
  grp.pad <- max.grp - rle.len
  id.raw <- rep(1L, length(x))
  id.raw[(cumsum(rle.len) + 1L)[-length(rle.len)]] <-
    grp.pad[-length(rle.len)] + 1L
  id <- cumsum(id.raw)

  res[id] <- x[ord]
  structure(colSums(res, na.rm=na.rm), groups=grp.rle[['values']])
}
system.time(sum_g3(x, grp))
```
```
   user  system elapsed
  1.186   0.374   1.634
```
```{r eval=FALSE}
# lens: how long each group is
# maxlen: longest group
sum_grp2 <- function(x, lens, maxlen, mode='sum') {

  res <- matrix(NA_real_, ncol=length(lens), nrow=maxlen)

  # Generate indices that will map to the correct spots in `res` from `x`,
  # which means add whatever padding we need to the index value for the next
  # column

  len_1 <- lens[-length(lens)]
  grp.pad <- (maxlen + 1L) - len_1
  id.raw <- rep(1L, length(x))
  len_1[1L] <- len_1[1L] + 1L
  id.raw[cumsum(len_1)] <- grp.pad
  id <- cumsum(id.raw)

  # Inject the x values according to these indices that should place each gropu
  # in a column

  res[id] <- x

  if(identical(mode, 'sum')) colSums(res, na.rm=TRUE)
  else if(identical(mode, 'mean')) colMeans(res, na.rm=TRUE)
  else stop("Invalid mode")
}
slope2 <- function(x, y, grp) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.len <- grp.rle[['lengths']]
  max.grp <- max(grp.len)

  xo <- x[ord]
  xi <- rep(seq_along(grp.len), grp.len)
  xu <- sum_grp2(xo, grp.len, max.grp, 'mean')
  x_ux <- xo - xu[xi]

  yo <- y[ord]
  yu <- sum_grp2(yo, grp.len, max.grp, 'mean')
  y_uy <- yo - yu[xi]

  x_ux.y_uy <- sum_grp2(x_ux * y_uy, grp.len, max.grp, 'sum')
  x_ux2 <- sum_grp2(x_ux ^ 2, grp.len, max.grp, 'sum')

  x_ux.y_uy / x_ux2
}
slope2a_int <- function(xo, yo, lens) {
  max.grp <- lens[1]

  xi <- rep(seq_along(lens), lens)
  xu <- sum_grp2(xo, lens, max.grp, 'mean')
  x_ux <- xo - xu[xi]

  yu <- sum_grp2(yo, lens, max.grp, 'mean')
  y_uy <- yo - yu[xi]

  x_ux.y_uy <- sum_grp2(x_ux * y_uy, lens, max.grp, 'sum')
  x_ux2 <- sum_grp2(x_ux ^ 2, lens, max.grp, 'sum')

  x_ux.y_uy / x_ux2
}
slope2a <- function(x, y, grp, splits=5) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.len <- grp.rle[['lengths']]

  ord2 <- order(rep(grp.len, grp.len), decreasing=TRUE)
  ordg <- order(grp.len, decreasing=TRUE)
  grp.len.o <- grp.len[ordg]
  len.max <- grp.len.o[1L]  # will break if no groups
  len.min <- grp.len.o[length(grp.len.o)]

  # order the inputs

  ord3 <- ord[ord2]
  xo <- x[ord3]
  yo <- y[ord3]
  go <- grp.ord[ord2]

  # simple initial cut, just cut into equal splits

  cuts <- as.integer(
    round(seq(1L, length(grp.len) + 1L, length.out=splits + 1L))
  )
  grp.len.o.c <- cumsum(c(1L, grp.len.o))
  res <- vector("list", splits)

  for(i in seq_len(splits)){
    # Figure out that starting and ending elements for each group

    start.g <- cuts[i]
    end.g <- cuts[i + 1L]

    start <- grp.len.o.c[start.g]
    end <- grp.len.o.c[end.g] - 1L

    idx.g <- start.g:(end.g - 1L)
    idx <- start:end

    res[[i]] <- slope2a_int(xo[idx], yo[idx], grp.len.o[idx.g])
  }
  # Reorder back in ascending group order instead of group size order

  res.fin <- numeric(length(grp.len))
  res.fin[ordg] <- unlist(res)
  res.fin
}
system.time(slope2(x, y, grp))
RNGversion("3.5.2"); set.seed(42)
x2 <- runif(100)
y2 <- runif(100)
g2 <- sample(1:10, 100, rep=T)
```
```
   user  system elapsed
  2.827   0.932   3.807
  # can't reproduce the earlier timings...
   user  system elapsed
  3.185   1.285   4.783
  # now I can ...
```
```{r eval=FALSE}
system.time(slope2a(x, y, grp))
```
```
   user  system elapsed
  2.998   1.055   4.082
```
```{r eval=FALSE}
dummy <- function(sizes) {
  res <- vector("list", length(sizes))
  for(i in seq_along(sizes)) {
    mx <- matrix(numeric(), nrow=sizes[i], ncol=as.integer(1e6 / length(sizes)))
    res[[i]] <- colSums(mx, na.rm=TRUE)
  }
}
system.time(dummy(c(28, 21, 14, 7, 1)))
```

```{r eval=FALSE}

DT <- copy(DT.raw)
system.time(res.ref <- DT[, sum(x), keyby=grp][['V1']])
#   user  system elapsed
#  1.071   0.134   1.216
system.time(res <- sum_g2(x, grp))
#   user  system elapsed
#  1.286   0.309   1.692
all.equal(res, res.ref, check.attributes=FALSE) # TRUE

system.time(res2 <- sum_g2(x, grp))
system.time(res3 <- sum_g3(x, grp))
system.time(res4 <- rowsum(x, grp))

sum_winvector <- function(DF) {
  odata <- DF[order(DF$grp),,drop=FALSE]
  first_indices <- mark_first_in_each_group(odata, "grp")
  sum_g(odata[['x']], first_indices)
}
```
```
  user  system elapsed
 1.810   0.740   2.651
```
Note on vector size[^vec-size].


# Conclusions

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id='feedback-cont'></div>

# Appendix

## Acknowledgments

* Oleg Sklyar, Duncan Murdoch, Mike Smith, Dirk Eddelbuettel, Romain Francois,
  Karline Soetaert for [`inline`][7].

## Sorting Versions

I quickly threw together C functions that `unique` and `rowsum` on sorted data.
They support a very narrow set of inputs, but for those inputs the timing
comparisons relative to the base equivalents should be fair.  `.rowsums` does
benefit from being able to assume the input will be a vector and not a matrix,
but that should be a small difference.

I tried two variations from the one we looked at in the [main body](#unique-c)
of the post that generally support the timings against the base functions with
input sorting:

* 1e8 vector with ~10 sized groups
* 1e7 vector with ~1 sized groups

`rowsums` preserves the ~2x advantage in both scenarios.  `uniques` maintains
the advantage for the first scenario, but in the second is at parity.  This is
because for `uniques` we need to unsort the result and when there are no
duplicates the vector to unsort is large.  If we were allowed to return the
results sorted then `uniques` is faster, but by a smaller margin than with the
original example.

```{r eval=FALSE}
.uniques <- inline::cfunction(sig=c(x='integer'), body="
  R_xlen_t len, i, len_u = 1;
  SEXP res;
  int *xi = INTEGER(x);
  len = XLENGTH(x);

  if(len > 1) {
    // count uniques
    for(i = 1; i < len; ++i) {
      if(xi[i - 1] != xi[i]) {
        ++len_u;
    } }
    // allocate and record uniques
    res = PROTECT(allocVector(INTSXP, len_u));
    int *resi = INTEGER(res);
    *resi = 1;
    R_xlen_t j = 1;
    for(i = 1; i < len; ++i) {
      if(xi[i - 1] != xi[i]) {
        resi[j++] = i + 1;  // 1 based indexing
    } }
    UNPROTECT(1);
  } else {
    res = x;
  }
  return res;
")

.rowsums <- inline::cfunction(
  sig=c(x='numeric', g='integer'),
  body="
  R_xlen_t len, i, len_u = 1;
  SEXP res, res_x, res_g;
  int *gi = INTEGER(g);
  double *xi = REAL(x);
  len = XLENGTH(g);
  if(len != XLENGTH(x)) error(\"Unequal Length Vectors\");
  res = PROTECT(allocVector(VECSXP, 2));

  if(len > 1) {
    // count uniques
    for(i = 1; i < len; ++i) {
      if(gi[i - 1] != gi[i]) {
        ++len_u;
    } }
    // allocate and record uniques
    res_x = PROTECT(allocVector(REALSXP, len_u));
    res_g = PROTECT(allocVector(INTSXP, len_u));

    double *res_xi = REAL(res_x);
    int *res_gi = INTEGER(res_g);
    R_xlen_t j = 0;

    res_xi[0] = 0;
    for(i = 1; i < len; ++i) {
      res_xi[j] += xi[i - 1]; // we don't check for double overflow...
      if(gi[i - 1] != gi[i]) {
        res_gi[j] = gi[i - 1];
        ++j;
        res_xi[j] = 0;
    } }
    res_xi[j] += xi[i - 1];
    res_gi[j] = gi[i - 1];

    SET_VECTOR_ELT(res, 0, res_x);
    SET_VECTOR_ELT(res, 1, res_g);
    UNPROTECT(2);
  } else {
    // Don't seem to need to duplicate x/g
    SET_VECTOR_ELT(res, 0, x);
    SET_VECTOR_ELT(res, 1, g);
  }
  UNPROTECT(1);
  return res;
")
```

## Other Benchmarks


Ordered version:

```{r eval=FALSE}
system.time({
  o <- order(grp)
  go <- grp[o]
  id <- seq_along(grp)[o]
  id.split <- split(id, go)
  slope.ply <- vapply(id.split, function(id) slope(x[id], y[id]), 0)
})
```
```
   user  system elapsed
  9.565   0.203   9.867
```
```{r eval=FALSE}
all.equal(slope.ply, c(slope.rs), check.attributes=FALSE)
```
```
[1] TRUE
```

`data.table`, not optimized:

```{r eval=FALSE}
setDTthreads(1)
DT <- data.table(grp, x, y)
system.time(DT[, slope(x, y), grp])
```
```
   user  system elapsed
  7.798   0.082   7.959
```

## Function Definitions

### group_sum

```{r group_sum-def, eval=FALSE}
```

### Cumulative Group Sum With NA and Inf

A correct implementation of the single pass `cumsum` based `group_sum` requires
a bit of work to correctly handle both `NA` and `Inf` values.  Both of these
need to be pulled out of the data ahead of the cumulative step otherwise they
would wreck all subsequent calculations.  The rub is they need to be re-injected
into the results, and with `Inf` we need to account for groups computing to
`Inf`, `-Inf`, and even `NaN`.

I implemented a version of `group_sum` that handles these for illustrative
purposes.  It is lightly tested so you should not consider it to be generally
robust.

```{r eval=FALSE}
group_sum3 <- function(x, grp, na.rm=FALSE) {
  if(length(x) != length(grp)) stop("Unequal length args")
  if(!(is.atomic(x) && is.atomic(y))) stop("Non-atomic args")
  if(anyNA(grp)) stop("NA vals not supported in `grp`")

  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.rle.c <- cumsum(grp.rle[['lengths']])
  x.ord <- x[ord]

  # NA and Inf handling. Checking inf makes this 5% slower, but
  # doesn't seem worth adding special handling for cases w/o Infs

  has.na <- anyNA(x)
  if(has.na) {
    na.x <- which(is.na(x.ord))
    x.ord[na.x] <- 0
  } else na.x <- integer()
  inf.x <- which(is.infinite(x.ord))
  any.inf <- length(inf.x) > 0
  if(any.inf) {
    inf.vals <- x.ord[inf.x]
    x.ord[inf.x] <- 0
  }
  x.grp.c <- cumsum(x.ord)[grp.rle.c]
  x.grp.c[-1L] <- x.grp.c[-1L] - x.grp.c[-length(x.grp.c)]

  # Re-inject NAs and Infs as needed

  if(any.inf) {
    inf.grps <- findInterval(inf.x, grp.rle.c, left.open=TRUE) + 1L
    inf.rle <- rle(inf.grps)
    inf.res <- rep(Inf, length(inf.rle[['lengths']]))
    inf.neg <- inf.vals < 0

    # If more than one Inf val in group, need to make sure we don't have
    # Infs of different signs as those add up to NaN
    if(any(inf.long <- (inf.rle[['lengths']] > 1L))) {
      inf.pos.g <- group_sum2(!inf.neg, inf.grps)
      inf.neg.g <- group_sum2(inf.neg, inf.grps)
      inf.res[inf.neg.g > 0] <- -Inf
      inf.res[inf.pos.g & inf.neg.g] <- NaN
    } else {
      inf.res[inf.neg] <- -Inf
    }
    x.grp.c[inf.rle[['values']]] <- inf.res
  }
  if(!na.rm && has.na)
    x.grp.c[findInterval(na.x, grp.rle.c, left.open=TRUE) + 1L] <- NA

  structure(x.grp.c, groups=grp.rle[['values']], n=grp.rle[['lengths']])
}
system.time(x.grpsum3 <- group_sum3(x, grp))
```
```
   user  system elapsed
  1.189   0.346   1.540
```
```{r echo=FALSE}
all.equal(x.grpsum, x.grpsum3)
```
```
[1] TRUE
```
```{r echo=FALSE, eval=FALSE}
## some tests
RNGversion("3.5.2"); set.seed(42)
g20 <- rep(c(1, 3, 8, 9), c(2, 3, 4, 1))
x20 <- runif(length(g2))
rbind(g20, x20)
o20 <- sample(length(g2))
oo20 <- order(o20)
g2 <- g20[o20]

xs <- replicate(4, x20[o20], simplify=FALSE)
xs[[1]][oo20][c(2, 6)] <- Inf
xs[[2]][oo20][c(2, 6)] <- NA
xs[[3]] <- xs[[2]]
xs[[4]][oo20][c(1, 2, 3, 4, 7, 8, 10)] <- c(Inf, -Inf, Inf, Inf, NA, Inf, -Inf)

mapply(
  function(x, g, na.rm) {
    all.equal(
      group_sum3(x, g, na.rm=na.rm),
      c(rowsum(x, g, na.rm=na.rm)),
      check.attributes=FALSE
    )
  },
  xs, list(g2), c(FALSE, FALSE, TRUE, FALSE)
)
```

[^knowledge-caveat]: Given how long it's taken me to find out about `rowsum` it
  is fair to question whether I would know whether there are many other
  functions of this kind out there or not.
[^vec-size]: Numeric vectors require 8 bytes per element plus some overhead for
  the object meta data.
[^int-vec]: R code that carries out looped operations over vectors in compiled
  code rather than in R-level code.
[^love-r-but]: I love R, but the madness around [text decoration
  conventions][1], or lack thereof is something that I could do without.  Sorry
  for the rant, but I was particularly triggered by this example.  I also do
  understand that in large collaborative project like this one, things like this
  are bound to happen and in the end it's not a big deal.
[^not-quite-sum]: `mean(x)` in R is not exactly equivalent to
  `sum(x) / length(x)` because `mean` has a two pass precision improvement
  algorithm.
[^forgive-me]: I am prone to absurd pedantic turns of phrase when giddy, so
  please forgive me.  I did mention that I am `rowsum` made me giddy, right?
[prior post][8]
[^slope-mod]: This is a slightly modified version of the original from [prior
  post][8] that is faster because it uses `mean.default` instead of `mean`.
[^timing-note]: Timings in the [prior post][8] are slower due to the use of
  `mean` instead of `mean.default` in the [`slope` function](#slope-ex).
[^dt-optim]: In order to unlock the full performance of `data.table` we need to
  engage in some manipulations.  We covered these in the [prior group statistics
  post][9].
[^prior-art]: I did some light research but did not find other
  obvious uses of this method.  Since this approach was not really practical
  until `data.table` radix sorting was added to base R in version 3.3.0, its
  plausible it is somewhat rare.  Send me links if there are other examples.

[1]: https://twitter.com/BrodieGaslam/status/976616435836510210
[2]: https://en.wikipedia.org/wiki/Hash_table
[3]: https://stackoverflow.com/a/11227902/2725969
[4]: https://www.extremetech.com/extreme/188776-how-l1-and-l2-cpu-caches-work-and-why-theyre-an-essential-part-of-modern-chips
[5]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/unique.c#L1514
[6]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/library/base/R/rowsum.R#L26
[7]: https://cran.r-project.org/web/packages/inline/index.html
[8]: /2019/02/24/a-strategy-for-faster-group-statisitics/#blood-from-a-turnip/
[9]: /2019/02/24/a-strategy-for-faster-group-statisitics/#optim-start/
[10]: https://github.com/JohnMount
[11]: https://github.com/WinVector/FastBaseR/blob/f4d4236/R/cumsum.R#L105
[12]: https://en.wikipedia.org/wiki/Double-precision_floating-point_format
[13]: https://github.com/wch/r-source/blob/R-3-5-branch/src/main/cum.c#L30
[14]: https://en.wikipedia.org/wiki/Extended_precision#Working_range
