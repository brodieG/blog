---
title: Faster Group Stats in Base R
author: ~
date: '2019-03-03'
slug: faster-group-stats-in-base-r
categories: [r]
tags: [optim, rdatatable]
image: /front-img/rock-em-sock-em.png
imagerect: /front-img/rock-em-sock-em-wide.png
imagemrgvt: 0%
imagemrghz: 0%
draft: true
weight: 1
contenttype: article
descriptionlong: "In which scrappy base R takes on the reigning group stats champ
  data.table, with suprising results.  Cache effects, branch prediction, pixie
  dust, IEEE 754, and more!"
---

```{r echo=FALSE}
options(digits=3, crayon.enabled=TRUE)
library(ggplot2)
knitr::opts_chunk$set(comment = "", fig.align='center', error=TRUE)
```
```{r echo=FALSE, comment="", results='asis'}
old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)
```
```{r, echo=FALSE}
writeFansi <- function(x) {
  writeLines(
    paste0(
      "<pre></pre><pre><code>",
      paste0(fansi::sgr_to_html(x), collapse="\n"),
      "</code></pre>"
  ) )
}
```

# In One Corner...

<!-- this needs to become a shortcode -->
<img
  id='front-img' src='/front-img/rock-em-sock-em.png'
  class='post-inset-image'
/>
<!--
Image source:
https://www.flickr.com/photos/ariels_photos/4195885445/in/photolist-7oM1yv-8fyCrd-na8xCB-naaZpy-na8EkB-LTtYa-6WxNKb-9JDyW-qhNGC7-9JDm3-9JDt2-7rYhX7-9JDgE-9JDSo-qighyD-5Eefam-MP7aqp-27hhiHB-9JDNS-9JDoL-7f7eAt-imcMdo-ioeXfv-PEADr-8fyCqS-8f4Nm4-8f4Nm8-8fdcpz-eoigzJ-enHBht-4faLtc-8fyCrq-8fyCrJ-UQBbNR-9JB87-8fdcqR-fuJahW-8eWrsR-8fdcqa-8fyCr1-eoigHu-6PbaCE-idksM7-enHzAn-enHB3p-6P6ZXr-bLw8Sz-4fGYzV-6PbaqN-eoihsS

Ariel Waldman Copyright 2009
CC BY-SA 2.0
-->
As we saw in our [previous post ][21], `data.table` is the group statistics
heavyweight champ.  Its `gforce` functions and fast grouping put it head and
shoulders above all challengers. And yet, here we are, about to throw our hat in
the ring with nothing but base R functionality.  Are we out of our minds?

TL;DR: some surprising results, close encounters with cache effects and branch
prediction from R, and an epic battle with double precision imprecision.  Oh,
and pixie dust.

# The Ring

Ten million observations, ~one million groups, no holds barred:

DISCLAIMER ABOUT GROUP COUNT?

```{r}
RNGversion("3.5.2"); set.seed(42)
n     <- 1e7
n.grp <- 1e6
grp   <- sample(n.grp, n, replace=TRUE)
noise <- rep(c(.001, -.001), n/2)
x     <- runif(n) + noise
y     <- runif(n) + noise  # we'll use this later
```
```{r sys-time, echo=FALSE}
sys.time <- function(exp, reps=11) {
  res <- matrix(0, reps, 5)
  time.call <- quote(system.time({NULL}))
  time.call[[2]][[2]] <- substitute(exp)
  gc()
  for(i in seq_len(reps)) {
    res[i,] <- eval(time.call, parent.frame())
  }
  structure(res, class='proc_time2')
}
print.proc_time2 <- function(x, ...) {
  print(
    structure(
      x[order(x[,3]),][floor(nrow(x)/2),],
      names=c("user.self", "sys.self", "elapsed", "user.child", "sys.child"),
      class='proc_time'
) ) }
```

Let's try a warm up round by computing sums of `x` grouped by `grp`.<span
id=sum-unsorted></span>

```{r eval=FALSE}
sys.time({
  grp.dat <- split(x, grp)
  x.ref <- vapply(grp.dat, sum, 0)
})
```
```
   user  system elapsed
  6.552   0.161   6.806
```

This is roughly equivalent to and slightly faster than `tapply(x, grp, sum)`.

`sys.time` is a wrapper around `system.time` that runs the expression eleven
times and returns the median timing.  It is [defined in the
appendix](#sys.time).

```{r eval=FALSE}
library(data.table)
DT <- data.table(grp, x)
setDTthreads(1)
sys.time(x.dt <- DT[, sum(x), keyby=grp][[2]])
```
```
   user  system elapsed
  0.941   0.030   0.973
```

Ouch.  Even without multithreading `data.table` crushes `split`/`vapply`.

We use one thread for more stable and comparable results.  We'll show some
multi-threaded benchmarks at the end.

# Pixie Dust

Something pretty remarkable happened between R3.2x and R3.3.0: the `data.table`
team [contributed their radix sort to R][24].  In particular with `order`, the
radix sort is **fifty** times faster than the shell sort it replaces.

Reason #1 why this is a huge deal:

```{r eval=FALSE}
sys.time({
  o <- order(grp)
  go <- grp[o]
  xo <- x[o]
  grp.dat.o <- split(xo, go)
  x.ref.o <- vapply(grp.dat.o, sum, 0)
})
```
```
   user  system elapsed
  2.470   0.141   2.611
```

Compare to our previous run:<span id=sum-unsorted></span>

```{r eval=FALSE}
sys.time({
  grp.dat <- split(x, grp)
  x.ref <- vapply(grp.dat, sum, 0)
})
```
```
   user  system elapsed
  6.552   0.161   6.806
```

Sorting values by group prior to running `split`/`vapply` makes them twice
as fast as in the unsorted case, **including the time required to sort the
data**.  We just doubled the speed of an important workhorse R function without
touching the code.  And we get the exact same results:

```{r eval=FALSE}
identical(x.ref, x.ref.o)
```
```
[1] TRUE
```

We can tell that the difference in the two times must come from the `split` step
as the result of that step is the same in both cases:

```{r eval=FALSE}
identical(grp.dat, grp.dat.o)
```

Pixie dust.  `data.table` pixie dust, really.  Maybe it's not exactly gracious
of us to take advantage of a `data.table` improvement in base R to do battle
with `data.table`, but what's the fun in being gracious nowadays?

```{r eval=FALSE, echo=FALSE}
vs <- function(x, grp) {
  grp.dat <- split(x, grp)
  vapply(grp.dat, sum, 0)
}
vso <- function(x, grp) {
  o <- order(grp)
  go <- grp[o]
  xo <- x[o]
  grp.dat.o <- split(xo, go)
  vapply(grp.dat.o, sum, 0)
}

treeprof::treeprof(vso(x, grp))
treeprof::treeprof(vs(x, grp))
treeprof::treeprof(vs(x, grp))
treeprof::treeprof(vso(x, grp))

treeprof::treeprof(tapply(x, grp, sum))
```
                                   milliseconds
tapply -------------------------- : 8401 -    0
    lapply ---------------------- : 5218 -  374
    |   FUN --------------------- : 4844 - 3744
    |       sort ---------------- : 1100 -    0
    |           unique.default -- :  997 -  997
    |           sort.default ---- :  103 -    0
    |               sort.int ---- :  103 -   57
    |                   order --- :   46 -   46
    split ----------------------- : 2939 -    0
    |   split.default ----------- : 2939 - 2939
    as.integer ------------------ :  101 -  101
    lengths --------------------- :   92 -   92


```{r eval=FALSE}
grp.o <- grp[order(grp)]
sys.time(unique(grp.o))
```
```
   user  system elapsed
  0.382   0.049   0.434
```
```{r eval=FALSE}
sys.time(unique(grp))
```
```
   user  system elapsed
  1.213   0.054   1.275
```

Internally `unique` uses a [hash table][2] to detect duplicate values, so it may
seem odd that the lookup speed is affected by ordering as the hashing time
should not be affected by the input order.  Most likely it is some combination
of caching and [branch prediction][3].  In the sorted version, we will look up
the same key repeatedly, which means the corresponding value will be warm and
snug in the [highest speed cache][4].  Additionally, since all duplicated values
are in sequence, the processor will do a pretty good job predicting branches
just by assuming the next value will be duplicated.  It will get it wrong at
each transition to a new value, but it will be right more often than not if
there is sufficient duplication.  I have no idea if this is actually what is
going on or not, but it seems likely, and I'm tickled pink by the thought that
I'm noticing these effects from R-level for the first time.

Now, for why I'm ranting about this, it turns out that the combined operation of
sorting and `unique`ing is faster than the just `unique`ing:

```{r eval=FALSE}
sys.time(unique(grp[order(grp)]))
```
```
   user  system elapsed
  0.854   0.060   0.918
```

This also has a big impact on `rowsum`, as it uses both [`unique`][6] on top of
its own [hash table][5]:<span id=rowsum-ordered></span>

```{r eval=FALSE}
sys.time({
  o <- order(grp)
  x.rs2 <- rowsum(x[o], grp[o], reorder=FALSE)
})
```
```
   user  system elapsed
  1.283   0.105   1.430
```

This puts us in striking distance of single-threaded `data.table`:

```{r echo=FALSE}
times <- data.frame(
  Function=c('tapply', 'rowsum', 'rowsum-sorted', 'data.table'),
  time=c(7.308, 2.310, 1.430, 0.973)
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col() +
  # facet_wrap(~Function) +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```


<span id=alt-algo></span>
One last thing before we move on: knowing that the data is sorted opens up
different algorithms for many tasks such as `unique` and `rowsum`.  To
illustrate I implemented the alternate [algorithms in C in the
appendix](#sorting-versions).  We wrap those in R functions that order
the data first, and to distinguish them from their base counterparts we
pluralize them.  The trailing "s" indicate these are the sorting versions of the
functions, or maybe that they are the speedy versions.  My dog ate the memo so I
don't know anymore[^love-r-but]...

Here is what happens with `uniques`:<span id=unique-c></span>

```{r eval=FALSE}
uniques <- function(x, res.sorted=FALSE) {
  o <- order(x)
  xo <- x[o]
  i <- .uniques(xo)          # C code: see appendix
  if(!res.sorted) xo[i][order(o[i])] else xo[i]
}
sys.time(uniques(grp))
```
```
   user  system elapsed
  0.587   0.018   0.615
```

This is almost twice as fast as the original version with the sorted data, and
it could be made faster if we were okay with getting results back sorted.  Most
of the time is actually spent in the `o <- order(x)` step, so the actually
unique-ing is an order of magnitude faster!

Similar story for `rowsums`:

```{r eval=FALSE}
rowsums <- function(x, group) {
  o <- order(group)
  .rowsums(x[o], group[o])    # C code: see appendix
}
system.time(rowsums(x, grp))
```
```
   user  system elapsed
  0.820   0.022   0.846
```

The implementations are not equivalent, but for these specific inputs they
should be comparable.  Your mileage will vary depending on the degree of
uniqueness of the data.  More details [in the appendix](#sorting-versions).

The main point I'm trying to make here is that it is a **big deal** that `order`
can order fast enough that we can switch the algorithms we use downstream and
get an even bigger performance improvement.  A big thank you to team
`data.table` for sharing the pixie dust.

# So You Think You Can Group-Stat?

Okay, great, we can sum quickly in base R.  One measly stat.  What good is that
if we want to compute something more complex like the slope of a bivariate
regression, as we did in our [prior post][8]?  As a refresher this is what the
calculation looks like:

$$\frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i -
\bar{x})^{2}}$$

The R equivalent is[^slope-mod]:<span id='slope-ex'></a>

```{r eval=FALSE}
slope <- function(x, y) {
  x_ux <- x - mean.default(x)
  y_uy <- y - mean.default(y)
  sum(x_ux * y_uy) / sum(x_ux ^ 2)
}
```

We can see that `sum` shows up explicitly, and somewhat implicitly via
`mean`[^not-quite-sum].  There are many statistics that essentially boil down to
adding things together, so we can use `rowsum` as the wedge to breach the
heretofore impenetrable barrier to fast grouped statistics in R[^forgive-me]:
<span id=group-slope-rs></span>

```{r eval=FALSE}
group_slope <- function(x, y, grp) {
  ## order inputs by group
  o <- order(grp)
  go <- grp[o]
  xo <- x[o]
  yo <- y[o]

  ## group sizes and group indices
  grle <- rle(go)
  gn <- grle[['lengths']]
  gi <- rep(seq_along(gn), gn)

  ## compute mean(x) and mean(y), and recycle them
  ## to each element of `x` and `y`:
  sx <- rowsum(xo, go)     # sum(x)
  ux <- (sx/gn)[gi]        # mean(x), recycled to original vec length
  sy <- rowsum(yo, go)
  uy <- (sy/gn)[gi]

  ## (x - mean(x)) and (y - mean(y))
  x_ux <- xo - ux
  y_uy <- yo - uy

  ## Slopes!
  setNames(rowsum(x_ux * y_uy, go) / rowsum(x_ux ^ 2, go), grle[['values']])
}
```

The non-obvious steps involve `gn` and `gi`:

```{r eval=FALSE}
grle <- rle(go)
gn <- grle[['lengths']]
gi <- rep(seq_along(gn), gn)
```

`base::rle` computes "Run Lengths" a.k.a. the length of sequences of repeated
values.  With our ordered groups this corresponds to the size of the groups.
We can illustrate in simplified form:

```{r}
(xo <- 2:6)                              # some values
(go <- c(3, 3, 5, 5, 5))                 # their groups
(gn <- rle(go)[['lengths']])             # the size of the groups
```

`gn` tells us how many elements there are in each group.  This allows us to
compute the `$\bar{x}$` values:

```{r}
(sx <- rowsum(xo, go))                   # sum of each group
(ux <- sx / gn)                          # mean of each group
```

But we need to compute `$x - \bar{x}$`, which means we need to recycle each
group's `$\bar{x}$` value for each `$x$`.  This is what `gi` does:

```{r}
(gi <- rep(seq_along(gn), gn))
cbind(x=xo, ux=ux[gi], g=go)
```

For each original `$x$` value, we have associated the corresponding `$\bar{x}$`
value.  In `group_slope` we combined the mean calculation and recycling into one
step: `xu <- (xs/gn)[gi]`.

This is pretty fast with our original 10MM record data set:

```{r eval=FALSE}
sys.time(slope.rs <- group_slope(x, y, grp))
```
```
   user  system elapsed
  3.937   0.681   4.622
```

Let's look at all the timings, including variations from our [prior grouped
statistics post][8], as well as the "pixie dust" (ordered) version of the
`vapply` method ([code and updated timings in
appendix](#other-slope-benchmarks):


```{r rowsum-timings-all, echo=FALSE}
funs <- c('*pply', '*pply', 'data.table', 'data.table', 'rowsum')
times <- data.frame(
  Function=factor(funs, levels=unique(funs)),
  Version=c('normal', 'ordered', 'normal', 'optim', 'normal'),
  time=c(12.573, 8.557 , 7.172, 3.697, 4.622)
)
ggplot(times, aes(x=Version, y=time)) +
  geom_col() +
  facet_grid(.~Function, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

`rowsum` is substantially faster than everything except the
~3x faster than the basic `*pply` version, and faster than the
~2x faster than the unoptimized `data.table`[^timing-note].  Sprinkling pixie
dust on `*pply` by ordering the inputs (see [appendix](#tapply-ordered) gets us
close to unoptimized `data.table`, but `rowsum` is still ~2x faster.  But
`data.table` keeps the performance crown, albeit with a little help[^dt-optim].
More details for the benchmarks are [in the appendix](#other-benchmarks).

So like Rocky Balboa we walk off into the sunset, without the title but
satisfied we put up a good fight.  Except...

# Round II

Remember how we saw earlier that a fast order opened up opportunities for [new
algorithms](#alt-algo)?  [John Mount][10], shows how we can compute group sums
[using `cumsum`][11] on group-ordered data[^prior-art].  With a little work we
can generalize it.

The concept is simple: order by group, compute cumulative sum, pull out the last
value for each group, and take their differences.  Visually:

```{r cumsum-ex, echo=FALSE}
library(ggbg)
RNGversion("3.5.2"); set.seed(42)
n1 <- 7
x1 <- seq_len(n1)
y1 <- runif(n1);
colors <- c('#3333ee', '#33ee33', '#eeee33')
g1 <- sample(1:3, n1, replace=TRUE)
steps <- c(
  '1 - Start', '2 - Sort By Group', '3 - Cumulative Sum',
  '4 - Last Value in Group', '5 - Take Differences', '6 - Group Sums!'
)
steps <- factor(steps, levels=steps)
df1 <- data.frame(
  x1, y1, g1=as.character(g1), step=steps[[1]], stringsAsFactors=FALSE
)
df2 <- df1[order(g1),]
df2[['x1']] <- x1
df2[['step']] <- steps[[2]]
df3 <- df2
df3 <- transform(
  df3, yc=cumsum(y1), step=steps[[3]],
  last=c(head(g1, -1) != tail(g1, -1), TRUE)
)
df4 <- transform(df3, step=steps[[4]])
df5 <- transform(
  subset(df4, last), x1=3:5, y1=c(yc[[1]], diff(yc)), step=steps[[5]]
)
df6 <- transform(df5, step=steps[[6]])

plot.extra <- list(
  facet_wrap(~step, ncol=2),
  ylab(NULL), xlab(NULL),
  theme(
    axis.text.x=element_blank(), axis.text.y=element_blank(),
    panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
    axis.ticks=element_blank()
  ),
  scale_fill_manual(values=setNames(colors, 1:3), guide=FALSE)
)

ggplot(mapping=aes(x=x1, y=y1, fill=g1)) +
  geom_col(data=df1) +
  geom_col(data=df2) +
  geom_col(data=df3, mapping=aes(y=yc)) +
  geom_col(
    data=df4, mapping=aes(y=yc, alpha=I(ifelse(last, 1, .15))),
    width=0.9
  ) +
  geom_col(data=df5, width=0.9, position='waterfall') +
  geom_col(data=df6, width=0.9) +
  plot.extra
```

This is the data we used for the visualization:

```{r echo=FALSE}
x1 <- y1 # a bit confusing with x/y for ggplot
```
```{r}
g1
x1
```

The first three steps are obvious:

```{r}
ord <- order(g1)
go <- g1[ord]
xo <- x1[ord]
xc <- cumsum(xo)
```

Picking the last value from each group is a little harder, but we can do so with
the help of `base::rle`.  `rle` returns the lengths of repeated-value
sequences within a vector.  In a vector of ordered group ids, we can use it to
compute the lengths of each group:

```{r}
go
grle <- rle(go)
(gn <- grle[['lengths']])
```

This tells us the first group has two elements, the second also two, and the
last three.  We can translate this into indices of the original vector with
`cumsum`, and use it to pull out the relevant values from the cumulative sum of
the `x` values:

```{r}
(gnc <- cumsum(gn))
(xc.last <- xc[gnc])
```

To finish we just take the differences:

```{r}
diff(c(0, xc.last))
```

I wrapped the whole thing into the [`group_sum` function](#group_sum) you can
see in the appendix:

```{r "group_sum-def", echo=FALSE}
group_sum <- function(x, grp) {
  ## Order groups and values
  ord <- order(grp)
  go <- grp[ord]
  xo <- x[ord]

  ## Last values
  grle <- rle(go)
  gnc <- cumsum(grle[['lengths']])
  xc <- cumsum(xo)
  xc.last <- xc[gnc]

  ## Take diffs and return
  gs <- diff(c(0, xc.last))
  setNames(gs, grle[['values']])
}
```
```{r}
group_sum(x1, g1)
```

Every step of `group_sum` is internally vectorized[^int-vec], so the function is
fast.  We demonstrate here with the original 10MM data set:

```{r eval=FALSE}
sys.time(x.grpsum <- group_sum(x, grp))
```
```
   user  system elapsed
  1.098   0.244   1.344
```
```{r eval=FALSE}
all.equal(x.grpsum, c(x.ref), check.attributes=FALSE)
```
```
[1] TRUE
```

This is slightly faster than [pre-ordered `rowsum`](#rowsum-ordered)
calculation, but as we will see shortly it has the additional advantage that we
can capture and re-use intermediate steps.  I wrote a [variation](#cumulative
group-sum-with-na-and-inf) that handles `NAs` and `Inf` values to show it can be
done, but we will ignore than wrinkle going forward for clarity's sake.

Now for the real fun: let's re-implement the [initial `rowsum` group slope
function](#group-slope-rs) with this method:

```{r}
.group_sum_int <- function(x, last.in.group) {
  xgc <- cumsum(x)[last.in.group]
  diff(c(0, xgc))
}
group_slope2 <- function(x, y, grp, group_sum=.group_sum_int) {
  ## order inputs by group
  o <- order(grp)
  go <- grp[o]
  xo <- x[o]
  yo <- y[o]

  ## group sizes and group indices
  grle <- rle(go)
  gn <- grle[['lengths']]
  gi <- rep(seq_along(gn), gn)
  gnc <- cumsum(gn)    # Last index in each group

  ## compute mean(x) and mean(y), and recycle them
  ## to each element of `x` and `y`:
  sx <- group_sum(xo, gnc)
  ux <- (sx/gn)[gi]
  sy <- group_sum(yo, gnc)
  uy <- (sy/gn)[gi]

  ## (x - mean(x)) and (y - mean(y))
  x_ux <- xo - ux
  y_uy <- yo - uy

  ## Slopes!
  x_ux.y_uy <- group_sum(x_ux * y_uy, gnc)
  x_ux2 <- group_sum(x_ux ^ 2, gnc)
  setNames(x_ux.y_uy / x_ux2, grle[['values']])
}
```
Let's time it:
```{r eval=FALSE}
sys.time(slope.gs <- group_slope2(x, y, grp))
```

<!-- ed note: the timings below appear sensitive to other stuff running on the
system.  This is the fastest I've recorded:
   user  system elapsed
  1.864   0.496   2.368
-->
```
   user  system elapsed
  1.928   0.656   2.587
```

Oh snap, 30% faster than `data.table`!

```{r slope-timings-2, echo=FALSE}
funs <- c('*pply', '*pply', 'data.table', 'data.table', 'rowsum', 'cumsum')
times <- data.frame(
  Function=factor(funs, levels=unique(funs)),
  Version=c('normal', 'ordered', 'normal', 'optim', 'normal', 'normal'),
  time=c(14.184, 9.867 , 7.172, 3.697, 4.915, 2.647)
)
ggplot(times, aes(x=Version, y=time)) +
  geom_col() +
  facet_grid(.~Function, scales='free_x', space='free_x') +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

The key advantage `group_slope2` has over the `rowsum` version is that it can
re-use `gnc`, the vector of indices to the last value in each group.  Computing
`gnc` is the expensive part of the `cumsum` group sum calculation:<span
id=basic-calcs></span>

```{r echo=FALSE}
o <- order(grp)
go <- grp[o]
xo <- x[o]
gn <- rle(go)[['lengths']]
gi <- rep(seq_along(gn), gn)
gnc <- cumsum(gn)
```
```{r eval=FALSE}
o <- order(grp)
go <- grp[o]
xo <- x[o]
sys.time({
  gn <- rle(go)[['lengths']]
  gi <- rep(seq_along(gn), gn)
  gnc <- cumsum(gn)
})
```
```
   user  system elapsed
  0.398   0.134   0.535
```

Once we have `gnc` the group sum is blazing fast:

```{r eval=FALSE}
sys.time(.group_sum_int(xo, gnc))
```
```
   user  system elapsed
  0.042   0.008   0.050
```

We compute group sums four times when computing the slope, so being able to
re-use the last-value-in-group indices `gnc` is a big deal.

# Precision Schmecision

## A Disclaimer

I have no special knowledge of floating point precision issues.  Everything I
know is from reading the [wiki page][12], random web research, and
experimentation.  If your billion dollar Mars lander burns up on entry because
you relied on information in this post, you only have yourself to blame.

## Oh the Horror!

The skeptical reader might have noticed that we did not compare the results of
`group_slope2` with reference values.  It turns out that our blazing fast
benchmark hero is cutting some corners:

```{r eval=FALSE}
all.equal(slope.gs, c(slope.rs), check.attributes=FALSE)
```
```
[1] "Mean relative difference: 0.0001161377"
```
With a generous tolerance we find equality:
```{r eval=FALSE}
all.equal(slope.gs, c(slope.rs), check.attributes=FALSE, tolerance=2e-3)
```
```
[1] TRUE
```

Let's find the worst offender, which for convenience we'll call `B`.  Its index
in the group list is then `B.gi` and its group is `B.g`:<span
id=prec-error></span>

```{r echo=FALSE}
 # hard coding the above value so that rest of stuff that we actually
 # run can use it
B.gi <- 616793L
B.g <- 616826L
B.i <- go == B.g
```
```{r eval=FALSE}
B.gi <- which.max(abs(slope.gs / slope.rs - 1))
slope.gs[B.gi]
```
```
 616826
-3014.2
```
```{r eval=FALSE}
slope.rs[B.gi]
```
```
   616826
-2977.281
```

Not very good.  Let's get the indices of the interesting values:

```{r eval=FALSE}
B.g <- as.integer(names(slope.gs[B.gi]))
B.i <- go == B.g           # indices corresponding to our group
```
```{r echo=FALSE}
B.i <- which(go == B.g)    # indices corresponding to our group
B.ni <- tail(B.i, 1)       # last element in group
A.ni <- head(B.i, 1) - 1   # last element in previous group
```

And the actual values:

```{r eval=FALSE}
(B.x <- xo[B.i])  # x values in our group
```
```
[1] 0.4239786 0.4239543
```
```{r eval=FALSE}
(B.y <- yo[B.i])  # y values in our group
```
```
[1] 0.7637899 0.8360645
```

The `x` values are close to each other and are ultimately responsible for the
precision issues.  However, they are well within the range of values that should
be manageable for [double precision floats][12] ("doubles" henceforth), which
are what R "numeric" values are stored as.  Our problem starts with the next
step which is to compute `$\sum{(x - \bar{x})^2}$`.  To see how bad it is need
to calculate all the steps up to just before the group sum of `$(x -
\bar{x})^2$`.  Recall we calculated `xo` (`x` ordered), `gi` (group indices),
`gn` (group lengths), and `gnc` (group lengths accumulated)
[previously](#basic-calcs).

```{r}
old.opt <- options(digits=22, scipen=100) # show more digits
sx <- .group_sum_int(xo, gnc)    # sum(x)
ux <- (sx/gn)[gi]                # mean(x), recycled
x_ux <- xo - ux                  # (x - mean(x))
x_ux2 <- (x_ux ^ 2)              # (x - mean(x)) ^ 2
x_ux2c <- cumsum(x_ux2)
```

We now have the cumulative sum of the value we want to examine, `$(x -
\bar{x})^2$`.  Before we go further, let's revisit our algorithm to see why we
have a problem:


```{r cumsum-review, echo=FALSE}
dfa <- data.frame(
  x1=1:7, y1=0, label=c('A.1', 'A.n', 'B.1', 'B.n', 'C.1', 'C.2', 'C.n'),
  step=steps[4]
)
dfb <- data.frame(
  x1=3:5, y1=0, label=c('A.s', 'B.s', 'C.s'), step=steps[[5]]
)
ggplot(mapping=aes(x=x1, y=y1, fill=g1)) +
  geom_col(
    data=df4, mapping=aes(y=yc, alpha=I(ifelse(last, 1, .15))),
    width=0.9
  ) +
  geom_col(data=df5, width=0.9, position='waterfall') +
  geom_text(data=dfa, aes(fill=NULL, label=label), vjust=1.2) +
  geom_text(data=dfb, aes(fill=NULL, label=label), vjust=1.2) +
  plot.extra
```

The cumulative sums will be quite precise as [R internally uses a
representation][13] that allows [18-19 digits of precision][14] for
`cumsum`[^extended-precision].  This additional precision is
rounded back to standard double precision on exit from `cumsum`.  Where things
go wrong is when we take the difference of the resulting doubles.  First, let's
get the indices that correspond to the last values of the group of interest
(`B`) and the preceding group (`A`):

```{r}
B.ni <- tail(B.i, 1)       # last element in group
A.ni <- head(B.i, 1) - 1   # last element in previous group
```

The values are then:

```{r}
(B.n <- x_ux2c[B.ni])   # end val of target group
```

And:

```{r}
(A.n <- x_ux2c[A.ni])   # end val of prior group
```

These values are awful close to each other.  In our algorithm we take the
difference of these two numbers to get the group sum.  But when values are so
close relative to their magnitudes, doing so is the computing equivalent of
walking down the stairs into a dark basement with ominous background music in a
horror flick.  To leave no doubt about the carnage that is about to unfold
(viewer discretion advised):<span id=carnage></span>

```{r echo=FALSE}
B.s <- B.n - A.n
```
```{r eval=FALSE}
B.s <- B.n - A.n
rbind(A.n, B.n, B.s)
```
<pre></pre>
```{r echo=FALSE}
val <- "                                       [,1]
A.n  \033[42m462824.40162014\033[43m582\033[m89287984371185302734
B.n  \033[42m462824.40162014\033[43m611\033[m99671030044555664062
B.s       0.000000000\033[43m29\033[m10383045673370361328"
writeLines(val)
```

We highlight the approximate double precision digits in green, and the
additional digits that `cumsum` might use internally in yellow[^extended-2].
The colors here have no semantic relation to those in the illustrative
plots.<span id=mu-prec></span>  The only precision left is in the extended
precision range, but that is not actually available outside of `cumsum`.

So how much precision does `B.s` actually have?  The naive decimal view suggests
quite a bit with all those trailing digits.  The color highlighting tells us
instead there probably is very little as `B.s` does not have any digits in the
"green".  Finally, the observed error of -1.2% suggests there must be some
precision.

We can get a better idea by comparing the binary representation of the `A.n` and
`B.n` values to their difference `B.s`.  Even though `A.n` and `B.n` were
computed in 80 bit extended precision by `cumsum`, only the rounded 64 bit
values are available in the results:

```{r echo=FALSE}
val <- "+18     +10        0        -10       -20       -30 -34  <- Exponent (2^n)
 |       |         |         |         |         |   |
\033[34m+\033[39;42m11100001111111010000110011011010000100100111110111111\033[m: A.n
\033[34m+\033[39;42m11100001111111010000110011011010000100100111111000100\033[m: B.n
\033[34m+\033[m00000000000000000000000000000000000000000000000000\033[42m101\033[m: B.s"
writeLines(val)
```

In "doubles" the exponent is encoded with 11 of the 64 bits.  Here it is
`+18` for `A.n` and `B.n`, and `-32` for `B.s`. Another bit is used to encode
the sign, and the remaining 53 bits are use to encode the normalized value
(a.k.a.  significand) shown in green here[^fifty-three].  The precision of
doubles is dictated by the significand.

After taking the difference of `A.n` and `B.n` we are left with three bits of
precision for `B.s`[^three-bits].  But what about all those digits we saw
previously?  Well, check this out:

```{r}
2^-32 + 2^-34
```
```{r}
B.s
```

The entire precision of `B.s` is encapsulated in `$2^{-32} + 2^{-34}$`.
Each non-zero bit in the significand is represented by `$2^{-n}$` where `$n$` is
the position in the bit field.  All those digits are just an artifact of the
conversion of a limited precision binary number into decimal representation.

The last significant bit in our starting numbers corresponds to
`$2^{-34}$`, so at the limit both `A.n` and `B.n` could be off by as much as
`$\pm2^{-35}$` as a result of rounding down to 64 bit precision.  It follows
that `B.s`, which is the difference of `A.n` and `B.n`, could have up to twice
the error:

$$2 \times \pm2^{-35} = \pm2^{-34}$$

With a baseline value of `B.s` of `$2^{-32} + 2^{-34}$`, the relative error
becomes[^rel-err]:

$$\pm\frac{2^{-34}}{2^{-32} + 2^{-34}} = \pm20\%$$

If the theoretical precision is only `$\pm20\%$`, how did we end up with only a
`$-1.2\%$` error?  Sheer luck.  Let's look again at the `A.n` and `B.n` values,
but this time comparing the underlying 80 bit representations used by `cumsum`.
The 80 bit floating point representation adds 10 bits to the significand.  We'll
discuss shortly how we got these 80 bit representations, but first let's look at
`A.n`:<span id='a-err'></span>

```{r a-n-err, echo=FALSE}
val <- "+18     +10        0        -10       -20       -30       -40
 |       |         |         |         |         |   *     |   *
\033[34m+\033[42;39m11100001111111010000110011011010000100100111110111111\033[m00000000000: 64 bit
\033[34m+\033[42;39m11100001111111010000110011011010000100100111110111110\033[43m1110100010\033[m1: 80 bit
\033[34m+\033[39m00000000000000000000000000000000000000000000000000000000\033[43m1011101\033[m1: A.n Err
 "
writeLines(val)
```

And `B.n`:

```{r echo=FALSE}
val <- "+18     +10        0        -10       -20       -30       -40
 |       |         |         |         |         |   *     |   *
\033[34m+\033[39;42m11100001111111010000110011011010000100100111111000100\033[m00000000000: 64 bit
\033[34m+\033[39;42m11100001111111010000110011011010000100100111111000011\033[43m1111100010\033[m0: 80 bit
\033[34m+\033[39m0000000000000000000000000000000000000000000000000000000000\033[43m11110\033[m0: B.n Err"
writeLines(val)
```

The rounding error between the 80 and 64 bit representations for each of `A.n`
and `B.n` is small compared to the difference between the `A.n` and `B.n`.
Additionally, the rounding errors are in the same direction.  As a result the
total error caused by the rounding from 80 bits to 64 bits should be
`$(0.00111100 - 1.01111011) \times 2^{-38}$`, or a hair under `$2^{-38}$`:

```{r echo=FALSE}
options(old.opt) # switch back to normal display
options(digits=7)
```
```{r}
A.n.err.bits <- c(1,0,1,1,1,0,1,1)
B.n.err.bits <- c(0,0,1,1,1,1,0,0)
exps <- 2^-(38:45)
(err.est <- sum(B.n.err.bits * exps) - sum(A.n.err.bits * exps))
```

Here we estimated the error in our calculation from the binary representation,
and got a value very close to the actual error:<span id=prec-loss-err></span>

```{r}
(err.obs <-  B.s - sum((xo[B.i] - mean(xo[B.i]))^2))
```

This supports that we are carrying out the calculations on the 80 bit
representations correctly.  The numbers are not exactly the same because even
the 80 bit representation is limited in precision so we can only approximate the
error.

If we had been unlucky maybe the true values of `A.n` and `B.n` would have been
as follows:

```{r echo=FALSE}
val <- "+18     +10        0        -10       -20       -30       -40
 |       |         |         |         |         |   *     |   *
\033[34m+\033[39;42m11100001111111010000110011011010000100100111110111111\033[m00000000000: 64 bit
\033[34m+\033[39;42m11100001111111010000110011011010000100100111110111110\033[43m1000000000\033[m1: 80 bit
\033[34m+\033[m00000000000000000000000000000000000000000000000000000\033[43m0111111111\033[m1: A.n Err"
writeLines(val)
```

And:

```{r echo=FALSE}
val <- "+18     +10        0        -10       -20       -30       -40
 |       |         |         |         |         |   *     |   *
\033[34m+\033[39;42m11100001111111010000110011011010000100100111111000100\033[m00000000000: 64 bit
\033[34m+\033[39;42m11100001111111010000110011011010000100100111111000100\033[43m1000000000\033[m0: 80 bit
\033[31m-\033[39m00000000000000000000000000000000000000000000000000000\033[43m1000000000\033[m0: B.n Err"
writeLines(val)
```

These have the same 64 bit representations as the original values, but the error
is quite different:

```{r}
A.n.err.bits <- c(0,1,1,1,1,1,1,1,1,1,1)
B.n.err.bits <- -c(1,0,0,0,0,0,0,0,0,0,0)  # note this is negative
exps <- 2^-(35:45)

(err.est.max <- sum(B.n.err.bits * exps) - sum(A.n.err.bits * exps))
err.est.max / B.s
err.obs / B.s
```

In this case we get roughly the maximum possible error, which is ~16 times
larger than the observed one.

## A New Hope

```{r eval=FALSE, echo=FALSE}
options(old.opt)
```

Before we give up it is worth pointing out that precision loss in some cases is
a feature.  For example, [Drew Schmidt's `float`][17] package implements single
precision numerics for R as an explicit trade-off of precision for memory and
speed.  In deep learning, [reduced precision is all the rage][20].  In a sense,
we are trading off precision for speed in our `cumsum` group sums approach,
though not explicitly.

Still, it would be nice if we could make a version of this method that
doesn't suffer from this precision infirmity.  And it turns out we can!  The
main source of precision loss is due to the accumulated sum eventually growing
much larger than any given individual group in size.  This became particularly
bad for [cumulative sum of `$(x - \bar{x})^2$`](#carnage) due to the squaring
that exacerbates relative magnitude differences.

If you are still wondering how we came up with the 80 bit representations of the
[cumulative sums earlier](#eighty-bit), wonder now more.  For example, for
`A.n`, we make a copy of `$(x - \bar{x})^2$` vector, and append the negative
of `A.n` at the position following `A.n` position:

```{r bin_rep, echo=FALSE}
 # Only works for normal finitite non-zero non-NA scalar doubles

bin_rep <- function(x) {
  stopifnot(length(x) == 1)
  sign <- if(x > 0) '+' else '-'
  x <- abs(x)
  exp <- floor(log2(x))
  sig <- integer(53)
  exps <- ((exp):(exp - 53 + 1))

  for(i in seq_along(exps)) if(sig[i] <- x >= 2^exps[i]) x <- x - 2^exps[i]

  writeLines(sprintf("Exp: 2^%d Sig: %s%s", exp, sign, paste0(sig, collapse="")))
}
```
```{r}
x_ux2.A.n <- x_ux2[1:(A.ni + 1)]
x_ux2.A.n[A.ni + 1] <- -A.n
```

We now have the original `$(x - \bar{x})^2$` values up to the element just past
group `A`, and at that position, the **cumulative** sum of all the prior values,
`A.n`.  In theory, if we compute the cumulative sum again the final value should
be zero.  But it's not:

```{r}
A.n.err <- tail(cumsum(x_ux2.A.n), 1)
bin_rep(A.n.err)
```

`bin_rep`, defined in the appendix, extracts the binary representation of
doubles.  `A.n.err` is not zero because the cumulative sum was computed with 80
bit precision.  By adding `-A.n` in 64 bit precision to the end of the vector we
are left with the difference between the 80 bit and 64 bit versions of the
cumulative sum.  In this case you might recognize that `A.n.err` as the error we
showed when we compared the [80 and 64 bit versions of `A.n`](#a-err):

```{r a-n-err, echo=FALSE}
```

We got the 80 bit version of `A.n` by subtracting the error to the 64 bit
version[^by-hand].

More generally, we can compute the 80->64 bit conversion errors for every group
by adding computing the group sums as we did previously, and then adding their
negatives back to the input vector for a second pass cumulative sum.  Visually:

```{r echo=FALSE}
steps <- c(
  '2a - Sort By Group', '3a - Insert Neg Group Sum', '4a - Cumulative Sum',
  '5a - Last Val Is Error'
)
df2a <- transform(df2, step=factor(steps[1], levels=steps))
df3 <- rbind(
  transform(df2a, x1=x1+cumsum(c(0, diff(as.integer(g1))))),
  with(
    df2,
    data.frame(
      x1=c(3L,6L,10L),
      y1=-rowsum(y1,g1)+(runif(3)-.5)*.05,
      g1=as.character(1:3),
      step=step[1]
) ) )
df3 <- df3[order(df3[['x1']]),]
df3[['step']] <- steps[2]
df4 <- transform(df3, y1=cumsum(y1), step=steps[3])
df5 <-
  transform(df4, last=c(head(g1, -1) != tail(g1, -1), TRUE), step=steps[4])
df5a <- transform(subset(df5, last))

ggplot(mapping=aes(x=x1, y=y1, fill=g1)) +
  geom_col(data=df2) +
  geom_col(data=df3) +
  geom_col(data=df4) +
  geom_col(data=df5, mapping=aes(alpha=I(ifelse(last, 1, .15)))) +
  geom_tile(
    data=df5a, mapping=aes(fill=NA, color=g1), alpha=1,
    height=.6, width=1.2, size=0.5
  ) +
  scale_colour_manual(
    values=setNames(c(colors[1:2], 'yellow'), 1:3), guide=FALSE
  ) +
  plot.extra
```

If the first pass group sum was fully precise, the values inside the
highlighting boxes in the last panel should be zero. The small values we see
inside the boxes represent the errors of each group computation[^for-effect].
Steps that we are not showing above are collecting these values, taking their
differences, and adding them back to the first pass values.

While the second pass is still subject to precision issues, these are greatly
reduced because the cumulative sum is reset to near zero after each group, so we
will never be subtracting two large but near numbers from each other.

`.group_sum_int2` below embodies the approach.  One slight modification is that
we subtract the first pass group value from the last value in the group instead
of appending it to the group:

```{r}
.group_sum_int2 <- function(x, last.in.group){
  x.grp <- .group_sum_int(x, last.in.group)     # imprecise pass
  x[last.in.group] <- x[last.in.group] - x.grp  # subtract from each group
  x.grp + .group_sum_int(x, last.in.group)      # compute errors and add
}
```
```{r eval=FALSE}
sys.time(slope.gs2 <- group_slope2(x, y, grp, group_sum=.group_sum_int2))
```
```
   user  system elapsed
  2.477   0.747   3.278
```
```{r eval=FALSE}
all.equal(c(slope.rs), slope.gs2, check.attributes=FALSE)
```
```
[1] TRUE
```
```{r eval=FALSE}
quantile(slope.rs - slope.gs2, na.rm=TRUE)
```
```
       0%       25%       50%       75%      100%
-7.11e-15 -2.78e-17  0.00e+00  2.78e-17  3.55e-15
```

This does not match the original calculations exactly, but there is essentially
no error left.  Compare to the single pass calculation:

```{r eval=FALSE}
quantile(slope.rs - slope.gs, na.rm=TRUE)
```
```
       0%       25%       50%       75%      100%
-1.35e-02 -3.65e-12 -3.51e-16  3.60e-12  3.69e+01
```

The new error is _sixteen_ orders of magnitude smaller than the original.

Two-pass precision improvement methods have been around for long time.  For
example R's own `mean` uses a [variation on this method][18].

## I Swear, It's a Feature

We noted earlier that precision loss can be a feature.  We can make it so in
this case by providing some controls for it.  We saw earlier that it is possible
to [estimate precision loss](#prec-loss-err), so we can use this decide whether
we want to trigger the second precision correcting pass.  [`.group_sum_int3` in
the appendix](#adjustable-precision-group-sums) does exactly this by providing a
`p.bits` parameter.  This parameter specifies the minimum number of bits of
precision in the resulting group sums.  If as a result of the first pass we
determine the worst case loss could take us below `p.bits`, we run the second
precision improvement pass:

```{r group_sum_int3, eval=FALSE, echo=FALSE}
.group_sum_int3 <- function(x, last.in.group, p.bits=53, info=FALSE) {
  xgc <- cumsum(x)[last.in.group]
  gmax <- floor(log2(max(abs(range(xgc)))))
  gs <- diff(c(0, xgc))
  gsabs <- abs(gs)
  gmin <- floor(log2(min(gsabs[gsabs > 0])))
  precision <- 53 + (gmin - gmax)
  if(precision < p.bits) {
    x[last.in.group] <- x[last.in.group] - gs
    gs <- gs + .group_sum_int(x, last.in.group)
  }
  if(info) # info about precision and second pass
    structure(gs, precision=precision, precision.mitigation=precision < p.bits)
  else gs
}
```
```{r eval=FALSE}
microbenchmark::microbenchmark(times=10,
  .group_sum_int(x_ux2, gnc),            # original single pass
  .group_sum_int2(x_ux2, gnc),           # two pass, always
  .group_sum_int3(x_ux2, gnc, p.bits=2), # check, single pass
  .group_sum_int3(x_ux2, gnc, p.bits=3)  # check, two pass
)
```
```
                                    expr   min    lq mean median  uq max neval
              .group_sum_int(x_ux2, gnc)  44.0  94.5  101     97 115 153    10
             .group_sum_int2(x_ux2, gnc) 196.2 258.2  277    271 294 365    10
 .group_sum_int3(x_ux2, gnc, p.bits = 2)  79.7 119.4  119    126 128 133    10
 .group_sum_int3(x_ux2, gnc, p.bits = 3) 294.5 308.3  324    312 349 371    10
```

In this example the worst case precision is 2 bits, so with `p.bits=2` we run in
one pass, whereas with `p.bits=3` two passes are required.  There is some
overhead in checking precision, but it is small enough relative to the cost of
the second pass that it is probably warranted in many cases.

Let's try our slope calculation, this time again demanding at least 16 bits of
precision.  For our calculation 16 bits of precision implies the error will be
at most `$\pm2^{-16} \approx 1.53 \times 10^{-5}$`.

```{r eval=FALSE}
.group_sum_int_16 <- function(x, last.in.group)
  .group_sum_int3(x, last.in.group, p.bits=16)

sys.time(slope.gs3 <- group_slope2(x, y, grp, group_sum=.group_sum_int_16))
```
```
   user  system elapsed
  2.095   0.778   2.874
```
<!-- fastest:
   user  system elapsed
  2.164   0.682   2.856
-->
```
[1] TRUE
```
```{r eval=FALSE}
quantile((slope.rs - slope.gs3) / slope.rs, na.rm=TRUE)
```
```
       0%       25%       50%       75%      100%
-2.91e-08 -2.51e-14  0.00e+00  2.51e-14  4.73e-09
```

Indeed the errors are no larger than that.  The precision check occurs only at
the group sum step, so if other steps steps accumulate errors it is possible for
the final precision of a more complex computation to end up below the specified
levels.

<!-- ed note: fastest recorded, again with wifi off, everything shut down
   user  system elapsed
  2.234   0.598   2.842
-->

Updated reference times for `data.table`.  This is after we killed all
extraneous processes, including firefox.




Note on vector size[^vec-size].


# Conclusions

<!-- this needs to become a shortcode -->
<!-- this is populated by JS in feedback.html partial -->
<div id='feedback-cont'></div>

# Outtakes

## Outtakes in Blog Post?

When I first started to write the blog post I was exploring three different
methods for computing group sums.  As is usually the case with R, there are many
ways to do one things.  I include the slower two methods for completeness.

## It's All In the Capitalization

We take our first swing with `base::rowsum`.  That's right, `rowsum`, not
`rowSums`.  Lower case singular makes all the difference in the world.  This is
R, after all, and as any self respecting R user that has read the
seminal paper "Semantics of Capitalization, Punctuation, and Pluralization in R
Function Names" can tell you, the former sums rows within groups, and the latter
sums columns within rows.  You did read the memo, right?  Good.

Even though the function names are self explanatory, let's illustrate for
completeness:

```{r}
mx <- matrix(1:8, 4, byrow=TRUE) * 10 ^ (0:3)
mx
rowsum(mx, group=rep(c('odd', 'even'), 2))
```

`rowsum` preserved the two columns, but collapsed the rows by the `group` value.

```{r eval=FALSE}
rowSums(mx)       # note: output has been edited for clarity
```
```{r echo=FALSE}
matrix(rowSums(mx))
```

`rowSums` collapsed the columns but preserved the rows.  Normally `rowSums`
returns a vector, but here we display it as a one column matrix so the
relationship to the input matrix is clear.

Many R functions compute primarily in compiled code (e.g `sum`), several compute
on arbitrary groups (e.g. `tapply`), some even compute on equal sized groups in
compiled code (e.g. `rowSums`).  As far as I know, `rowsum` (singular, lower
case...) is the only one that computes on arbitrary groups in compiled code.
This is a big deal.  One of R's biggest weaknesses is that computing on groups
requires calling R-level functions for each group, defeating the compiled code
vectorizations that usually make R speedy.  `rowsum` gives us a tool to
circumvent this limitation.



All joking aside, I have known about `rowSums` for a long time, and have seen
the name `rowsum` in passing, but it never occurred to me that the latter might
do something as extraordinary as what it actually does: compute on arbitrary
groups in compiled code.


but very few do both.


`rowsum`
there
I only more recently
discovered `rowsum`.  It probably says more about me than I should admit but I
was downright giddy when I realized _exactly_ what `rowsum` does.  After all,
there are many base R functions that compute on arbitrary groups, and many base
R functions that work directly in compiled code, but as far as I know base R
functions that compute on arbitrary groups in compiled code are
rare[^knowledge-caveat].  This is very useful:

<!--
I have known about rowSums for a long time, and I only more recently discovered rowsum. Imagine the excitement a taxidermist might feel on realizing they were just given the corpse of an albino fox, and not that of a white cat as they initially thought. That should capture the magnitude and significance of my excitement when I realized what rowsum does.

How can a base R function possibly compete for excitement with a dead fox? Well, there are many base R functions that compute on arbitrary groups, and many base R functions that work directly in compiled code, but as far as I know base R functions that compute on arbitrary groups in compiled code are rare1. This is very useful:
-->
```{r eval=FALSE}
sys.time(x.rs <- rowsum(x, grp))
```
```
   user  system elapsed
  2.251   0.058   2.310
```
```{r eval=FALSE}
all.equal(c(x.ref), c(x.rs), check.attributes=FALSE)
```
```
[1] TRUE
```

Over three times faster than the original `tapply` solution, and at least
plausibly in the same conversation as the `data.table` one.

```{r echo=FALSE}
funs <- c('tapply', 'rowsum', 'data.table')
times <- data.frame(
  Function=factor(funs, levels=funs),
  time=c(7.308, 2.310, 0.973)
)
ggplot(times, aes(x=Function, y=time)) +
  geom_col() +
  # facet_wrap(~Function) +
  ylab("Time in Sec (Less is Better)") +
  geom_text(aes(label=sprintf("%0.2f", time)), vjust=-1) +
  scale_y_continuous(expand=expand_scale(mult=c(0, .1)))
```

I've wondered if I am alone in my delayed awareness of `rowsum`, but my attempts
to measure the relative popularity of the two functions via search engine were
fruitless.  Given the glaring semantic cues in `rowsum` vs `rowSums` I would
have thought they would do better...  Thankfully years of using "R" as a search
term have inured me to this type of search disappointment.

## colSums

```{r eval=FALSE}
sum_g3 <- function(x, grp, na.rm=TRUE) {
  ord <- order(grp)
  id.ord <- id[ord]
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  max.grp <- max(grp.rle[['lengths']])

  # this NA handling doesn't work b/c for na.rm=FALSE you still get NAs

  res <- matrix(NA_real_, ncol=length(grp.rle[['lengths']]), nrow=max.grp)

  # each group that isn't as long as the longest group needs padding

  rle.len <- grp.rle[['lengths']]
  grp.pad <- max.grp - rle.len
  id.raw <- rep(1L, length(x))
  id.raw[(cumsum(rle.len) + 1L)[-length(rle.len)]] <-
    grp.pad[-length(rle.len)] + 1L
  id <- cumsum(id.raw)

  res[id] <- x[ord]
  structure(colSums(res, na.rm=na.rm), groups=grp.rle[['values']])
}
system.time(sum_g3(x, grp))
```
```
   user  system elapsed
  1.186   0.374   1.634
```
```{r eval=FALSE}
  # lens: how long each group is
  # maxlen: longest group
sum_grp2 <- function(x, lens, maxlen, mode='sum') {

  res <- matrix(NA_real_, ncol=length(lens), nrow=maxlen)

  # Generate indices that will map to the correct spots in `res` from `x`,
  # which means add whatever padding we need to the index value for the next
  # column

  len_1 <- lens[-length(lens)]
  grp.pad <- (maxlen + 1L) - len_1
  id.raw <- rep(1L, length(x))
  len_1[1L] <- len_1[1L] + 1L
  id.raw[cumsum(len_1)] <- grp.pad
  id <- cumsum(id.raw)

  # Inject the x values according to these indices that should place each gropu
  # in a column

  res[id] <- x

  if(identical(mode, 'sum')) colSums(res, na.rm=TRUE)
  else if(identical(mode, 'mean')) colMeans(res, na.rm=TRUE)
  else stop("Invalid mode")
}
```
```
slope2 <- function(x, y, grp) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.len <- grp.rle[['lengths']]
  max.grp <- max(grp.len)

  xo <- x[ord]
  xi <- rep(seq_along(grp.len), grp.len)
  xu <- sum_grp2(xo, grp.len, max.grp, 'mean')
  x_ux <- xo - xu[xi]

  yo <- y[ord]
  yu <- sum_grp2(yo, grp.len, max.grp, 'mean')
  y_uy <- yo - yu[xi]

  x_ux.y_uy <- sum_grp2(x_ux * y_uy, grp.len, max.grp, 'sum')
  x_ux2 <- sum_grp2(x_ux ^ 2, grp.len, max.grp, 'sum')

  x_ux.y_uy / x_ux2
}
slope2a_int <- function(xo, yo, lens) {
  max.grp <- lens[1]

  xi <- rep(seq_along(lens), lens)
  xu <- sum_grp2(xo, lens, max.grp, 'mean')
  x_ux <- xo - xu[xi]

  yu <- sum_grp2(yo, lens, max.grp, 'mean')
  y_uy <- yo - yu[xi]

  x_ux.y_uy <- sum_grp2(x_ux * y_uy, lens, max.grp, 'sum')
  x_ux2 <- sum_grp2(x_ux ^ 2, lens, max.grp, 'sum')

  x_ux.y_uy / x_ux2
}
slope2a <- function(x, y, grp, splits=5) {
  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.len <- grp.rle[['lengths']]

  ord2 <- order(rep(grp.len, grp.len), decreasing=TRUE)
  ordg <- order(grp.len, decreasing=TRUE)
  grp.len.o <- grp.len[ordg]
  len.max <- grp.len.o[1L]  # will break if no groups
  len.min <- grp.len.o[length(grp.len.o)]

  # order the inputs

  ord3 <- ord[ord2]
  xo <- x[ord3]
  yo <- y[ord3]
  go <- grp.ord[ord2]

  # simple initial cut, just cut into equal splits

  cuts <- as.integer(
    round(seq(1L, length(grp.len) + 1L, length.out=splits + 1L))
  )
  grp.len.o.c <- cumsum(c(1L, grp.len.o))
  res <- vector("list", splits)

  for(i in seq_len(splits)){
    # Figure out that starting and ending elements for each group

    start.g <- cuts[i]
    end.g <- cuts[i + 1L]

    start <- grp.len.o.c[start.g]
    end <- grp.len.o.c[end.g] - 1L

    idx.g <- start.g:(end.g - 1L)
    idx <- start:end

    res[[i]] <- slope2a_int(xo[idx], yo[idx], grp.len.o[idx.g])
  }
  # Reorder back in ascending group order instead of group size order

  res.fin <- numeric(length(grp.len))
  res.fin[ordg] <- unlist(res)
  res.fin
}
system.time(slope2(x, y, grp))
RNGversion("3.5.2"); set.seed(42)
x2 <- runif(100)
y2 <- runif(100)
g2 <- sample(1:10, 100, rep=T)
```
```
   user  system elapsed
  2.827   0.932   3.807
  # can't reproduce the earlier timings...
   user  system elapsed
  3.185   1.285   4.783
  # now I can ...
```
```{r eval=FALSE}
system.time(slope2a(x, y, grp))
```
```
   user  system elapsed
  2.998   1.055   4.082
```
# Appendix

## Acknowledgments

* Matt Dowle and the `data.table` team for contributing their `radix` order to
  R.
* Romain Francois for `seven31` which allowed us to debug double precision
  issues.
* Oleg Sklyar, Duncan Murdoch, Mike Smith, Dirk Eddelbuettel, Romain Francois,
  Karline Soetaert for [`inline`][7].
* Ggplot2
* Ggbeeswarm
* Reshape2

## References

* Ulrich Drepper's [What Every Programmer Should Know About Memory][27]
* [Stuffed Cow post on Ivy Bridge Cache Replacement policies][26]
* Cache timings?
* Nima Honarmand [Memory Prefetching][28], including discussion of stream
  buffers.

## Sorting Versions

I quickly threw together C functions that `unique` and `rowsum` on sorted data.
They support a very narrow set of inputs, but for those inputs the timing
comparisons relative to the base equivalents should be fair.  `.rowsums` does
benefit from being able to assume the input will be a vector and not a matrix,
but that should be a small difference.

I tried two variations from the one we looked at in the [main body](#unique-c)
of the post that generally support the timings against the base functions with
input sorting:

* 1e8 vector with ~10 sized groups
* 1e7 vector with ~1 sized groups

`rowsums` preserves the ~2x advantage in both scenarios.  `uniques` maintains
the advantage for the first scenario, but in the second is at parity.  This is
because for `uniques` we need to unsort the result and when there are no
duplicates the vector to unsort is large.  If we were allowed to return the
results sorted then `uniques` is faster, but by a smaller margin than with the
original example.

```{r eval=FALSE}
.uniques <- inline::cfunction(sig=c(x='integer'), body="
  R_xlen_t len, i, len_u = 1;
  SEXP res;
  int *xi = INTEGER(x);
  len = XLENGTH(x);

  if(len > 1) {
    // count uniques
    for(i = 1; i < len; ++i) {
      if(xi[i - 1] != xi[i]) {
        ++len_u;
    } }
    // allocate and record uniques
    res = PROTECT(allocVector(INTSXP, len_u));
    int *resi = INTEGER(res);
    *resi = 1;
    R_xlen_t j = 1;
    for(i = 1; i < len; ++i) {
      if(xi[i - 1] != xi[i]) {
        resi[j++] = i + 1;  // 1 based indexing
    } }
    UNPROTECT(1);
  } else {
    res = x;
  }
  return res;
")

.rowsums <- inline::cfunction(
  sig=c(x='numeric', g='integer'),
  body="
  R_xlen_t len, i, len_u = 1;
  SEXP res, res_x, res_g;
  int *gi = INTEGER(g);
  double *xi = REAL(x);
  len = XLENGTH(g);
  if(len != XLENGTH(x)) error(\"Unequal Length Vectors\");
  res = PROTECT(allocVector(VECSXP, 2));

  if(len > 1) {
    // count uniques
    for(i = 1; i < len; ++i) {
      if(gi[i - 1] != gi[i]) {
        ++len_u;
    } }
    // allocate and record uniques
    res_x = PROTECT(allocVector(REALSXP, len_u));
    res_g = PROTECT(allocVector(INTSXP, len_u));

    double *res_xi = REAL(res_x);
    int *res_gi = INTEGER(res_g);
    R_xlen_t j = 0;

    res_xi[0] = 0;
    for(i = 1; i < len; ++i) {
      res_xi[j] += xi[i - 1]; // we don't check for double overflow...
      if(gi[i - 1] != gi[i]) {
        res_gi[j] = gi[i - 1];
        ++j;
        res_xi[j] = 0;
    } }
    res_xi[j] += xi[i - 1];
    res_gi[j] = gi[i - 1];

    SET_VECTOR_ELT(res, 0, res_x);
    SET_VECTOR_ELT(res, 1, res_g);
    UNPROTECT(2);
  } else {
    // Don't seem to need to duplicate x/g
    SET_VECTOR_ELT(res, 0, x);
    SET_VECTOR_ELT(res, 1, g);
  }
  UNPROTECT(1);
  return res;
")
```

## Other Slope Benchmarks

### vapply

Normal:

```{r eval=FALSE}
sys.time({
  id <- seq_along(grp)
  id.split <- split(id, grp)
  slope.ply <- vapply(id.split, function(id) slope(x[id], y[id]), 0)
})
```
```
   user  system elapsed
 12.416   0.142  12.573
```
```{r eval=FALSE}
all.equal(slope.ply, c(slope.rs), check.attributes=FALSE)
```
```
[1] TRUE
```

Sorted version:


```{r eval=FALSE}
sys.time({
  o <- order(grp)
  go <- grp[o]
  id <- seq_along(grp)[o]
  id.split <- split(id, go)
  slope.ply2 <- vapply(id.split, function(id) slope(x[id], y[id]), 0)
})
```
```
   user  system elapsed
  8.397   0.155   8.557
```
```{r eval=FALSE}
all.equal(slope.ply2, c(slope.rs), check.attributes=FALSE)
```
```
[1] TRUE
```

### data.table

Normal:

```{r eval=FALSE}
setDTthreads(1)
DT <- data.table(grp, x, y)
sys.time(DT[, slope(x, y), grp])
```
```
   user  system elapsed
  7.117   0.046   7.172
```

Normal multi-thread:

```{r eval=FALSE}
setDTthreads(0)
DT <- data.table(grp, x, y)
sys.time(DT[, slope(x, y), grp])
```
```
   user  system elapsed
   8.53    0.11    6.73
```

Optimized:

```{r eval=FALSE}
library(data.table)
setDTthreads(1)
sys.time({
  DT <- data.table(grp, x, y)
  setkey(DT, grp)
  DTsum <- DT[, .(ux=mean(x), uy=mean(y)), keyby=grp]
  DT[DTsum, `:=`(x_ux=x - ux, y_uy=y - uy)]
  DT[, `:=`(x_ux.y_uy=x_ux * y_uy, x_ux2=x_ux^2)]
  DTsum <- DT[, .(x_ux.y_uy=sum(x_ux.y_uy), x_ux2=sum(x_ux2)), keyby=grp]
  res.slope.dt2 <- DTsum[, .(grp, V1=x_ux.y_uy / x_ux2)]
})
```
```
   user  system elapsed
  3.102   0.352   3.454
```

Optimized multi-core:

```{r}
setDTthreads(0)
sys.time({
  DT <- data.table(grp, x, y)
  setkey(DT, grp)
  DTsum <- DT[, .(ux=mean(x), uy=mean(y)), keyby=grp]
  DT[DTsum, `:=`(x_ux=x - ux, y_uy=y - uy)]
  DT[, `:=`(x_ux.y_uy=x_ux * y_uy, x_ux2=x_ux^2)]
  DTsum <- DT[, .(x_ux.y_uy=sum(x_ux.y_uy), x_ux2=sum(x_ux2)), keyby=grp]
  res.slope.dt2 <- DTsum[, .(grp, V1=x_ux.y_uy / x_ux2)]
})
```
```
   user  system elapsed
  7.147   0.855   2.609
```

## Function Definitions

```{r sys-time, echo=FALSE}
```

### group_sum

```{r group_sum-def, eval=FALSE}
```

### Cumulative Group Sum With NA and Inf

A correct implementation of the single pass `cumsum` based `group_sum` requires
a bit of work to correctly handle both `NA` and `Inf` values.  Both of these
need to be pulled out of the data ahead of the cumulative step otherwise they
would wreck all subsequent calculations.  The rub is they need to be re-injected
into the results, and with `Inf` we need to account for groups computing to
`Inf`, `-Inf`, and even `NaN`.

I implemented a version of `group_sum` that handles these for illustrative
purposes.  It is lightly tested so you should not consider it to be generally
robust.

```{r eval=FALSE}
group_sum3 <- function(x, grp, na.rm=FALSE) {
  if(length(x) != length(grp)) stop("Unequal length args")
  if(!(is.atomic(x) && is.atomic(y))) stop("Non-atomic args")
  if(anyNA(grp)) stop("NA vals not supported in `grp`")

  ord <- order(grp)
  grp.ord <- grp[ord]
  grp.rle <- rle(grp.ord)
  grp.rle.c <- cumsum(grp.rle[['lengths']])
  x.ord <- x[ord]

  # NA and Inf handling. Checking inf makes this 5% slower, but
  # doesn't seem worth adding special handling for cases w/o Infs

  has.na <- anyNA(x)
  if(has.na) {
    na.x <- which(is.na(x.ord))
    x.ord[na.x] <- 0
  } else na.x <- integer()
  inf.x <- which(is.infinite(x.ord))
  any.inf <- length(inf.x) > 0
  if(any.inf) {
    inf.vals <- x.ord[inf.x]
    x.ord[inf.x] <- 0
  }
  x.grp.c <- cumsum(x.ord)[grp.rle.c]
  x.grp.c[-1L] <- x.grp.c[-1L] - x.grp.c[-length(x.grp.c)]

  # Re-inject NAs and Infs as needed

  if(any.inf) {
    inf.grps <- findInterval(inf.x, grp.rle.c, left.open=TRUE) + 1L
    inf.rle <- rle(inf.grps)
    inf.res <- rep(Inf, length(inf.rle[['lengths']]))
    inf.neg <- inf.vals < 0

    # If more than one Inf val in group, need to make sure we don't have
    # Infs of different signs as those add up to NaN
    if(any(inf.long <- (inf.rle[['lengths']] > 1L))) {
      inf.pos.g <- group_sum2(!inf.neg, inf.grps)
      inf.neg.g <- group_sum2(inf.neg, inf.grps)
      inf.res[inf.neg.g > 0] <- -Inf
      inf.res[inf.pos.g & inf.neg.g] <- NaN
    } else {
      inf.res[inf.neg] <- -Inf
    }
    x.grp.c[inf.rle[['values']]] <- inf.res
  }
  if(!na.rm && has.na)
    x.grp.c[findInterval(na.x, grp.rle.c, left.open=TRUE) + 1L] <- NA

  structure(x.grp.c, groups=grp.rle[['values']], n=grp.rle[['lengths']])
}
system.time(x.grpsum3 <- group_sum3(x, grp))
```
```
   user  system elapsed
  1.189   0.346   1.540
```
```{r echo=FALSE}
all.equal(x.grpsum, x.grpsum3)
```
```
[1] TRUE
```
```{r echo=FALSE, eval=FALSE}
  ## some tests
RNGversion("3.5.2"); set.seed(42)
g20 <- rep(c(1, 3, 8, 9), c(2, 3, 4, 1))
x20 <- runif(length(g2))
rbind(g20, x20)
o20 <- sample(length(g2))
oo20 <- order(o20)
g2 <- g20[o20]

xs <- replicate(4, x20[o20], simplify=FALSE)
xs[[1]][oo20][c(2, 6)] <- Inf
xs[[2]][oo20][c(2, 6)] <- NA
xs[[3]] <- xs[[2]]
xs[[4]][oo20][c(1, 2, 3, 4, 7, 8, 10)] <- c(Inf, -Inf, Inf, Inf, NA, Inf, -Inf)

mapply(
  function(x, g, na.rm) {
    all.equal(
      group_sum3(x, g, na.rm=na.rm),
      c(rowsum(x, g, na.rm=na.rm)),
      check.attributes=FALSE
    )
  },
  xs, list(g2), c(FALSE, FALSE, TRUE, FALSE)
)
```

### Adjustable Precision Group Sums

```{r group_sum_int3, eval=FALSE}
```





[^knowledge-caveat]: Given how long it's taken me to find out about `rowsum` it
  is fair to question whether I would know whether there are many other
  functions of this kind out there or not.
[^vec-size]: Numeric vectors require 8 bytes per element plus some overhead for
  the object meta data.
[^int-vec]: R code that carries out looped operations over vectors in compiled
  code rather than in R-level code.
[^love-r-but]: I love R, but the madness around [text decoration
  conventions][1], or lack thereof is something that I could do without.  Sorry
  for the rant, but I was particularly triggered by this example.  I also do
  understand that in large collaborative project like this one, things like this
  are bound to happen and in the end it's not a big deal.
[^not-quite-sum]: `mean(x)` in R is not exactly equivalent to
  `sum(x) / length(x)` because `mean` has a two pass precision improvement
  algorithm.
[^forgive-me]: I am prone to absurd pedantic turns of phrase when giddy, so
  please forgive me.  I did mention that I am `rowsum` made me giddy, right?
[prior post][8]
[^slope-mod]: This is a slightly modified version of the original from [prior
  post][8] that is faster because it uses `mean.default` instead of `mean`.
[^timing-note]: Timings in the [prior post][8] are slower due to the use of
  `mean` instead of `mean.default` in the [`slope` function](#slope-ex).
[^dt-optim]: In order to unlock the full performance of `data.table` we need to
  engage in some manipulations.  We covered these in the [prior group statistics
  post][9].
[^prior-art]: I did some light research but did not find other
  obvious uses of this method.  Since this approach was not really practical
  until `data.table` radix sorting was added to base R in version 3.3.0, its
  plausible it is somewhat rare.  Send me links if there are other examples.
[^extended-precision]: The additional precision is only available internally;
  the return value is truncated to the regular double precision.
[^extended-2]: The extended precision digits are never actually directly visible
  in R as they are truncated away before return from C code, we exercise
  poetic license here for illustrative purposes.
[^precision-digits]: Double precision is roughly `log10(2^53)`, or 15.95
  in decimal, but precision can really only be measured in binary "digits", so
  you cannot directly compute how much precision is lost by comparing two
  decimal numbers.  This particular subtraction loses us 21 bits of precision,
  or about 6.32 digits of decimal precision.
[^mag-diff]: Adding numbers with low precision but very different magnitudes
  can create a bit pattern in the double representation that suggests high
  precision, but that is just an artifact of the computation.  The actual
  precision is no higher than the precision of the highest magnitude number.
[^mean-pathological]: Incidentally, this implies that you will get less
  precise answers if your vector is [sorted by value][19] instead of even just
  randomly.
[^mag-prec]: There is still the general order of magnitude left, and since this
  is a base 2 order of magnitude you should still be at +-50% accuracy (i.e.
  value is between x and 2x).
[^prec-mismatch]: Precision mismatch can arise when adding or subtracting
  numbers different magnitudes, different precisions, or some combination of the
  two.  Only the most significant digits of the larger magnitude number that are
  either larger than the largest digit of the other number or overlap with a
  "precise" digit of the smaller number survive.  As always all these
  determinations should be made on binary digits, although it should hold
  roughly true for decimal digits.
[^for-effect]: We exaggerated the errors for expository purposes.
[^full-prec-assumption]: This is assuming the entire precision of
  `x_ux2.cum.pre` and `x_ux2.cum` is real.  As you saw previously these involve
  numbers that are not [fully precise](#mu-prec), which may cause concern.
  However, what we actually care about is that the cumulative sum of these
  numbers is numerically precise, not semantically precise.
[^plus-minus]: 3 bits allow us to represent `$2^3 == 8$` values, so naively
  error spans `$1/8$`, or `$\pm1/16 == \pm6.25%$`.
[^fifty-three]: You'll notice that we have reported a total of 65, not 64 bits
 (11 + 1 + 53).  This is because in reality "doubles" only store the last 52 bits
 of the significand.  The first one is always assumed to be 1, except for the
 special case where the exponent is all zeroes.  Because of this it does not
 need to be explicitly encoded.
[^three-bits]: Internally the `B.s` will be stored with all 53 bits of
  precision, but the trailing 50 bits will be all zeroes and carry no real
  precision.
[^rel-err]: The relative error value will fluctuate depending on the value we
  are measuring it to.  Our reference value is in binary `1.01` (exp:
  `$2^{-32}`), but it could also have been `1.11` or `0.01`, so for 3 bit values
  the relative error can be as low as ~14% or as high as 100%.
[^by-hand]: since R doesn't support 80 bit doubles at the user level, we had to
  do the binary addition by hand...


1. Read factor level     (int)
2. Read offset in group  (int)xx  Random seek in lvls long vector
3. Read input            (dbl)
4. Read list element     (ptr)xx
5. Read group element    (dbl)x   In most cases should be from after read above
6. Write group elment    (dbl)    Random seek in lvls long vec of vecs
7. Increment offset      (int).
8. Write offset          (int)    Should still be in cache

[1]: https://twitter.com/BrodieGaslam/status/976616435836510210
[3]: https://stackoverflow.com/a/11227902/2725969
[4]: https://www.extremetech.com/extreme/188776-how-l1-and-l2-cpu-caches-work-and-why-theyre-an-essential-part-of-modern-chips
[5]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/main/unique.c#L1514
[6]: https://github.com/wch/r-source/blob/tags/R-3-5-3/src/library/base/R/rowsum.R#L26
[7]: https://cran.r-project.org/web/packages/inline/index.html
[8]: /2019/02/24/a-strategy-for-faster-group-statisitics/#blood-from-a-turnip/
[9]: /2019/02/24/a-strategy-for-faster-group-statisitics/#optim-start/
[10]: https://github.com/JohnMount
[11]: https://github.com/WinVector/FastBaseR/blob/f4d4236/R/cumsum.R#L105
[12]: https://en.wikipedia.org/wiki/Double-precision_floating-point_format
[13]: https://github.com/wch/r-source/blob/R-3-5-branch/src/main/cum.c#L30
[14]: https://en.wikipedia.org/wiki/Extended_precision#Working_range
[15]: https://github.com/romainfrancois
[16]: https://github.com/ThinkR-open/seven31
[17]: https://github.com/wrathematics/float
[18]: https://github.com/wch/r-source/blob/R-3-3-branch/src/main/summary.c#L434
[19]: https://twitter.com/BrodieGaslam/status/1113783101262585856
[20]: https://www.ibm.com/blogs/research/2018/12/8-bit-precision-training/
[21]: /2019/02/24/a-strategy-for-faster-group-statisitics/
[24]: https://twitter.com/BrodieGaslam/status/1106231241488154626
<!-- 
Analysis of cache replacement policies with experiments.
-->

[26]: http://blog.stuffedcow.net/2013/01/ivb-cache-replacement/
<!-- 
Ulrich Drepper Paper, lots of goodies.

Table 2.2: DDR3 array/bus frequencies (933MHz for DDR3-1866)
Bottom pg 15: Latency numbers for Pentium M, including 240 cycles for main
memory.

Oddly doesn't really talk about interleaved access.
-->

[27]: https://people.freebsd.org/~lstewart/articles/cpumemory.pdf

[28]: https://compas.cs.stonybrook.edu/~nhonarmand/courses/sp16/cse502/slides/13-prefetch.pdf
[34]: https://iis-people.ee.ethz.ch/~gmichi/asocd/addinfo/Out-of-Order_execution.pdf


<!-- Paper that discusses memory page handling, some details on banks, and of
most interest is the idea that you can have a policy that closes pages right
away or leaves them open depending on expectation that prob of next read being
on last page read -->

[36]: https://arxiv.org/pdf/1509.03740.pdf

<!-- 
Very detailed overview of architecture of Core i7 and Xeon 5500 (Circa 2008?),
timings in cycles for cache accesses, and ns for memory 

Includes the 4, 10, 40 cycle access times, plus 100ns for main memory (p8,9,22).

Details about TLB (separate large page TLBs, 2 level TLB, see p9, 64 and 512
entries)
-->

[37]: https://software.intel.com/sites/products/collateral/hpc/vtune/performance_analysis_guide.pdf

<!-- Typical timings in mostly ns-->

[38]: http://norvig.com/21-days.html#answers


<!-- 
Wiki page on DDR3 

Timings for 1866 suggest 11-14 nanoseconds for CAS and Row activation, which
strongly suggests 100ns is way more reasonable than 250 cycles unless the 250
cycles is dominated by bus latency..
-->

[39]: https://en.wikipedia.org/wiki/DDR3_SDRAM#JEDEC_standard_modules

<!--
2014 blogpost with latencies, has main memory at 360 cycles
-->
[40]: http://sgros.blogspot.com/2014/08/memory-access-latencies.html

<!--
More timings, has main memory at 200 cycles, but L1 cache at 0.5ns!  Even at 
possible with 4GHz that's two cycles, seems wrong?  This does tie out with the
100ns number though.  It's possible that in my system the memory access latency
is ironically lower because the clock is so slow, i.e. it always takes about
100ns to get to memory.
-->
https://gist.github.com/jboner/2841832

<!--
CMU paper on DRAM refresh notes typically 8 banks per rank
-->

[43]: https://users.ece.cmu.edu/~omutlu/pub/dram-access-refresh-parallelization_hpca14.pdf

<!-- 
Archives New Zealand photo of coal train 
CC BY 2.0
-->

[44]: https://www.flickr.com/photos/archivesnz/35756097220/in/photolist-WtDkYQ-pFXbm3-EC9kXN-7kXxX1-8U6V9V-8ARY8U-8AS1m3-e2vora-54GqWx-2ejTYGK-24Jjyh2-6VJXqq-Hr5etz-am3WNE-niGFx9-c47ogL-pvnydm-8ANKYR-q8dYGa-GC23Cx-2cwaLQc-22GYq5b-7bdD4a-5J8sM1-a7C8ZL-oeGeph-4qHJ3t-iP2gsi-28A2Rjv-8WetRn-cpRqNW-d4paYu-ppvSpN-UAE5gq-YmgY8J-g1EDcJ-bCWKu5-9fyM4U-YQekTS-sdCeYu-BseS2w-pkMARp-ebgH77-2ekscff-ixN2mt-dXp7D2-a8h1SU-e1d6MP-Ttk3vp-2d4h1cC

<!-- 
Reorder Buffer Size testing

Of interest:
* Mentions the ~200 cycle main memory access time, but the tests themselves
  suggest potentially faster (~150) cycles.
* Shows reorder buffer size likely ~168 for Sandybridge, which is the generation
  prior to Skylake on my system.
-->
[45]: http://blog.stuffedcow.net/2013/05/measuring-rob-capacity/

<!--
Not a reference, but someone that at least seems to have some idea of what's
going on, but no idea if they are right or not.
-->

[46]: https://forums.tomshardware.com/threads/what-exactly-is-a-memory-bank.2919273/#post-18421032

<!--
RIDL and Fallout website, in particular has a great diagram of memory system
architecture, including ROB entry counts, line fill buffer mention, separate L1
and L2 TLBs.
-->

[47]: https://mdsattacks.com/

<!--
Bad concurrency with great but low rez annotated die image
-->

[48]: https://www.flickr.com/search/?text=cpu+die+cache

<!--
Westmere 6 core image
-->
[50]: http://i.imgur.com/AawdBre.jpg


